{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Keras to Build and Train Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we will use a neural network to predict diabetes using the Pima Diabetes Dataset.  We will start by training a Random Forest to get a performance baseline.  Then we will use the Keras package to quickly build and train a neural network and compare the performance.  We will see how different network structures affect the performance, training time, and level of overfitting (or underfitting).\n",
    "\n",
    "## UCI Pima Diabetes Dataset\n",
    "\n",
    "* UCI ML Repositiory (http://archive.ics.uci.edu/ml/datasets/Pima+Indians+Diabetes)\n",
    "\n",
    "\n",
    "### Attributes: (all numeric-valued)\n",
    "   1. Number of times pregnant\n",
    "   2. Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n",
    "   3. Diastolic blood pressure (mm Hg)\n",
    "   4. Triceps skin fold thickness (mm)\n",
    "   5. 2-Hour serum insulin (mu U/ml)\n",
    "   6. Body mass index (weight in kg/(height in m)^2)\n",
    "   7. Diabetes pedigree function\n",
    "   8. Age (years)\n",
    "   9. Class variable (0 or 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The UCI Pima Diabetes Dataset which has 8 numerical predictors and a binary outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Preliminaries\n",
    "\n",
    "from __future__ import absolute_import, division, print_function  # Python 2/3 compatibility\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_curve, roc_auc_score, roc_curve, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "## Import Keras objects for Deep Learning\n",
    "\n",
    "from keras.models  import Sequential, K\n",
    "from keras.layers import Input, Dense, Flatten, Dropout, BatchNormalization\n",
    "from keras.optimizers import Adam, SGD, RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Load in the data set (Internet Access needed)\n",
    "\n",
    "url = \"http://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data\"\n",
    "names = [\"times_pregnant\", \"glucose_tolerance_test\", \"blood_pressure\", \"skin_thickness\", \"insulin\", \n",
    "         \"bmi\", \"pedigree_function\", \"age\", \"has_diabetes\"]\n",
    "diabetes_df = pd.read_csv(url, names=names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>times_pregnant</th>\n",
       "      <th>glucose_tolerance_test</th>\n",
       "      <th>blood_pressure</th>\n",
       "      <th>skin_thickness</th>\n",
       "      <th>insulin</th>\n",
       "      <th>bmi</th>\n",
       "      <th>pedigree_function</th>\n",
       "      <th>age</th>\n",
       "      <th>has_diabetes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>663</th>\n",
       "      <td>9</td>\n",
       "      <td>145</td>\n",
       "      <td>80</td>\n",
       "      <td>46</td>\n",
       "      <td>130</td>\n",
       "      <td>37.9</td>\n",
       "      <td>0.637</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>1</td>\n",
       "      <td>91</td>\n",
       "      <td>64</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>29.2</td>\n",
       "      <td>0.192</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>7</td>\n",
       "      <td>142</td>\n",
       "      <td>60</td>\n",
       "      <td>33</td>\n",
       "      <td>190</td>\n",
       "      <td>28.8</td>\n",
       "      <td>0.687</td>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>0</td>\n",
       "      <td>104</td>\n",
       "      <td>64</td>\n",
       "      <td>23</td>\n",
       "      <td>116</td>\n",
       "      <td>27.8</td>\n",
       "      <td>0.454</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>13</td>\n",
       "      <td>106</td>\n",
       "      <td>72</td>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>36.6</td>\n",
       "      <td>0.178</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     times_pregnant  glucose_tolerance_test  blood_pressure  skin_thickness  \\\n",
       "663               9                     145              80              46   \n",
       "240               1                      91              64              24   \n",
       "223               7                     142              60              33   \n",
       "277               0                     104              64              23   \n",
       "86               13                     106              72              54   \n",
       "\n",
       "     insulin   bmi  pedigree_function  age  has_diabetes  \n",
       "663      130  37.9              0.637   40             1  \n",
       "240        0  29.2              0.192   21             0  \n",
       "223      190  28.8              0.687   61             0  \n",
       "277      116  27.8              0.454   23             0  \n",
       "86         0  36.6              0.178   45             0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a peek at the data -- if there are lots of \"NaN\" you may have internet connectivity issues\n",
    "print(diabetes_df.shape)\n",
    "diabetes_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = diabetes_df.iloc[:, :-1].values\n",
    "y = diabetes_df[\"has_diabetes\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split the data to Train, and Test (75%, 25%)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=11111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3489583333333333, 0.6510416666666666)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(y), np.mean(1-y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we see that about 35% of the patients in this dataset have diabetes, while 65% do not.  This means we can get an accuracy of 65% without any model - just declare that no one has diabetes. We will calculate the ROC-AUC score to evaluate performance of our model, and also look at the accuracy as well to see if we improved upon the 65% accuracy.\n",
    "## Exercise: Get a baseline performance using Random Forest\n",
    "To begin, and get a baseline for classifier performance:\n",
    "1. Train a Random Forest model with 200 trees on the training data.\n",
    "2. Calculate the accuracy and roc_auc_score of the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Train the RF Model\n",
    "rf_model = RandomForestClassifier(n_estimators=200)\n",
    "rf_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 0.776\n",
      "roc-auc is 0.829\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test set - both \"hard\" predictions, and the scores (percent of trees voting yes)\n",
    "y_pred_class_rf = rf_model.predict(X_test)\n",
    "y_pred_prob_rf = rf_model.predict_proba(X_test)\n",
    "\n",
    "\n",
    "print('accuracy is {:.3f}'.format(accuracy_score(y_test,y_pred_class_rf)))\n",
    "print('roc-auc is {:.3f}'.format(roc_auc_score(y_test,y_pred_prob_rf[:,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAecAAAHiCAYAAADSwATnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1154f7240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_roc(y_test, y_pred, model_name):\n",
    "    fpr, tpr, thr = roc_curve(y_test, y_pred)\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    ax.plot(fpr, tpr, 'k-')\n",
    "    ax.plot([0, 1], [0, 1], 'k--', linewidth=.5)  # roc curve for random model\n",
    "    ax.grid(True)\n",
    "    ax.set(title='ROC Curve for {} on PIMA diabetes problem'.format(model_name),\n",
    "           xlim=[-0.01, 1.01], ylim=[-0.01, 1.01])\n",
    "\n",
    "\n",
    "plot_roc(y_test, y_pred_prob_rf[:, 1], 'RF')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Single Hidden Layer Neural Network\n",
    "\n",
    "We will use the Sequential model to quickly build a neural network.  Our first network will be a single layer network.  We have 8 variables, so we set the input shape to 8.  Let's start by having a single hidden layer with 12 nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## First let's normalize the data\n",
    "## This aids the training of neural nets by providing numerical stability\n",
    "## Random Forest does not need this as it finds a split only, as opposed to performing matrix multiplications\n",
    "\n",
    "\n",
    "normalizer = StandardScaler()\n",
    "X_train_norm = normalizer.fit_transform(X_train)\n",
    "X_test_norm = normalizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the Model \n",
    "# Input size is 8-dimensional\n",
    "# 1 hidden layer, 12 hidden nodes, sigmoid activation\n",
    "# Final layer has just one node with a sigmoid activation (standard for binary classification)\n",
    "\n",
    "model_1 = Sequential([\n",
    "    Dense(12, input_shape=(8,), activation=\"relu\"),\n",
    "    Dense(1, activation=\"sigmoid\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 12)                108       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 13        \n",
      "=================================================================\n",
      "Total params: 121\n",
      "Trainable params: 121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#  This is a nice tool to view the model you have created and count the parameters\n",
    "\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comprehension question:\n",
    "Why do we have 121 parameters?  Does that make sense?\n",
    "\n",
    "\n",
    "Let's fit our model for 200 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 576 samples, validate on 192 samples\n",
      "Epoch 1/200\n",
      "576/576 [==============================] - 0s 523us/step - loss: 0.7395 - acc: 0.5260 - val_loss: 0.7084 - val_acc: 0.5052\n",
      "Epoch 2/200\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.7297 - acc: 0.5347 - val_loss: 0.7003 - val_acc: 0.5156\n",
      "Epoch 3/200\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.7206 - acc: 0.5382 - val_loss: 0.6927 - val_acc: 0.5365\n",
      "Epoch 4/200\n",
      "576/576 [==============================] - 0s 91us/step - loss: 0.7122 - acc: 0.5503 - val_loss: 0.6856 - val_acc: 0.5417\n",
      "Epoch 5/200\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.7042 - acc: 0.5556 - val_loss: 0.6790 - val_acc: 0.5573\n",
      "Epoch 6/200\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.6968 - acc: 0.5677 - val_loss: 0.6729 - val_acc: 0.5729\n",
      "Epoch 7/200\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.6899 - acc: 0.5729 - val_loss: 0.6672 - val_acc: 0.5729\n",
      "Epoch 8/200\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.6834 - acc: 0.5851 - val_loss: 0.6618 - val_acc: 0.5781\n",
      "Epoch 9/200\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.6774 - acc: 0.5920 - val_loss: 0.6568 - val_acc: 0.5833\n",
      "Epoch 10/200\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.6716 - acc: 0.5955 - val_loss: 0.6522 - val_acc: 0.5885\n",
      "Epoch 11/200\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.6663 - acc: 0.6024 - val_loss: 0.6478 - val_acc: 0.6198\n",
      "Epoch 12/200\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.6613 - acc: 0.6111 - val_loss: 0.6436 - val_acc: 0.6354\n",
      "Epoch 13/200\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.6565 - acc: 0.6181 - val_loss: 0.6397 - val_acc: 0.6458\n",
      "Epoch 14/200\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.6520 - acc: 0.6319 - val_loss: 0.6361 - val_acc: 0.6510\n",
      "Epoch 15/200\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.6477 - acc: 0.6372 - val_loss: 0.6326 - val_acc: 0.6510\n",
      "Epoch 16/200\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.6436 - acc: 0.6458 - val_loss: 0.6294 - val_acc: 0.6667\n",
      "Epoch 17/200\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.6397 - acc: 0.6458 - val_loss: 0.6263 - val_acc: 0.6667\n",
      "Epoch 18/200\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.6361 - acc: 0.6510 - val_loss: 0.6234 - val_acc: 0.6719\n",
      "Epoch 19/200\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.6325 - acc: 0.6597 - val_loss: 0.6206 - val_acc: 0.6771\n",
      "Epoch 20/200\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.6292 - acc: 0.6615 - val_loss: 0.6179 - val_acc: 0.6719\n",
      "Epoch 21/200\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.6260 - acc: 0.6649 - val_loss: 0.6154 - val_acc: 0.6719\n",
      "Epoch 22/200\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.6230 - acc: 0.6632 - val_loss: 0.6130 - val_acc: 0.6771\n",
      "Epoch 23/200\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.6201 - acc: 0.6684 - val_loss: 0.6107 - val_acc: 0.6823\n",
      "Epoch 24/200\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.6173 - acc: 0.6701 - val_loss: 0.6085 - val_acc: 0.6875\n",
      "Epoch 25/200\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.6146 - acc: 0.6736 - val_loss: 0.6064 - val_acc: 0.6875\n",
      "Epoch 26/200\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.6120 - acc: 0.6788 - val_loss: 0.6044 - val_acc: 0.6927\n",
      "Epoch 27/200\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.6095 - acc: 0.6823 - val_loss: 0.6024 - val_acc: 0.6979\n",
      "Epoch 28/200\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.6071 - acc: 0.6840 - val_loss: 0.6006 - val_acc: 0.6979\n",
      "Epoch 29/200\n",
      "576/576 [==============================] - 0s 93us/step - loss: 0.6049 - acc: 0.6823 - val_loss: 0.5988 - val_acc: 0.6979\n",
      "Epoch 30/200\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.6026 - acc: 0.6840 - val_loss: 0.5971 - val_acc: 0.6979\n",
      "Epoch 31/200\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.6005 - acc: 0.6840 - val_loss: 0.5955 - val_acc: 0.6927\n",
      "Epoch 32/200\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.5985 - acc: 0.6858 - val_loss: 0.5940 - val_acc: 0.6927\n",
      "Epoch 33/200\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.5965 - acc: 0.6910 - val_loss: 0.5925 - val_acc: 0.6927\n",
      "Epoch 34/200\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.5946 - acc: 0.6962 - val_loss: 0.5910 - val_acc: 0.6927\n",
      "Epoch 35/200\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.5928 - acc: 0.6997 - val_loss: 0.5896 - val_acc: 0.6875\n",
      "Epoch 36/200\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.5910 - acc: 0.6997 - val_loss: 0.5883 - val_acc: 0.6875\n",
      "Epoch 37/200\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.5892 - acc: 0.6997 - val_loss: 0.5870 - val_acc: 0.6875\n",
      "Epoch 38/200\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.5875 - acc: 0.7031 - val_loss: 0.5857 - val_acc: 0.6875\n",
      "Epoch 39/200\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.5858 - acc: 0.7031 - val_loss: 0.5845 - val_acc: 0.6875\n",
      "Epoch 40/200\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.6822 - acc: 0.562 - 0s 58us/step - loss: 0.5842 - acc: 0.7101 - val_loss: 0.5833 - val_acc: 0.6875\n",
      "Epoch 41/200\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.5826 - acc: 0.7118 - val_loss: 0.5821 - val_acc: 0.6979\n",
      "Epoch 42/200\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.5811 - acc: 0.7135 - val_loss: 0.5810 - val_acc: 0.6979\n",
      "Epoch 43/200\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.5796 - acc: 0.7153 - val_loss: 0.5799 - val_acc: 0.6979\n",
      "Epoch 44/200\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.5781 - acc: 0.7188 - val_loss: 0.5788 - val_acc: 0.7083\n",
      "Epoch 45/200\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.5767 - acc: 0.7188 - val_loss: 0.5778 - val_acc: 0.7083\n",
      "Epoch 46/200\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.5753 - acc: 0.7240 - val_loss: 0.5767 - val_acc: 0.7031\n",
      "Epoch 47/200\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.5739 - acc: 0.7257 - val_loss: 0.5757 - val_acc: 0.7083\n",
      "Epoch 48/200\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5725 - acc: 0.7257 - val_loss: 0.5747 - val_acc: 0.7083\n",
      "Epoch 49/200\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.5713 - acc: 0.7257 - val_loss: 0.5738 - val_acc: 0.7083\n",
      "Epoch 50/200\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.5699 - acc: 0.7274 - val_loss: 0.5728 - val_acc: 0.7135\n",
      "Epoch 51/200\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.5687 - acc: 0.7274 - val_loss: 0.5719 - val_acc: 0.7135\n",
      "Epoch 52/200\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.5674 - acc: 0.7274 - val_loss: 0.5710 - val_acc: 0.7083\n",
      "Epoch 53/200\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.5662 - acc: 0.7274 - val_loss: 0.5701 - val_acc: 0.7083\n",
      "Epoch 54/200\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.5650 - acc: 0.7274 - val_loss: 0.5693 - val_acc: 0.7083\n",
      "Epoch 55/200\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.5639 - acc: 0.7292 - val_loss: 0.5684 - val_acc: 0.7083\n",
      "Epoch 56/200\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.5627 - acc: 0.7274 - val_loss: 0.5676 - val_acc: 0.7188\n",
      "Epoch 57/200\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.5616 - acc: 0.7274 - val_loss: 0.5668 - val_acc: 0.7188\n",
      "Epoch 58/200\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.5605 - acc: 0.7274 - val_loss: 0.5659 - val_acc: 0.7188\n",
      "Epoch 59/200\n",
      "576/576 [==============================] - 0s 91us/step - loss: 0.5595 - acc: 0.7292 - val_loss: 0.5652 - val_acc: 0.7188\n",
      "Epoch 60/200\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.5584 - acc: 0.7292 - val_loss: 0.5644 - val_acc: 0.7188\n",
      "Epoch 61/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 61us/step - loss: 0.5574 - acc: 0.7292 - val_loss: 0.5636 - val_acc: 0.7188\n",
      "Epoch 62/200\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.5563 - acc: 0.7326 - val_loss: 0.5629 - val_acc: 0.7188\n",
      "Epoch 63/200\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.5553 - acc: 0.7326 - val_loss: 0.5621 - val_acc: 0.7188\n",
      "Epoch 64/200\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.5543 - acc: 0.7326 - val_loss: 0.5614 - val_acc: 0.7188\n",
      "Epoch 65/200\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.5533 - acc: 0.7361 - val_loss: 0.5607 - val_acc: 0.7188\n",
      "Epoch 66/200\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.5524 - acc: 0.7361 - val_loss: 0.5600 - val_acc: 0.7188\n",
      "Epoch 67/200\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.5515 - acc: 0.7378 - val_loss: 0.5593 - val_acc: 0.7188\n",
      "Epoch 68/200\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.5506 - acc: 0.7378 - val_loss: 0.5587 - val_acc: 0.7188\n",
      "Epoch 69/200\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.5497 - acc: 0.7378 - val_loss: 0.5580 - val_acc: 0.7188\n",
      "Epoch 70/200\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.5488 - acc: 0.7378 - val_loss: 0.5574 - val_acc: 0.7188\n",
      "Epoch 71/200\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.5479 - acc: 0.7361 - val_loss: 0.5568 - val_acc: 0.7240\n",
      "Epoch 72/200\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.5470 - acc: 0.7361 - val_loss: 0.5561 - val_acc: 0.7240\n",
      "Epoch 73/200\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.5462 - acc: 0.7361 - val_loss: 0.5555 - val_acc: 0.7240\n",
      "Epoch 74/200\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.5453 - acc: 0.7378 - val_loss: 0.5549 - val_acc: 0.7240\n",
      "Epoch 75/200\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.5444 - acc: 0.7378 - val_loss: 0.5544 - val_acc: 0.7240\n",
      "Epoch 76/200\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.5436 - acc: 0.7378 - val_loss: 0.5538 - val_acc: 0.7240\n",
      "Epoch 77/200\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.5428 - acc: 0.7378 - val_loss: 0.5532 - val_acc: 0.7240\n",
      "Epoch 78/200\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.5420 - acc: 0.7378 - val_loss: 0.5526 - val_acc: 0.7240\n",
      "Epoch 79/200\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.5412 - acc: 0.7396 - val_loss: 0.5521 - val_acc: 0.7240\n",
      "Epoch 80/200\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.5404 - acc: 0.7378 - val_loss: 0.5515 - val_acc: 0.7292\n",
      "Epoch 81/200\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.5396 - acc: 0.7396 - val_loss: 0.5510 - val_acc: 0.7292\n",
      "Epoch 82/200\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.5388 - acc: 0.7413 - val_loss: 0.5504 - val_acc: 0.7292\n",
      "Epoch 83/200\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5381 - acc: 0.7413 - val_loss: 0.5499 - val_acc: 0.7292\n",
      "Epoch 84/200\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.5373 - acc: 0.7413 - val_loss: 0.5494 - val_acc: 0.7292\n",
      "Epoch 85/200\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.5366 - acc: 0.7413 - val_loss: 0.5488 - val_acc: 0.7292\n",
      "Epoch 86/200\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.5359 - acc: 0.7448 - val_loss: 0.5483 - val_acc: 0.7292\n",
      "Epoch 87/200\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.5351 - acc: 0.7431 - val_loss: 0.5478 - val_acc: 0.7292\n",
      "Epoch 88/200\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.5344 - acc: 0.7465 - val_loss: 0.5473 - val_acc: 0.7292\n",
      "Epoch 89/200\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.5337 - acc: 0.7431 - val_loss: 0.5468 - val_acc: 0.7344\n",
      "Epoch 90/200\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.5330 - acc: 0.7431 - val_loss: 0.5463 - val_acc: 0.7344\n",
      "Epoch 91/200\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.5323 - acc: 0.7448 - val_loss: 0.5459 - val_acc: 0.7344\n",
      "Epoch 92/200\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.5316 - acc: 0.7448 - val_loss: 0.5454 - val_acc: 0.7344\n",
      "Epoch 93/200\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5309 - acc: 0.7465 - val_loss: 0.5449 - val_acc: 0.7344\n",
      "Epoch 94/200\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.5302 - acc: 0.7465 - val_loss: 0.5445 - val_acc: 0.7344\n",
      "Epoch 95/200\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.5296 - acc: 0.7465 - val_loss: 0.5440 - val_acc: 0.7396\n",
      "Epoch 96/200\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.5289 - acc: 0.7465 - val_loss: 0.5435 - val_acc: 0.7396\n",
      "Epoch 97/200\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5283 - acc: 0.7465 - val_loss: 0.5431 - val_acc: 0.7396\n",
      "Epoch 98/200\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.5276 - acc: 0.7465 - val_loss: 0.5427 - val_acc: 0.7396\n",
      "Epoch 99/200\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.5270 - acc: 0.7465 - val_loss: 0.5422 - val_acc: 0.7396\n",
      "Epoch 100/200\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.5263 - acc: 0.7483 - val_loss: 0.5418 - val_acc: 0.7396\n",
      "Epoch 101/200\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.5257 - acc: 0.7517 - val_loss: 0.5414 - val_acc: 0.7396\n",
      "Epoch 102/200\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.5251 - acc: 0.7500 - val_loss: 0.5410 - val_acc: 0.7396\n",
      "Epoch 103/200\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.5245 - acc: 0.7500 - val_loss: 0.5405 - val_acc: 0.7396\n",
      "Epoch 104/200\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.5239 - acc: 0.7483 - val_loss: 0.5401 - val_acc: 0.7396\n",
      "Epoch 105/200\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.5233 - acc: 0.7500 - val_loss: 0.5397 - val_acc: 0.7396\n",
      "Epoch 106/200\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.5227 - acc: 0.7500 - val_loss: 0.5393 - val_acc: 0.7396\n",
      "Epoch 107/200\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.5222 - acc: 0.7483 - val_loss: 0.5389 - val_acc: 0.7396\n",
      "Epoch 108/200\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.5216 - acc: 0.7483 - val_loss: 0.5385 - val_acc: 0.7396\n",
      "Epoch 109/200\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.5210 - acc: 0.7483 - val_loss: 0.5381 - val_acc: 0.7396\n",
      "Epoch 110/200\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.5205 - acc: 0.7500 - val_loss: 0.5377 - val_acc: 0.7396\n",
      "Epoch 111/200\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.5199 - acc: 0.7500 - val_loss: 0.5374 - val_acc: 0.7396\n",
      "Epoch 112/200\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.5194 - acc: 0.7517 - val_loss: 0.5370 - val_acc: 0.7396\n",
      "Epoch 113/200\n",
      "576/576 [==============================] - 0s 80us/step - loss: 0.5188 - acc: 0.7517 - val_loss: 0.5366 - val_acc: 0.7396\n",
      "Epoch 114/200\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.5183 - acc: 0.7517 - val_loss: 0.5362 - val_acc: 0.7396\n",
      "Epoch 115/200\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5178 - acc: 0.7517 - val_loss: 0.5358 - val_acc: 0.7396\n",
      "Epoch 116/200\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.5172 - acc: 0.7517 - val_loss: 0.5355 - val_acc: 0.7396\n",
      "Epoch 117/200\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.5167 - acc: 0.7517 - val_loss: 0.5351 - val_acc: 0.7396\n",
      "Epoch 118/200\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.5162 - acc: 0.7517 - val_loss: 0.5348 - val_acc: 0.7396\n",
      "Epoch 119/200\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.5157 - acc: 0.7517 - val_loss: 0.5344 - val_acc: 0.7396\n",
      "Epoch 120/200\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.5152 - acc: 0.7517 - val_loss: 0.5341 - val_acc: 0.7396\n",
      "Epoch 121/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 52us/step - loss: 0.5147 - acc: 0.7517 - val_loss: 0.5337 - val_acc: 0.7396\n",
      "Epoch 122/200\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.5142 - acc: 0.7517 - val_loss: 0.5334 - val_acc: 0.7396\n",
      "Epoch 123/200\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.5137 - acc: 0.7500 - val_loss: 0.5330 - val_acc: 0.7396\n",
      "Epoch 124/200\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.5133 - acc: 0.7500 - val_loss: 0.5327 - val_acc: 0.7396\n",
      "Epoch 125/200\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.5127 - acc: 0.7500 - val_loss: 0.5324 - val_acc: 0.7396\n",
      "Epoch 126/200\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.5123 - acc: 0.7500 - val_loss: 0.5321 - val_acc: 0.7396\n",
      "Epoch 127/200\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.5118 - acc: 0.7500 - val_loss: 0.5317 - val_acc: 0.7396\n",
      "Epoch 128/200\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.5113 - acc: 0.7500 - val_loss: 0.5314 - val_acc: 0.7396\n",
      "Epoch 129/200\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.5109 - acc: 0.7517 - val_loss: 0.5311 - val_acc: 0.7396\n",
      "Epoch 130/200\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.5104 - acc: 0.7517 - val_loss: 0.5308 - val_acc: 0.7396\n",
      "Epoch 131/200\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.5100 - acc: 0.7517 - val_loss: 0.5305 - val_acc: 0.7396\n",
      "Epoch 132/200\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.5095 - acc: 0.7517 - val_loss: 0.5302 - val_acc: 0.7396\n",
      "Epoch 133/200\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.5091 - acc: 0.7500 - val_loss: 0.5299 - val_acc: 0.7396\n",
      "Epoch 134/200\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.5087 - acc: 0.7500 - val_loss: 0.5296 - val_acc: 0.7396\n",
      "Epoch 135/200\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.5083 - acc: 0.7500 - val_loss: 0.5293 - val_acc: 0.7396\n",
      "Epoch 136/200\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.5078 - acc: 0.7535 - val_loss: 0.5291 - val_acc: 0.7396\n",
      "Epoch 137/200\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.5074 - acc: 0.7500 - val_loss: 0.5288 - val_acc: 0.7396\n",
      "Epoch 138/200\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.5070 - acc: 0.7535 - val_loss: 0.5285 - val_acc: 0.7396\n",
      "Epoch 139/200\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.5066 - acc: 0.7517 - val_loss: 0.5282 - val_acc: 0.7344\n",
      "Epoch 140/200\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.5061 - acc: 0.7517 - val_loss: 0.5279 - val_acc: 0.7344\n",
      "Epoch 141/200\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.5058 - acc: 0.7552 - val_loss: 0.5277 - val_acc: 0.7344\n",
      "Epoch 142/200\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.5053 - acc: 0.7552 - val_loss: 0.5274 - val_acc: 0.7344\n",
      "Epoch 143/200\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.5049 - acc: 0.7552 - val_loss: 0.5271 - val_acc: 0.7344\n",
      "Epoch 144/200\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.5045 - acc: 0.7552 - val_loss: 0.5268 - val_acc: 0.7344\n",
      "Epoch 145/200\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.5041 - acc: 0.7552 - val_loss: 0.5265 - val_acc: 0.7344\n",
      "Epoch 146/200\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5037 - acc: 0.7552 - val_loss: 0.5263 - val_acc: 0.7344\n",
      "Epoch 147/200\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.5034 - acc: 0.7552 - val_loss: 0.5260 - val_acc: 0.7396\n",
      "Epoch 148/200\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.5030 - acc: 0.7517 - val_loss: 0.5257 - val_acc: 0.7396\n",
      "Epoch 149/200\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.5026 - acc: 0.7535 - val_loss: 0.5255 - val_acc: 0.7396\n",
      "Epoch 150/200\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5022 - acc: 0.7535 - val_loss: 0.5252 - val_acc: 0.7396\n",
      "Epoch 151/200\n",
      "576/576 [==============================] - 0s 102us/step - loss: 0.5018 - acc: 0.7535 - val_loss: 0.5249 - val_acc: 0.7396\n",
      "Epoch 152/200\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.5014 - acc: 0.7535 - val_loss: 0.5247 - val_acc: 0.7396\n",
      "Epoch 153/200\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.5011 - acc: 0.7535 - val_loss: 0.5244 - val_acc: 0.7396\n",
      "Epoch 154/200\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.5007 - acc: 0.7535 - val_loss: 0.5242 - val_acc: 0.7396\n",
      "Epoch 155/200\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.5003 - acc: 0.7535 - val_loss: 0.5239 - val_acc: 0.7396\n",
      "Epoch 156/200\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4999 - acc: 0.7535 - val_loss: 0.5237 - val_acc: 0.7396\n",
      "Epoch 157/200\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4996 - acc: 0.7517 - val_loss: 0.5235 - val_acc: 0.7396\n",
      "Epoch 158/200\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4992 - acc: 0.7517 - val_loss: 0.5232 - val_acc: 0.7396\n",
      "Epoch 159/200\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4988 - acc: 0.7500 - val_loss: 0.5230 - val_acc: 0.7396\n",
      "Epoch 160/200\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4985 - acc: 0.7517 - val_loss: 0.5228 - val_acc: 0.7396\n",
      "Epoch 161/200\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4981 - acc: 0.7517 - val_loss: 0.5225 - val_acc: 0.7344\n",
      "Epoch 162/200\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4978 - acc: 0.7517 - val_loss: 0.5223 - val_acc: 0.7344\n",
      "Epoch 163/200\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4974 - acc: 0.7517 - val_loss: 0.5221 - val_acc: 0.7344\n",
      "Epoch 164/200\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4971 - acc: 0.7517 - val_loss: 0.5219 - val_acc: 0.7344\n",
      "Epoch 165/200\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4967 - acc: 0.7517 - val_loss: 0.5217 - val_acc: 0.7344\n",
      "Epoch 166/200\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4964 - acc: 0.7517 - val_loss: 0.5215 - val_acc: 0.7344\n",
      "Epoch 167/200\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4961 - acc: 0.7552 - val_loss: 0.5213 - val_acc: 0.7344\n",
      "Epoch 168/200\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4957 - acc: 0.7552 - val_loss: 0.5211 - val_acc: 0.7344\n",
      "Epoch 169/200\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4954 - acc: 0.7552 - val_loss: 0.5209 - val_acc: 0.7344\n",
      "Epoch 170/200\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4951 - acc: 0.7552 - val_loss: 0.5206 - val_acc: 0.7292\n",
      "Epoch 171/200\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4947 - acc: 0.7552 - val_loss: 0.5205 - val_acc: 0.7292\n",
      "Epoch 172/200\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4944 - acc: 0.7552 - val_loss: 0.5202 - val_acc: 0.7292\n",
      "Epoch 173/200\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4941 - acc: 0.7552 - val_loss: 0.5201 - val_acc: 0.7292\n",
      "Epoch 174/200\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4938 - acc: 0.7552 - val_loss: 0.5199 - val_acc: 0.7292\n",
      "Epoch 175/200\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4935 - acc: 0.7552 - val_loss: 0.5197 - val_acc: 0.7292\n",
      "Epoch 176/200\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4932 - acc: 0.7552 - val_loss: 0.5195 - val_acc: 0.7292\n",
      "Epoch 177/200\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4928 - acc: 0.7569 - val_loss: 0.5193 - val_acc: 0.7292\n",
      "Epoch 178/200\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4925 - acc: 0.7569 - val_loss: 0.5191 - val_acc: 0.7292\n",
      "Epoch 179/200\n",
      "576/576 [==============================] - 0s 85us/step - loss: 0.4922 - acc: 0.7587 - val_loss: 0.5189 - val_acc: 0.7292\n",
      "Epoch 180/200\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4919 - acc: 0.7587 - val_loss: 0.5187 - val_acc: 0.7344\n",
      "Epoch 181/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 71us/step - loss: 0.4916 - acc: 0.7587 - val_loss: 0.5185 - val_acc: 0.7344\n",
      "Epoch 182/200\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4913 - acc: 0.7587 - val_loss: 0.5184 - val_acc: 0.7396\n",
      "Epoch 183/200\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4910 - acc: 0.7587 - val_loss: 0.5182 - val_acc: 0.7396\n",
      "Epoch 184/200\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4907 - acc: 0.7587 - val_loss: 0.5180 - val_acc: 0.7396\n",
      "Epoch 185/200\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4904 - acc: 0.7587 - val_loss: 0.5178 - val_acc: 0.7396\n",
      "Epoch 186/200\n",
      "576/576 [==============================] - 0s 102us/step - loss: 0.4901 - acc: 0.7587 - val_loss: 0.5176 - val_acc: 0.7396\n",
      "Epoch 187/200\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4898 - acc: 0.7587 - val_loss: 0.5174 - val_acc: 0.7396\n",
      "Epoch 188/200\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4895 - acc: 0.7587 - val_loss: 0.5173 - val_acc: 0.7448\n",
      "Epoch 189/200\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4892 - acc: 0.7587 - val_loss: 0.5171 - val_acc: 0.7448\n",
      "Epoch 190/200\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4889 - acc: 0.7604 - val_loss: 0.5169 - val_acc: 0.7500\n",
      "Epoch 191/200\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4886 - acc: 0.7604 - val_loss: 0.5167 - val_acc: 0.7500\n",
      "Epoch 192/200\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.6239 - acc: 0.718 - 0s 56us/step - loss: 0.4884 - acc: 0.7622 - val_loss: 0.5165 - val_acc: 0.7500\n",
      "Epoch 193/200\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4881 - acc: 0.7622 - val_loss: 0.5164 - val_acc: 0.7500\n",
      "Epoch 194/200\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4878 - acc: 0.7639 - val_loss: 0.5162 - val_acc: 0.7500\n",
      "Epoch 195/200\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4875 - acc: 0.7656 - val_loss: 0.5160 - val_acc: 0.7500\n",
      "Epoch 196/200\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4872 - acc: 0.7674 - val_loss: 0.5158 - val_acc: 0.7500\n",
      "Epoch 197/200\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4870 - acc: 0.7674 - val_loss: 0.5157 - val_acc: 0.7500\n",
      "Epoch 198/200\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4867 - acc: 0.7656 - val_loss: 0.5155 - val_acc: 0.7500\n",
      "Epoch 199/200\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4864 - acc: 0.7656 - val_loss: 0.5153 - val_acc: 0.7500\n",
      "Epoch 200/200\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4861 - acc: 0.7656 - val_loss: 0.5152 - val_acc: 0.7500\n"
     ]
    }
   ],
   "source": [
    "# Fit(Train) the Model\n",
    "\n",
    "# Compile the model with Optimizer, Loss Function and Metrics\n",
    "# Roc-Auc is not available in Keras as an off the shelf metric yet, so we will skip it here.\n",
    "\n",
    "model_1.compile(SGD(lr = .003), \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "run_hist_1 = model_1.fit(X_train_norm, y_train, validation_data=(X_test_norm, y_test), epochs=200)\n",
    "# the fit function returns the run history. \n",
    "# It is very convenient, as it contains information about the model fit, iterations etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Like we did for the Random Forest, we generate two kinds of predictions\n",
    "#  One is a hard decision, the other is a probabilitistic score.\n",
    "\n",
    "y_pred_class_nn_1 = model_1.predict_classes(X_test_norm)\n",
    "y_pred_prob_nn_1 = model_1.predict(X_test_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0]], dtype=int32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check out the outputs to get a feel for how keras apis work.\n",
    "y_pred_class_nn_1[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.45738238],\n",
       "       [0.7645373 ],\n",
       "       [0.34554237],\n",
       "       [0.2731436 ],\n",
       "       [0.21423137],\n",
       "       [0.65639967],\n",
       "       [0.05214481],\n",
       "       [0.4357455 ],\n",
       "       [0.7049758 ],\n",
       "       [0.26793998]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_prob_nn_1[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 0.750\n",
      "roc-auc is 0.809\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAecAAAHiCAYAAADSwATnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nnAEAiDMUZwAA4gzFGQCAOENxBgAgzvw/kp/zYQ1wrEgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10aa114a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Print model performance and plot the roc curve\n",
    "print('accuracy is {:.3f}'.format(accuracy_score(y_test,y_pred_class_nn_1)))\n",
    "print('roc-auc is {:.3f}'.format(roc_auc_score(y_test,y_pred_prob_nn_1)))\n",
    "\n",
    "plot_roc(y_test, y_pred_prob_nn_1, 'NN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There may be some variation in exact numbers due to randomness, but you should get results in the same ballpark as the Random Forest - between 75% and 85% accuracy, between .8 and .9 for AUC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the `run_hist_1` object that was created, specifically its `history` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_hist_1.history.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the training loss and the validation loss over the different epochs and see how it looks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1162f9f98>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkkcU9EVE8oiCvohIHlHQFxHJI/8fiZx/C/gftAIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1164629e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(run_hist_1.history[\"loss\"],'r', marker='.', label=\"Train Loss\")\n",
    "ax.plot(run_hist_1.history[\"val_loss\"],'b', marker='.', label=\"Validation Loss\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the losses are still going down on both the training set and the validation set.  This suggests that the model might benefit from further training.  Let's train the model a little more and see what happens. Note that it will pick up from where it left off. Train for 1000 more epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 576 samples, validate on 192 samples\n",
      "Epoch 1/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4859 - acc: 0.7656 - val_loss: 0.5150 - val_acc: 0.7500\n",
      "Epoch 2/1000\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.4856 - acc: 0.7656 - val_loss: 0.5148 - val_acc: 0.7448\n",
      "Epoch 3/1000\n",
      "576/576 [==============================] - 0s 115us/step - loss: 0.4853 - acc: 0.7656 - val_loss: 0.5147 - val_acc: 0.7448\n",
      "Epoch 4/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4851 - acc: 0.7656 - val_loss: 0.5145 - val_acc: 0.7448\n",
      "Epoch 5/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4848 - acc: 0.7656 - val_loss: 0.5144 - val_acc: 0.7448\n",
      "Epoch 6/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4846 - acc: 0.7656 - val_loss: 0.5142 - val_acc: 0.7448\n",
      "Epoch 7/1000\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.4843 - acc: 0.7656 - val_loss: 0.5141 - val_acc: 0.7448\n",
      "Epoch 8/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4840 - acc: 0.7656 - val_loss: 0.5139 - val_acc: 0.7448\n",
      "Epoch 9/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4838 - acc: 0.7656 - val_loss: 0.5138 - val_acc: 0.7448\n",
      "Epoch 10/1000\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.4835 - acc: 0.7656 - val_loss: 0.5136 - val_acc: 0.7448\n",
      "Epoch 11/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4833 - acc: 0.7674 - val_loss: 0.5135 - val_acc: 0.7448\n",
      "Epoch 12/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4830 - acc: 0.7674 - val_loss: 0.5133 - val_acc: 0.7448\n",
      "Epoch 13/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4827 - acc: 0.7674 - val_loss: 0.5132 - val_acc: 0.7448\n",
      "Epoch 14/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4825 - acc: 0.7674 - val_loss: 0.5130 - val_acc: 0.7448\n",
      "Epoch 15/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4822 - acc: 0.7656 - val_loss: 0.5129 - val_acc: 0.7396\n",
      "Epoch 16/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4820 - acc: 0.7674 - val_loss: 0.5127 - val_acc: 0.7396\n",
      "Epoch 17/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4817 - acc: 0.7674 - val_loss: 0.5126 - val_acc: 0.7396\n",
      "Epoch 18/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4815 - acc: 0.7691 - val_loss: 0.5125 - val_acc: 0.7396\n",
      "Epoch 19/1000\n",
      "576/576 [==============================] - 0s 82us/step - loss: 0.4812 - acc: 0.7691 - val_loss: 0.5123 - val_acc: 0.7396\n",
      "Epoch 20/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4810 - acc: 0.7691 - val_loss: 0.5122 - val_acc: 0.7396\n",
      "Epoch 21/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4807 - acc: 0.7674 - val_loss: 0.5120 - val_acc: 0.7396\n",
      "Epoch 22/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4805 - acc: 0.7691 - val_loss: 0.5119 - val_acc: 0.7396\n",
      "Epoch 23/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4803 - acc: 0.7656 - val_loss: 0.5118 - val_acc: 0.7396\n",
      "Epoch 24/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4800 - acc: 0.7656 - val_loss: 0.5116 - val_acc: 0.7396\n",
      "Epoch 25/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4798 - acc: 0.7674 - val_loss: 0.5115 - val_acc: 0.7396\n",
      "Epoch 26/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4795 - acc: 0.7639 - val_loss: 0.5113 - val_acc: 0.7396\n",
      "Epoch 27/1000\n",
      "576/576 [==============================] - 0s 131us/step - loss: 0.4793 - acc: 0.7639 - val_loss: 0.5112 - val_acc: 0.7396\n",
      "Epoch 28/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4791 - acc: 0.7639 - val_loss: 0.5111 - val_acc: 0.7396\n",
      "Epoch 29/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4788 - acc: 0.7639 - val_loss: 0.5109 - val_acc: 0.7344\n",
      "Epoch 30/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4786 - acc: 0.7656 - val_loss: 0.5108 - val_acc: 0.7344\n",
      "Epoch 31/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4784 - acc: 0.7674 - val_loss: 0.5107 - val_acc: 0.7344\n",
      "Epoch 32/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4781 - acc: 0.7674 - val_loss: 0.5106 - val_acc: 0.7344\n",
      "Epoch 33/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4779 - acc: 0.7674 - val_loss: 0.5104 - val_acc: 0.7344\n",
      "Epoch 34/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4777 - acc: 0.7674 - val_loss: 0.5103 - val_acc: 0.7344\n",
      "Epoch 35/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4774 - acc: 0.7674 - val_loss: 0.5102 - val_acc: 0.7344\n",
      "Epoch 36/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4772 - acc: 0.7674 - val_loss: 0.5100 - val_acc: 0.7344\n",
      "Epoch 37/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4770 - acc: 0.7674 - val_loss: 0.5099 - val_acc: 0.7292\n",
      "Epoch 38/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4768 - acc: 0.7674 - val_loss: 0.5098 - val_acc: 0.7292\n",
      "Epoch 39/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4765 - acc: 0.7656 - val_loss: 0.5097 - val_acc: 0.7292\n",
      "Epoch 40/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4763 - acc: 0.7674 - val_loss: 0.5096 - val_acc: 0.7292\n",
      "Epoch 41/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4761 - acc: 0.7656 - val_loss: 0.5095 - val_acc: 0.7292\n",
      "Epoch 42/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4759 - acc: 0.7656 - val_loss: 0.5093 - val_acc: 0.7292\n",
      "Epoch 43/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4757 - acc: 0.7656 - val_loss: 0.5092 - val_acc: 0.7292\n",
      "Epoch 44/1000\n",
      "576/576 [==============================] - 0s 115us/step - loss: 0.4754 - acc: 0.7656 - val_loss: 0.5091 - val_acc: 0.7292\n",
      "Epoch 45/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4753 - acc: 0.7656 - val_loss: 0.5090 - val_acc: 0.7292\n",
      "Epoch 46/1000\n",
      "576/576 [==============================] - 0s 88us/step - loss: 0.4751 - acc: 0.7656 - val_loss: 0.5089 - val_acc: 0.7292\n",
      "Epoch 47/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4749 - acc: 0.7656 - val_loss: 0.5088 - val_acc: 0.7292\n",
      "Epoch 48/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4747 - acc: 0.7656 - val_loss: 0.5087 - val_acc: 0.7292\n",
      "Epoch 49/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4744 - acc: 0.7656 - val_loss: 0.5086 - val_acc: 0.7292\n",
      "Epoch 50/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4742 - acc: 0.7656 - val_loss: 0.5085 - val_acc: 0.7292\n",
      "Epoch 51/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4741 - acc: 0.7656 - val_loss: 0.5084 - val_acc: 0.7292\n",
      "Epoch 52/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4738 - acc: 0.7656 - val_loss: 0.5083 - val_acc: 0.7292\n",
      "Epoch 53/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4737 - acc: 0.7656 - val_loss: 0.5082 - val_acc: 0.7292\n",
      "Epoch 54/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4734 - acc: 0.7639 - val_loss: 0.5081 - val_acc: 0.7292\n",
      "Epoch 55/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4732 - acc: 0.7639 - val_loss: 0.5080 - val_acc: 0.7292\n",
      "Epoch 56/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4730 - acc: 0.7639 - val_loss: 0.5079 - val_acc: 0.7292\n",
      "Epoch 57/1000\n",
      "576/576 [==============================] - 0s 93us/step - loss: 0.4729 - acc: 0.7639 - val_loss: 0.5079 - val_acc: 0.7292\n",
      "Epoch 58/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4727 - acc: 0.7639 - val_loss: 0.5078 - val_acc: 0.7292\n",
      "Epoch 59/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4725 - acc: 0.7639 - val_loss: 0.5077 - val_acc: 0.7292\n",
      "Epoch 60/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4723 - acc: 0.7639 - val_loss: 0.5076 - val_acc: 0.7292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4721 - acc: 0.7639 - val_loss: 0.5075 - val_acc: 0.7292\n",
      "Epoch 62/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4719 - acc: 0.7639 - val_loss: 0.5074 - val_acc: 0.7292\n",
      "Epoch 63/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4717 - acc: 0.7656 - val_loss: 0.5073 - val_acc: 0.7292\n",
      "Epoch 64/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4715 - acc: 0.7639 - val_loss: 0.5073 - val_acc: 0.7292\n",
      "Epoch 65/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4713 - acc: 0.7656 - val_loss: 0.5072 - val_acc: 0.7292\n",
      "Epoch 66/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4711 - acc: 0.7656 - val_loss: 0.5071 - val_acc: 0.7292\n",
      "Epoch 67/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4710 - acc: 0.7656 - val_loss: 0.5070 - val_acc: 0.7292\n",
      "Epoch 68/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4708 - acc: 0.7656 - val_loss: 0.5070 - val_acc: 0.7292\n",
      "Epoch 69/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4706 - acc: 0.7656 - val_loss: 0.5069 - val_acc: 0.7292\n",
      "Epoch 70/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4704 - acc: 0.7656 - val_loss: 0.5068 - val_acc: 0.7292\n",
      "Epoch 71/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4702 - acc: 0.7656 - val_loss: 0.5068 - val_acc: 0.7292\n",
      "Epoch 72/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4701 - acc: 0.7639 - val_loss: 0.5067 - val_acc: 0.7292\n",
      "Epoch 73/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4699 - acc: 0.7639 - val_loss: 0.5066 - val_acc: 0.7292\n",
      "Epoch 74/1000\n",
      "576/576 [==============================] - 0s 106us/step - loss: 0.4697 - acc: 0.7639 - val_loss: 0.5066 - val_acc: 0.7292\n",
      "Epoch 75/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4695 - acc: 0.7639 - val_loss: 0.5065 - val_acc: 0.7292\n",
      "Epoch 76/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4694 - acc: 0.7639 - val_loss: 0.5064 - val_acc: 0.7292\n",
      "Epoch 77/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4692 - acc: 0.7639 - val_loss: 0.5064 - val_acc: 0.7292\n",
      "Epoch 78/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4691 - acc: 0.7639 - val_loss: 0.5063 - val_acc: 0.7292\n",
      "Epoch 79/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4689 - acc: 0.7639 - val_loss: 0.5062 - val_acc: 0.7344\n",
      "Epoch 80/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4687 - acc: 0.7639 - val_loss: 0.5062 - val_acc: 0.7344\n",
      "Epoch 81/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4685 - acc: 0.7639 - val_loss: 0.5061 - val_acc: 0.7344\n",
      "Epoch 82/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4684 - acc: 0.7639 - val_loss: 0.5060 - val_acc: 0.7344\n",
      "Epoch 83/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4682 - acc: 0.7639 - val_loss: 0.5060 - val_acc: 0.7344\n",
      "Epoch 84/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4680 - acc: 0.7639 - val_loss: 0.5059 - val_acc: 0.7344\n",
      "Epoch 85/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4678 - acc: 0.7639 - val_loss: 0.5059 - val_acc: 0.7344\n",
      "Epoch 86/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4677 - acc: 0.7639 - val_loss: 0.5058 - val_acc: 0.7344\n",
      "Epoch 87/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4675 - acc: 0.7639 - val_loss: 0.5057 - val_acc: 0.7344\n",
      "Epoch 88/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4673 - acc: 0.7639 - val_loss: 0.5057 - val_acc: 0.7344\n",
      "Epoch 89/1000\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4672 - acc: 0.7639 - val_loss: 0.5056 - val_acc: 0.7344\n",
      "Epoch 90/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4670 - acc: 0.7656 - val_loss: 0.5056 - val_acc: 0.7292\n",
      "Epoch 91/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4668 - acc: 0.7656 - val_loss: 0.5055 - val_acc: 0.7292\n",
      "Epoch 92/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4667 - acc: 0.7656 - val_loss: 0.5055 - val_acc: 0.7292\n",
      "Epoch 93/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4665 - acc: 0.7656 - val_loss: 0.5054 - val_acc: 0.7292\n",
      "Epoch 94/1000\n",
      "576/576 [==============================] - 0s 87us/step - loss: 0.4663 - acc: 0.7674 - val_loss: 0.5054 - val_acc: 0.7292\n",
      "Epoch 95/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4662 - acc: 0.7674 - val_loss: 0.5053 - val_acc: 0.7292\n",
      "Epoch 96/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4660 - acc: 0.7674 - val_loss: 0.5052 - val_acc: 0.7292\n",
      "Epoch 97/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4659 - acc: 0.7674 - val_loss: 0.5052 - val_acc: 0.7292\n",
      "Epoch 98/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4657 - acc: 0.7674 - val_loss: 0.5051 - val_acc: 0.7292\n",
      "Epoch 99/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4655 - acc: 0.7674 - val_loss: 0.5051 - val_acc: 0.7292\n",
      "Epoch 100/1000\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.4654 - acc: 0.7674 - val_loss: 0.5050 - val_acc: 0.7292\n",
      "Epoch 101/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4652 - acc: 0.7691 - val_loss: 0.5050 - val_acc: 0.7292\n",
      "Epoch 102/1000\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.4650 - acc: 0.7691 - val_loss: 0.5049 - val_acc: 0.7292\n",
      "Epoch 103/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4649 - acc: 0.7691 - val_loss: 0.5049 - val_acc: 0.7292\n",
      "Epoch 104/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4647 - acc: 0.7691 - val_loss: 0.5048 - val_acc: 0.7292\n",
      "Epoch 105/1000\n",
      "576/576 [==============================] - 0s 79us/step - loss: 0.4646 - acc: 0.7691 - val_loss: 0.5048 - val_acc: 0.7292\n",
      "Epoch 106/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4644 - acc: 0.7691 - val_loss: 0.5047 - val_acc: 0.7292\n",
      "Epoch 107/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4643 - acc: 0.7708 - val_loss: 0.5047 - val_acc: 0.7292\n",
      "Epoch 108/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4641 - acc: 0.7708 - val_loss: 0.5046 - val_acc: 0.7292\n",
      "Epoch 109/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4639 - acc: 0.7708 - val_loss: 0.5046 - val_acc: 0.7292\n",
      "Epoch 110/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4638 - acc: 0.7708 - val_loss: 0.5045 - val_acc: 0.7292\n",
      "Epoch 111/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4637 - acc: 0.7708 - val_loss: 0.5045 - val_acc: 0.7292\n",
      "Epoch 112/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4635 - acc: 0.7708 - val_loss: 0.5044 - val_acc: 0.7292\n",
      "Epoch 113/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4634 - acc: 0.7708 - val_loss: 0.5044 - val_acc: 0.7292\n",
      "Epoch 114/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4632 - acc: 0.7708 - val_loss: 0.5043 - val_acc: 0.7292\n",
      "Epoch 115/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4631 - acc: 0.7708 - val_loss: 0.5043 - val_acc: 0.7292\n",
      "Epoch 116/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4629 - acc: 0.7708 - val_loss: 0.5042 - val_acc: 0.7292\n",
      "Epoch 117/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4628 - acc: 0.7708 - val_loss: 0.5042 - val_acc: 0.7292\n",
      "Epoch 118/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4626 - acc: 0.7708 - val_loss: 0.5041 - val_acc: 0.7292\n",
      "Epoch 119/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4625 - acc: 0.7726 - val_loss: 0.5041 - val_acc: 0.7292\n",
      "Epoch 120/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4623 - acc: 0.7708 - val_loss: 0.5040 - val_acc: 0.7292\n",
      "Epoch 121/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 49us/step - loss: 0.4622 - acc: 0.7708 - val_loss: 0.5040 - val_acc: 0.7292\n",
      "Epoch 122/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4620 - acc: 0.7708 - val_loss: 0.5040 - val_acc: 0.7292\n",
      "Epoch 123/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4619 - acc: 0.7708 - val_loss: 0.5039 - val_acc: 0.7292\n",
      "Epoch 124/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4618 - acc: 0.7708 - val_loss: 0.5039 - val_acc: 0.7292\n",
      "Epoch 125/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4616 - acc: 0.7708 - val_loss: 0.5038 - val_acc: 0.7292\n",
      "Epoch 126/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4615 - acc: 0.7708 - val_loss: 0.5038 - val_acc: 0.7240\n",
      "Epoch 127/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4613 - acc: 0.7708 - val_loss: 0.5038 - val_acc: 0.7240\n",
      "Epoch 128/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4612 - acc: 0.7708 - val_loss: 0.5037 - val_acc: 0.7240\n",
      "Epoch 129/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4611 - acc: 0.7708 - val_loss: 0.5037 - val_acc: 0.7240\n",
      "Epoch 130/1000\n",
      "576/576 [==============================] - 0s 96us/step - loss: 0.4609 - acc: 0.7708 - val_loss: 0.5037 - val_acc: 0.7240\n",
      "Epoch 131/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4608 - acc: 0.7708 - val_loss: 0.5036 - val_acc: 0.7240\n",
      "Epoch 132/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4607 - acc: 0.7708 - val_loss: 0.5036 - val_acc: 0.7240\n",
      "Epoch 133/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4606 - acc: 0.7708 - val_loss: 0.5036 - val_acc: 0.7240\n",
      "Epoch 134/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4604 - acc: 0.7708 - val_loss: 0.5035 - val_acc: 0.7240\n",
      "Epoch 135/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4603 - acc: 0.7708 - val_loss: 0.5035 - val_acc: 0.7240\n",
      "Epoch 136/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4602 - acc: 0.7708 - val_loss: 0.5035 - val_acc: 0.7240\n",
      "Epoch 137/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4601 - acc: 0.7708 - val_loss: 0.5034 - val_acc: 0.7240\n",
      "Epoch 138/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4599 - acc: 0.7708 - val_loss: 0.5034 - val_acc: 0.7240\n",
      "Epoch 139/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4598 - acc: 0.7708 - val_loss: 0.5034 - val_acc: 0.7240\n",
      "Epoch 140/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4597 - acc: 0.7708 - val_loss: 0.5033 - val_acc: 0.7240\n",
      "Epoch 141/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4595 - acc: 0.7708 - val_loss: 0.5033 - val_acc: 0.7240\n",
      "Epoch 142/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4594 - acc: 0.7708 - val_loss: 0.5033 - val_acc: 0.7188\n",
      "Epoch 143/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4593 - acc: 0.7708 - val_loss: 0.5032 - val_acc: 0.7188\n",
      "Epoch 144/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4592 - acc: 0.7708 - val_loss: 0.5032 - val_acc: 0.7188\n",
      "Epoch 145/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4590 - acc: 0.7708 - val_loss: 0.5032 - val_acc: 0.7188\n",
      "Epoch 146/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4589 - acc: 0.7708 - val_loss: 0.5031 - val_acc: 0.7188\n",
      "Epoch 147/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4588 - acc: 0.7726 - val_loss: 0.5031 - val_acc: 0.7188\n",
      "Epoch 148/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4587 - acc: 0.7726 - val_loss: 0.5031 - val_acc: 0.7188\n",
      "Epoch 149/1000\n",
      "576/576 [==============================] - 0s 84us/step - loss: 0.4586 - acc: 0.7726 - val_loss: 0.5030 - val_acc: 0.7188\n",
      "Epoch 150/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4585 - acc: 0.7726 - val_loss: 0.5030 - val_acc: 0.7188\n",
      "Epoch 151/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4583 - acc: 0.7726 - val_loss: 0.5030 - val_acc: 0.7188\n",
      "Epoch 152/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4582 - acc: 0.7726 - val_loss: 0.5030 - val_acc: 0.7188\n",
      "Epoch 153/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4581 - acc: 0.7743 - val_loss: 0.5029 - val_acc: 0.7188\n",
      "Epoch 154/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4580 - acc: 0.7743 - val_loss: 0.5029 - val_acc: 0.7188\n",
      "Epoch 155/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4579 - acc: 0.7743 - val_loss: 0.5029 - val_acc: 0.7188\n",
      "Epoch 156/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4578 - acc: 0.7743 - val_loss: 0.5028 - val_acc: 0.7188\n",
      "Epoch 157/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4576 - acc: 0.7743 - val_loss: 0.5028 - val_acc: 0.7188\n",
      "Epoch 158/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4575 - acc: 0.7760 - val_loss: 0.5028 - val_acc: 0.7188\n",
      "Epoch 159/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4574 - acc: 0.7760 - val_loss: 0.5027 - val_acc: 0.7188\n",
      "Epoch 160/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4573 - acc: 0.7778 - val_loss: 0.5027 - val_acc: 0.7188\n",
      "Epoch 161/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4572 - acc: 0.7795 - val_loss: 0.5027 - val_acc: 0.7188\n",
      "Epoch 162/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4571 - acc: 0.7778 - val_loss: 0.5027 - val_acc: 0.7188\n",
      "Epoch 163/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4570 - acc: 0.7812 - val_loss: 0.5026 - val_acc: 0.7188\n",
      "Epoch 164/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4568 - acc: 0.7812 - val_loss: 0.5026 - val_acc: 0.7188\n",
      "Epoch 165/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4567 - acc: 0.7812 - val_loss: 0.5026 - val_acc: 0.7188\n",
      "Epoch 166/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4566 - acc: 0.7812 - val_loss: 0.5026 - val_acc: 0.7188\n",
      "Epoch 167/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4565 - acc: 0.7812 - val_loss: 0.5025 - val_acc: 0.7188\n",
      "Epoch 168/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4564 - acc: 0.7812 - val_loss: 0.5025 - val_acc: 0.7188\n",
      "Epoch 169/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4563 - acc: 0.7795 - val_loss: 0.5025 - val_acc: 0.7188\n",
      "Epoch 170/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4562 - acc: 0.7795 - val_loss: 0.5024 - val_acc: 0.7188\n",
      "Epoch 171/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4561 - acc: 0.7778 - val_loss: 0.5024 - val_acc: 0.7188\n",
      "Epoch 172/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4560 - acc: 0.7795 - val_loss: 0.5024 - val_acc: 0.7188\n",
      "Epoch 173/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4559 - acc: 0.7795 - val_loss: 0.5024 - val_acc: 0.7188\n",
      "Epoch 174/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4558 - acc: 0.7778 - val_loss: 0.5023 - val_acc: 0.7188\n",
      "Epoch 175/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4557 - acc: 0.7795 - val_loss: 0.5023 - val_acc: 0.7188\n",
      "Epoch 176/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4556 - acc: 0.7778 - val_loss: 0.5023 - val_acc: 0.7188\n",
      "Epoch 177/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4555 - acc: 0.7795 - val_loss: 0.5022 - val_acc: 0.7188\n",
      "Epoch 178/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4554 - acc: 0.7812 - val_loss: 0.5022 - val_acc: 0.7188\n",
      "Epoch 179/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4553 - acc: 0.7812 - val_loss: 0.5022 - val_acc: 0.7188\n",
      "Epoch 180/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4552 - acc: 0.7778 - val_loss: 0.5022 - val_acc: 0.7188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 181/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4551 - acc: 0.7778 - val_loss: 0.5021 - val_acc: 0.7188\n",
      "Epoch 182/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4550 - acc: 0.7795 - val_loss: 0.5021 - val_acc: 0.7135\n",
      "Epoch 183/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4549 - acc: 0.7795 - val_loss: 0.5021 - val_acc: 0.7135\n",
      "Epoch 184/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4548 - acc: 0.7778 - val_loss: 0.5020 - val_acc: 0.7135\n",
      "Epoch 185/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4547 - acc: 0.7795 - val_loss: 0.5020 - val_acc: 0.7135\n",
      "Epoch 186/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4546 - acc: 0.7778 - val_loss: 0.5020 - val_acc: 0.7135\n",
      "Epoch 187/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4545 - acc: 0.7795 - val_loss: 0.5020 - val_acc: 0.7135\n",
      "Epoch 188/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4544 - acc: 0.7778 - val_loss: 0.5019 - val_acc: 0.7135\n",
      "Epoch 189/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4543 - acc: 0.7778 - val_loss: 0.5019 - val_acc: 0.7135\n",
      "Epoch 190/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4542 - acc: 0.7778 - val_loss: 0.5019 - val_acc: 0.7135\n",
      "Epoch 191/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4541 - acc: 0.7778 - val_loss: 0.5019 - val_acc: 0.7135\n",
      "Epoch 192/1000\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.4541 - acc: 0.7778 - val_loss: 0.5018 - val_acc: 0.7135\n",
      "Epoch 193/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4540 - acc: 0.7778 - val_loss: 0.5018 - val_acc: 0.7135\n",
      "Epoch 194/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4539 - acc: 0.7778 - val_loss: 0.5018 - val_acc: 0.7135\n",
      "Epoch 195/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4538 - acc: 0.7778 - val_loss: 0.5018 - val_acc: 0.7135\n",
      "Epoch 196/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4537 - acc: 0.7778 - val_loss: 0.5017 - val_acc: 0.7135\n",
      "Epoch 197/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4536 - acc: 0.7778 - val_loss: 0.5017 - val_acc: 0.7135\n",
      "Epoch 198/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4535 - acc: 0.7778 - val_loss: 0.5017 - val_acc: 0.7135\n",
      "Epoch 199/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4534 - acc: 0.7778 - val_loss: 0.5017 - val_acc: 0.7135\n",
      "Epoch 200/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4533 - acc: 0.7778 - val_loss: 0.5017 - val_acc: 0.7135\n",
      "Epoch 201/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4532 - acc: 0.7778 - val_loss: 0.5016 - val_acc: 0.7135\n",
      "Epoch 202/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4532 - acc: 0.7778 - val_loss: 0.5016 - val_acc: 0.7135\n",
      "Epoch 203/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4531 - acc: 0.7778 - val_loss: 0.5016 - val_acc: 0.7135\n",
      "Epoch 204/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4530 - acc: 0.7778 - val_loss: 0.5016 - val_acc: 0.7135\n",
      "Epoch 205/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4529 - acc: 0.7778 - val_loss: 0.5016 - val_acc: 0.7135\n",
      "Epoch 206/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4528 - acc: 0.7778 - val_loss: 0.5016 - val_acc: 0.7135\n",
      "Epoch 207/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4527 - acc: 0.7778 - val_loss: 0.5015 - val_acc: 0.7135\n",
      "Epoch 208/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4527 - acc: 0.7778 - val_loss: 0.5015 - val_acc: 0.7135\n",
      "Epoch 209/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4526 - acc: 0.7778 - val_loss: 0.5015 - val_acc: 0.7135\n",
      "Epoch 210/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4525 - acc: 0.7795 - val_loss: 0.5015 - val_acc: 0.7135\n",
      "Epoch 211/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4524 - acc: 0.7778 - val_loss: 0.5015 - val_acc: 0.7135\n",
      "Epoch 212/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4524 - acc: 0.7795 - val_loss: 0.5015 - val_acc: 0.7135\n",
      "Epoch 213/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4523 - acc: 0.7795 - val_loss: 0.5014 - val_acc: 0.7135\n",
      "Epoch 214/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4522 - acc: 0.7795 - val_loss: 0.5014 - val_acc: 0.7135\n",
      "Epoch 215/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4521 - acc: 0.7812 - val_loss: 0.5014 - val_acc: 0.7135\n",
      "Epoch 216/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4520 - acc: 0.7812 - val_loss: 0.5014 - val_acc: 0.7135\n",
      "Epoch 217/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4519 - acc: 0.7812 - val_loss: 0.5014 - val_acc: 0.7135\n",
      "Epoch 218/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4518 - acc: 0.7812 - val_loss: 0.5014 - val_acc: 0.7135\n",
      "Epoch 219/1000\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.6319 - acc: 0.562 - 0s 49us/step - loss: 0.4518 - acc: 0.7812 - val_loss: 0.5013 - val_acc: 0.7135\n",
      "Epoch 220/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4517 - acc: 0.7812 - val_loss: 0.5013 - val_acc: 0.7135\n",
      "Epoch 221/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4516 - acc: 0.7812 - val_loss: 0.5013 - val_acc: 0.7135\n",
      "Epoch 222/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4515 - acc: 0.7812 - val_loss: 0.5013 - val_acc: 0.7135\n",
      "Epoch 223/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4515 - acc: 0.7812 - val_loss: 0.5013 - val_acc: 0.7135\n",
      "Epoch 224/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4514 - acc: 0.7812 - val_loss: 0.5012 - val_acc: 0.7135\n",
      "Epoch 225/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4513 - acc: 0.7812 - val_loss: 0.5012 - val_acc: 0.7135\n",
      "Epoch 226/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4512 - acc: 0.7812 - val_loss: 0.5012 - val_acc: 0.7135\n",
      "Epoch 227/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4512 - acc: 0.7812 - val_loss: 0.5012 - val_acc: 0.7135\n",
      "Epoch 228/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4511 - acc: 0.7812 - val_loss: 0.5012 - val_acc: 0.7135\n",
      "Epoch 229/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4510 - acc: 0.7812 - val_loss: 0.5012 - val_acc: 0.7135\n",
      "Epoch 230/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4509 - acc: 0.7812 - val_loss: 0.5011 - val_acc: 0.7135\n",
      "Epoch 231/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4508 - acc: 0.7812 - val_loss: 0.5011 - val_acc: 0.7135\n",
      "Epoch 232/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4508 - acc: 0.7812 - val_loss: 0.5011 - val_acc: 0.7135\n",
      "Epoch 233/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4507 - acc: 0.7812 - val_loss: 0.5011 - val_acc: 0.7135\n",
      "Epoch 234/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4506 - acc: 0.7830 - val_loss: 0.5011 - val_acc: 0.7135\n",
      "Epoch 235/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4505 - acc: 0.7830 - val_loss: 0.5011 - val_acc: 0.7135\n",
      "Epoch 236/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4505 - acc: 0.7830 - val_loss: 0.5010 - val_acc: 0.7135\n",
      "Epoch 237/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4504 - acc: 0.7830 - val_loss: 0.5010 - val_acc: 0.7135\n",
      "Epoch 238/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4503 - acc: 0.7847 - val_loss: 0.5010 - val_acc: 0.7135\n",
      "Epoch 239/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4502 - acc: 0.7847 - val_loss: 0.5010 - val_acc: 0.7135\n",
      "Epoch 240/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 50us/step - loss: 0.4501 - acc: 0.7847 - val_loss: 0.5010 - val_acc: 0.7188\n",
      "Epoch 241/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4501 - acc: 0.7847 - val_loss: 0.5009 - val_acc: 0.7188\n",
      "Epoch 242/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4500 - acc: 0.7847 - val_loss: 0.5009 - val_acc: 0.7240\n",
      "Epoch 243/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4499 - acc: 0.7847 - val_loss: 0.5009 - val_acc: 0.7240\n",
      "Epoch 244/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4499 - acc: 0.7847 - val_loss: 0.5009 - val_acc: 0.7240\n",
      "Epoch 245/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4498 - acc: 0.7847 - val_loss: 0.5009 - val_acc: 0.7240\n",
      "Epoch 246/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4497 - acc: 0.7847 - val_loss: 0.5009 - val_acc: 0.7240\n",
      "Epoch 247/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4496 - acc: 0.7847 - val_loss: 0.5009 - val_acc: 0.7240\n",
      "Epoch 248/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4495 - acc: 0.7847 - val_loss: 0.5008 - val_acc: 0.7240\n",
      "Epoch 249/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4495 - acc: 0.7847 - val_loss: 0.5008 - val_acc: 0.7240\n",
      "Epoch 250/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4494 - acc: 0.7847 - val_loss: 0.5008 - val_acc: 0.7240\n",
      "Epoch 251/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4494 - acc: 0.7847 - val_loss: 0.5008 - val_acc: 0.7292\n",
      "Epoch 252/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4493 - acc: 0.7847 - val_loss: 0.5008 - val_acc: 0.7292\n",
      "Epoch 253/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4492 - acc: 0.7847 - val_loss: 0.5008 - val_acc: 0.7292\n",
      "Epoch 254/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4492 - acc: 0.7847 - val_loss: 0.5008 - val_acc: 0.7292\n",
      "Epoch 255/1000\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4491 - acc: 0.7847 - val_loss: 0.5008 - val_acc: 0.7292\n",
      "Epoch 256/1000\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4490 - acc: 0.7847 - val_loss: 0.5007 - val_acc: 0.7292\n",
      "Epoch 257/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4490 - acc: 0.7847 - val_loss: 0.5007 - val_acc: 0.7292\n",
      "Epoch 258/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4489 - acc: 0.7847 - val_loss: 0.5007 - val_acc: 0.7292\n",
      "Epoch 259/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4488 - acc: 0.7847 - val_loss: 0.5007 - val_acc: 0.7292\n",
      "Epoch 260/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4487 - acc: 0.7847 - val_loss: 0.5007 - val_acc: 0.7292\n",
      "Epoch 261/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4487 - acc: 0.7847 - val_loss: 0.5007 - val_acc: 0.7292\n",
      "Epoch 262/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4486 - acc: 0.7847 - val_loss: 0.5007 - val_acc: 0.7292\n",
      "Epoch 263/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4485 - acc: 0.7847 - val_loss: 0.5006 - val_acc: 0.7292\n",
      "Epoch 264/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4485 - acc: 0.7847 - val_loss: 0.5006 - val_acc: 0.7292\n",
      "Epoch 265/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4484 - acc: 0.7847 - val_loss: 0.5006 - val_acc: 0.7292\n",
      "Epoch 266/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4483 - acc: 0.7847 - val_loss: 0.5006 - val_acc: 0.7292\n",
      "Epoch 267/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4483 - acc: 0.7847 - val_loss: 0.5006 - val_acc: 0.7292\n",
      "Epoch 268/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4482 - acc: 0.7847 - val_loss: 0.5006 - val_acc: 0.7292\n",
      "Epoch 269/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4482 - acc: 0.7847 - val_loss: 0.5006 - val_acc: 0.7292\n",
      "Epoch 270/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4481 - acc: 0.7847 - val_loss: 0.5006 - val_acc: 0.7292\n",
      "Epoch 271/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4480 - acc: 0.7847 - val_loss: 0.5005 - val_acc: 0.7292\n",
      "Epoch 272/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4480 - acc: 0.7847 - val_loss: 0.5005 - val_acc: 0.7292\n",
      "Epoch 273/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4479 - acc: 0.7847 - val_loss: 0.5005 - val_acc: 0.7292\n",
      "Epoch 274/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4478 - acc: 0.7847 - val_loss: 0.5005 - val_acc: 0.7292\n",
      "Epoch 275/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4478 - acc: 0.7847 - val_loss: 0.5005 - val_acc: 0.7292\n",
      "Epoch 276/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4477 - acc: 0.7847 - val_loss: 0.5005 - val_acc: 0.7292\n",
      "Epoch 277/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4476 - acc: 0.7830 - val_loss: 0.5005 - val_acc: 0.7292\n",
      "Epoch 278/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4476 - acc: 0.7847 - val_loss: 0.5005 - val_acc: 0.7292\n",
      "Epoch 279/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4475 - acc: 0.7830 - val_loss: 0.5005 - val_acc: 0.7292\n",
      "Epoch 280/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4474 - acc: 0.7830 - val_loss: 0.5004 - val_acc: 0.7292\n",
      "Epoch 281/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4474 - acc: 0.7830 - val_loss: 0.5004 - val_acc: 0.7292\n",
      "Epoch 282/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4473 - acc: 0.7830 - val_loss: 0.5004 - val_acc: 0.7292\n",
      "Epoch 283/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4473 - acc: 0.7847 - val_loss: 0.5004 - val_acc: 0.7292\n",
      "Epoch 284/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4472 - acc: 0.7847 - val_loss: 0.5004 - val_acc: 0.7292\n",
      "Epoch 285/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4472 - acc: 0.7830 - val_loss: 0.5004 - val_acc: 0.7292\n",
      "Epoch 286/1000\n",
      "576/576 [==============================] - 0s 80us/step - loss: 0.4471 - acc: 0.7847 - val_loss: 0.5004 - val_acc: 0.7292\n",
      "Epoch 287/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4470 - acc: 0.7847 - val_loss: 0.5004 - val_acc: 0.7292\n",
      "Epoch 288/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4470 - acc: 0.7865 - val_loss: 0.5004 - val_acc: 0.7292\n",
      "Epoch 289/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4469 - acc: 0.7865 - val_loss: 0.5003 - val_acc: 0.7292\n",
      "Epoch 290/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4468 - acc: 0.7865 - val_loss: 0.5003 - val_acc: 0.7292\n",
      "Epoch 291/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4468 - acc: 0.7865 - val_loss: 0.5003 - val_acc: 0.7292\n",
      "Epoch 292/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4467 - acc: 0.7865 - val_loss: 0.5003 - val_acc: 0.7292\n",
      "Epoch 293/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4467 - acc: 0.7865 - val_loss: 0.5003 - val_acc: 0.7292\n",
      "Epoch 294/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4466 - acc: 0.7865 - val_loss: 0.5003 - val_acc: 0.7292\n",
      "Epoch 295/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4465 - acc: 0.7865 - val_loss: 0.5003 - val_acc: 0.7292\n",
      "Epoch 296/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4465 - acc: 0.7865 - val_loss: 0.5003 - val_acc: 0.7292\n",
      "Epoch 297/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4464 - acc: 0.7865 - val_loss: 0.5003 - val_acc: 0.7292\n",
      "Epoch 298/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4464 - acc: 0.7865 - val_loss: 0.5003 - val_acc: 0.7292\n",
      "Epoch 299/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4463 - acc: 0.7865 - val_loss: 0.5003 - val_acc: 0.7292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 300/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4462 - acc: 0.7865 - val_loss: 0.5002 - val_acc: 0.7292\n",
      "Epoch 301/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4462 - acc: 0.7865 - val_loss: 0.5002 - val_acc: 0.7292\n",
      "Epoch 302/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4461 - acc: 0.7865 - val_loss: 0.5002 - val_acc: 0.7292\n",
      "Epoch 303/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4460 - acc: 0.7865 - val_loss: 0.5002 - val_acc: 0.7292\n",
      "Epoch 304/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4460 - acc: 0.7865 - val_loss: 0.5002 - val_acc: 0.7292\n",
      "Epoch 305/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4459 - acc: 0.7865 - val_loss: 0.5002 - val_acc: 0.7292\n",
      "Epoch 306/1000\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4459 - acc: 0.7865 - val_loss: 0.5002 - val_acc: 0.7292\n",
      "Epoch 307/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4458 - acc: 0.7865 - val_loss: 0.5002 - val_acc: 0.7292\n",
      "Epoch 308/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4458 - acc: 0.7865 - val_loss: 0.5002 - val_acc: 0.7292\n",
      "Epoch 309/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4457 - acc: 0.7865 - val_loss: 0.5002 - val_acc: 0.7344\n",
      "Epoch 310/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4456 - acc: 0.7865 - val_loss: 0.5002 - val_acc: 0.7344\n",
      "Epoch 311/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4456 - acc: 0.7865 - val_loss: 0.5002 - val_acc: 0.7344\n",
      "Epoch 312/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4455 - acc: 0.7865 - val_loss: 0.5002 - val_acc: 0.7344\n",
      "Epoch 313/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4455 - acc: 0.7865 - val_loss: 0.5001 - val_acc: 0.7344\n",
      "Epoch 314/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4454 - acc: 0.7865 - val_loss: 0.5001 - val_acc: 0.7344\n",
      "Epoch 315/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4454 - acc: 0.7865 - val_loss: 0.5001 - val_acc: 0.7344\n",
      "Epoch 316/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4453 - acc: 0.7882 - val_loss: 0.5001 - val_acc: 0.7344\n",
      "Epoch 317/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4453 - acc: 0.7865 - val_loss: 0.5001 - val_acc: 0.7344\n",
      "Epoch 318/1000\n",
      "576/576 [==============================] - 0s 86us/step - loss: 0.4452 - acc: 0.7865 - val_loss: 0.5001 - val_acc: 0.7344\n",
      "Epoch 319/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4452 - acc: 0.7865 - val_loss: 0.5001 - val_acc: 0.7344\n",
      "Epoch 320/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4451 - acc: 0.7882 - val_loss: 0.5001 - val_acc: 0.7344\n",
      "Epoch 321/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4450 - acc: 0.7899 - val_loss: 0.5001 - val_acc: 0.7344\n",
      "Epoch 322/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4450 - acc: 0.7882 - val_loss: 0.5001 - val_acc: 0.7344\n",
      "Epoch 323/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4449 - acc: 0.7899 - val_loss: 0.5001 - val_acc: 0.7344\n",
      "Epoch 324/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4449 - acc: 0.7882 - val_loss: 0.5001 - val_acc: 0.7396\n",
      "Epoch 325/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4448 - acc: 0.7899 - val_loss: 0.5001 - val_acc: 0.7396\n",
      "Epoch 326/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4448 - acc: 0.7899 - val_loss: 0.5001 - val_acc: 0.7396\n",
      "Epoch 327/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4447 - acc: 0.7899 - val_loss: 0.5001 - val_acc: 0.7396\n",
      "Epoch 328/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4447 - acc: 0.7899 - val_loss: 0.5001 - val_acc: 0.7396\n",
      "Epoch 329/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4446 - acc: 0.7899 - val_loss: 0.5001 - val_acc: 0.7396\n",
      "Epoch 330/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4446 - acc: 0.7899 - val_loss: 0.5001 - val_acc: 0.7396\n",
      "Epoch 331/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4445 - acc: 0.7899 - val_loss: 0.5001 - val_acc: 0.7396\n",
      "Epoch 332/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4445 - acc: 0.7899 - val_loss: 0.5001 - val_acc: 0.7396\n",
      "Epoch 333/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4444 - acc: 0.7899 - val_loss: 0.5001 - val_acc: 0.7396\n",
      "Epoch 334/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4444 - acc: 0.7899 - val_loss: 0.5000 - val_acc: 0.7396\n",
      "Epoch 335/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4443 - acc: 0.7899 - val_loss: 0.5000 - val_acc: 0.7396\n",
      "Epoch 336/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4443 - acc: 0.7899 - val_loss: 0.5000 - val_acc: 0.7396\n",
      "Epoch 337/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4442 - acc: 0.7899 - val_loss: 0.5000 - val_acc: 0.7396\n",
      "Epoch 338/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4442 - acc: 0.7899 - val_loss: 0.5000 - val_acc: 0.7396\n",
      "Epoch 339/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4441 - acc: 0.7899 - val_loss: 0.5000 - val_acc: 0.7396\n",
      "Epoch 340/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4441 - acc: 0.7899 - val_loss: 0.5000 - val_acc: 0.7396\n",
      "Epoch 341/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4440 - acc: 0.7899 - val_loss: 0.5000 - val_acc: 0.7396\n",
      "Epoch 342/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4440 - acc: 0.7899 - val_loss: 0.5000 - val_acc: 0.7396\n",
      "Epoch 343/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4439 - acc: 0.7899 - val_loss: 0.5000 - val_acc: 0.7396\n",
      "Epoch 344/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4439 - acc: 0.7882 - val_loss: 0.5000 - val_acc: 0.7396\n",
      "Epoch 345/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4438 - acc: 0.7882 - val_loss: 0.5000 - val_acc: 0.7396\n",
      "Epoch 346/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4438 - acc: 0.7847 - val_loss: 0.5000 - val_acc: 0.7396\n",
      "Epoch 347/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4437 - acc: 0.7847 - val_loss: 0.5000 - val_acc: 0.7396\n",
      "Epoch 348/1000\n",
      "576/576 [==============================] - 0s 77us/step - loss: 0.4437 - acc: 0.7847 - val_loss: 0.5000 - val_acc: 0.7396\n",
      "Epoch 349/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4436 - acc: 0.7847 - val_loss: 0.4999 - val_acc: 0.7396\n",
      "Epoch 350/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4436 - acc: 0.7847 - val_loss: 0.4999 - val_acc: 0.7396\n",
      "Epoch 351/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4435 - acc: 0.7847 - val_loss: 0.4999 - val_acc: 0.7396\n",
      "Epoch 352/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4435 - acc: 0.7847 - val_loss: 0.4999 - val_acc: 0.7396\n",
      "Epoch 353/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4435 - acc: 0.7847 - val_loss: 0.4999 - val_acc: 0.7396\n",
      "Epoch 354/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4434 - acc: 0.7847 - val_loss: 0.4999 - val_acc: 0.7396\n",
      "Epoch 355/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4434 - acc: 0.7847 - val_loss: 0.4999 - val_acc: 0.7396\n",
      "Epoch 356/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4433 - acc: 0.7847 - val_loss: 0.4999 - val_acc: 0.7396\n",
      "Epoch 357/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4433 - acc: 0.7847 - val_loss: 0.4999 - val_acc: 0.7396\n",
      "Epoch 358/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4432 - acc: 0.7847 - val_loss: 0.4999 - val_acc: 0.7396\n",
      "Epoch 359/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4432 - acc: 0.7847 - val_loss: 0.4999 - val_acc: 0.7396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 360/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4431 - acc: 0.7847 - val_loss: 0.4999 - val_acc: 0.7396\n",
      "Epoch 361/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4431 - acc: 0.7847 - val_loss: 0.4999 - val_acc: 0.7396\n",
      "Epoch 362/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4430 - acc: 0.7847 - val_loss: 0.4999 - val_acc: 0.7396\n",
      "Epoch 363/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4430 - acc: 0.7847 - val_loss: 0.4998 - val_acc: 0.7448\n",
      "Epoch 364/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4429 - acc: 0.7847 - val_loss: 0.4998 - val_acc: 0.7448\n",
      "Epoch 365/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4429 - acc: 0.7847 - val_loss: 0.4998 - val_acc: 0.7448\n",
      "Epoch 366/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4429 - acc: 0.7847 - val_loss: 0.4998 - val_acc: 0.7448\n",
      "Epoch 367/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4428 - acc: 0.7847 - val_loss: 0.4998 - val_acc: 0.7448\n",
      "Epoch 368/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4428 - acc: 0.7847 - val_loss: 0.4998 - val_acc: 0.7448\n",
      "Epoch 369/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4427 - acc: 0.7830 - val_loss: 0.4998 - val_acc: 0.7448\n",
      "Epoch 370/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4427 - acc: 0.7847 - val_loss: 0.4998 - val_acc: 0.7448\n",
      "Epoch 371/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4426 - acc: 0.7847 - val_loss: 0.4998 - val_acc: 0.7448\n",
      "Epoch 372/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4426 - acc: 0.7830 - val_loss: 0.4998 - val_acc: 0.7448\n",
      "Epoch 373/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4426 - acc: 0.7847 - val_loss: 0.4998 - val_acc: 0.7448\n",
      "Epoch 374/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4425 - acc: 0.7830 - val_loss: 0.4998 - val_acc: 0.7448\n",
      "Epoch 375/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4424 - acc: 0.7830 - val_loss: 0.4997 - val_acc: 0.7448\n",
      "Epoch 376/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4424 - acc: 0.7830 - val_loss: 0.4997 - val_acc: 0.7448\n",
      "Epoch 377/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4424 - acc: 0.7830 - val_loss: 0.4997 - val_acc: 0.7448\n",
      "Epoch 378/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4423 - acc: 0.7830 - val_loss: 0.4997 - val_acc: 0.7448\n",
      "Epoch 379/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4423 - acc: 0.7830 - val_loss: 0.4997 - val_acc: 0.7448\n",
      "Epoch 380/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4422 - acc: 0.7865 - val_loss: 0.4997 - val_acc: 0.7448\n",
      "Epoch 381/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4422 - acc: 0.7865 - val_loss: 0.4997 - val_acc: 0.7448\n",
      "Epoch 382/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4422 - acc: 0.7865 - val_loss: 0.4997 - val_acc: 0.7448\n",
      "Epoch 383/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4421 - acc: 0.7847 - val_loss: 0.4997 - val_acc: 0.7448\n",
      "Epoch 384/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4421 - acc: 0.7865 - val_loss: 0.4997 - val_acc: 0.7448\n",
      "Epoch 385/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4420 - acc: 0.7865 - val_loss: 0.4997 - val_acc: 0.7448\n",
      "Epoch 386/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4420 - acc: 0.7865 - val_loss: 0.4997 - val_acc: 0.7448\n",
      "Epoch 387/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4420 - acc: 0.7865 - val_loss: 0.4997 - val_acc: 0.7448\n",
      "Epoch 388/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4419 - acc: 0.7865 - val_loss: 0.4997 - val_acc: 0.7448\n",
      "Epoch 389/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4419 - acc: 0.7865 - val_loss: 0.4997 - val_acc: 0.7448\n",
      "Epoch 390/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4419 - acc: 0.7865 - val_loss: 0.4997 - val_acc: 0.7448\n",
      "Epoch 391/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4418 - acc: 0.7865 - val_loss: 0.4997 - val_acc: 0.7448\n",
      "Epoch 392/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4417 - acc: 0.7865 - val_loss: 0.4996 - val_acc: 0.7448\n",
      "Epoch 393/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4417 - acc: 0.7865 - val_loss: 0.4996 - val_acc: 0.7448\n",
      "Epoch 394/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4417 - acc: 0.7865 - val_loss: 0.4996 - val_acc: 0.7448\n",
      "Epoch 395/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4416 - acc: 0.7865 - val_loss: 0.4996 - val_acc: 0.7448\n",
      "Epoch 396/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4416 - acc: 0.7865 - val_loss: 0.4996 - val_acc: 0.7448\n",
      "Epoch 397/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4416 - acc: 0.7865 - val_loss: 0.4996 - val_acc: 0.7448\n",
      "Epoch 398/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4415 - acc: 0.7865 - val_loss: 0.4996 - val_acc: 0.7448\n",
      "Epoch 399/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4415 - acc: 0.7882 - val_loss: 0.4996 - val_acc: 0.7448\n",
      "Epoch 400/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4414 - acc: 0.7882 - val_loss: 0.4996 - val_acc: 0.7448\n",
      "Epoch 401/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4414 - acc: 0.7882 - val_loss: 0.4996 - val_acc: 0.7448\n",
      "Epoch 402/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4414 - acc: 0.7882 - val_loss: 0.4996 - val_acc: 0.7448\n",
      "Epoch 403/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4413 - acc: 0.7882 - val_loss: 0.4996 - val_acc: 0.7448\n",
      "Epoch 404/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4413 - acc: 0.7882 - val_loss: 0.4996 - val_acc: 0.7448\n",
      "Epoch 405/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4412 - acc: 0.7882 - val_loss: 0.4996 - val_acc: 0.7448\n",
      "Epoch 406/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4412 - acc: 0.7882 - val_loss: 0.4996 - val_acc: 0.7448\n",
      "Epoch 407/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4412 - acc: 0.7882 - val_loss: 0.4995 - val_acc: 0.7448\n",
      "Epoch 408/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4411 - acc: 0.7899 - val_loss: 0.4995 - val_acc: 0.7448\n",
      "Epoch 409/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4411 - acc: 0.7899 - val_loss: 0.4995 - val_acc: 0.7448\n",
      "Epoch 410/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4410 - acc: 0.7882 - val_loss: 0.4995 - val_acc: 0.7448\n",
      "Epoch 411/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4410 - acc: 0.7882 - val_loss: 0.4995 - val_acc: 0.7448\n",
      "Epoch 412/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4410 - acc: 0.7899 - val_loss: 0.4995 - val_acc: 0.7448\n",
      "Epoch 413/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4409 - acc: 0.7899 - val_loss: 0.4995 - val_acc: 0.7448\n",
      "Epoch 414/1000\n",
      "576/576 [==============================] - 0s 77us/step - loss: 0.4409 - acc: 0.7899 - val_loss: 0.4995 - val_acc: 0.7448\n",
      "Epoch 415/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4408 - acc: 0.7882 - val_loss: 0.4995 - val_acc: 0.7448\n",
      "Epoch 416/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4408 - acc: 0.7882 - val_loss: 0.4995 - val_acc: 0.7448\n",
      "Epoch 417/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4408 - acc: 0.7899 - val_loss: 0.4995 - val_acc: 0.7448\n",
      "Epoch 418/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4407 - acc: 0.7882 - val_loss: 0.4995 - val_acc: 0.7448\n",
      "Epoch 419/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4407 - acc: 0.7899 - val_loss: 0.4995 - val_acc: 0.7396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 420/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4406 - acc: 0.7899 - val_loss: 0.4995 - val_acc: 0.7396\n",
      "Epoch 421/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4406 - acc: 0.7899 - val_loss: 0.4995 - val_acc: 0.7396\n",
      "Epoch 422/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4406 - acc: 0.7899 - val_loss: 0.4994 - val_acc: 0.7396\n",
      "Epoch 423/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4406 - acc: 0.7899 - val_loss: 0.4994 - val_acc: 0.7396\n",
      "Epoch 424/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4405 - acc: 0.7899 - val_loss: 0.4994 - val_acc: 0.7396\n",
      "Epoch 425/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4405 - acc: 0.7899 - val_loss: 0.4994 - val_acc: 0.7396\n",
      "Epoch 426/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4404 - acc: 0.7899 - val_loss: 0.4994 - val_acc: 0.7396\n",
      "Epoch 427/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4404 - acc: 0.7899 - val_loss: 0.4994 - val_acc: 0.7396\n",
      "Epoch 428/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4404 - acc: 0.7899 - val_loss: 0.4994 - val_acc: 0.7396\n",
      "Epoch 429/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4403 - acc: 0.7899 - val_loss: 0.4994 - val_acc: 0.7396\n",
      "Epoch 430/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4403 - acc: 0.7899 - val_loss: 0.4994 - val_acc: 0.7396\n",
      "Epoch 431/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4402 - acc: 0.7899 - val_loss: 0.4994 - val_acc: 0.7396\n",
      "Epoch 432/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4402 - acc: 0.7899 - val_loss: 0.4994 - val_acc: 0.7396\n",
      "Epoch 433/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4402 - acc: 0.7882 - val_loss: 0.4994 - val_acc: 0.7396\n",
      "Epoch 434/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4401 - acc: 0.7882 - val_loss: 0.4994 - val_acc: 0.7448\n",
      "Epoch 435/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4401 - acc: 0.7899 - val_loss: 0.4994 - val_acc: 0.7448\n",
      "Epoch 436/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4401 - acc: 0.7899 - val_loss: 0.4994 - val_acc: 0.7448\n",
      "Epoch 437/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4400 - acc: 0.7899 - val_loss: 0.4994 - val_acc: 0.7448\n",
      "Epoch 438/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4400 - acc: 0.7899 - val_loss: 0.4994 - val_acc: 0.7448\n",
      "Epoch 439/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4400 - acc: 0.7899 - val_loss: 0.4994 - val_acc: 0.7448\n",
      "Epoch 440/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4399 - acc: 0.7899 - val_loss: 0.4994 - val_acc: 0.7448\n",
      "Epoch 441/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4399 - acc: 0.7899 - val_loss: 0.4994 - val_acc: 0.7448\n",
      "Epoch 442/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4399 - acc: 0.7899 - val_loss: 0.4994 - val_acc: 0.7448\n",
      "Epoch 443/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4398 - acc: 0.7882 - val_loss: 0.4994 - val_acc: 0.7448\n",
      "Epoch 444/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4398 - acc: 0.7899 - val_loss: 0.4993 - val_acc: 0.7448\n",
      "Epoch 445/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4398 - acc: 0.7882 - val_loss: 0.4993 - val_acc: 0.7448\n",
      "Epoch 446/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4397 - acc: 0.7882 - val_loss: 0.4993 - val_acc: 0.7448\n",
      "Epoch 447/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4397 - acc: 0.7882 - val_loss: 0.4993 - val_acc: 0.7448\n",
      "Epoch 448/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4396 - acc: 0.7882 - val_loss: 0.4993 - val_acc: 0.7448\n",
      "Epoch 449/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4396 - acc: 0.7865 - val_loss: 0.4993 - val_acc: 0.7448\n",
      "Epoch 450/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4396 - acc: 0.7882 - val_loss: 0.4993 - val_acc: 0.7448\n",
      "Epoch 451/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4396 - acc: 0.7882 - val_loss: 0.4993 - val_acc: 0.7448\n",
      "Epoch 452/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4395 - acc: 0.7865 - val_loss: 0.4993 - val_acc: 0.7448\n",
      "Epoch 453/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4395 - acc: 0.7865 - val_loss: 0.4993 - val_acc: 0.7448\n",
      "Epoch 454/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4395 - acc: 0.7865 - val_loss: 0.4993 - val_acc: 0.7448\n",
      "Epoch 455/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4394 - acc: 0.7865 - val_loss: 0.4993 - val_acc: 0.7448\n",
      "Epoch 456/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4394 - acc: 0.7865 - val_loss: 0.4993 - val_acc: 0.7448\n",
      "Epoch 457/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4394 - acc: 0.7847 - val_loss: 0.4993 - val_acc: 0.7448\n",
      "Epoch 458/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4393 - acc: 0.7847 - val_loss: 0.4993 - val_acc: 0.7448\n",
      "Epoch 459/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4393 - acc: 0.7847 - val_loss: 0.4992 - val_acc: 0.7448\n",
      "Epoch 460/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4393 - acc: 0.7830 - val_loss: 0.4992 - val_acc: 0.7448\n",
      "Epoch 461/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4392 - acc: 0.7830 - val_loss: 0.4992 - val_acc: 0.7448\n",
      "Epoch 462/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4392 - acc: 0.7865 - val_loss: 0.4992 - val_acc: 0.7448\n",
      "Epoch 463/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4392 - acc: 0.7847 - val_loss: 0.4992 - val_acc: 0.7448\n",
      "Epoch 464/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4391 - acc: 0.7865 - val_loss: 0.4992 - val_acc: 0.7448\n",
      "Epoch 465/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4391 - acc: 0.7847 - val_loss: 0.4992 - val_acc: 0.7448\n",
      "Epoch 466/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4391 - acc: 0.7865 - val_loss: 0.4992 - val_acc: 0.7448\n",
      "Epoch 467/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4390 - acc: 0.7847 - val_loss: 0.4992 - val_acc: 0.7448\n",
      "Epoch 468/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4390 - acc: 0.7847 - val_loss: 0.4992 - val_acc: 0.7448\n",
      "Epoch 469/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4389 - acc: 0.7865 - val_loss: 0.4992 - val_acc: 0.7448\n",
      "Epoch 470/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4389 - acc: 0.7865 - val_loss: 0.4992 - val_acc: 0.7448\n",
      "Epoch 471/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4389 - acc: 0.7865 - val_loss: 0.4991 - val_acc: 0.7448\n",
      "Epoch 472/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4389 - acc: 0.7865 - val_loss: 0.4991 - val_acc: 0.7448\n",
      "Epoch 473/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4388 - acc: 0.7865 - val_loss: 0.4991 - val_acc: 0.7448\n",
      "Epoch 474/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4388 - acc: 0.7847 - val_loss: 0.4991 - val_acc: 0.7448\n",
      "Epoch 475/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4388 - acc: 0.7865 - val_loss: 0.4991 - val_acc: 0.7448\n",
      "Epoch 476/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4387 - acc: 0.7865 - val_loss: 0.4991 - val_acc: 0.7448\n",
      "Epoch 477/1000\n",
      "576/576 [==============================] - 0s 77us/step - loss: 0.4387 - acc: 0.7865 - val_loss: 0.4991 - val_acc: 0.7448\n",
      "Epoch 478/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4387 - acc: 0.7865 - val_loss: 0.4991 - val_acc: 0.7448\n",
      "Epoch 479/1000\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4386 - acc: 0.7865 - val_loss: 0.4991 - val_acc: 0.7448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 480/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4386 - acc: 0.7865 - val_loss: 0.4991 - val_acc: 0.7448\n",
      "Epoch 481/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4386 - acc: 0.7865 - val_loss: 0.4991 - val_acc: 0.7448\n",
      "Epoch 482/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4385 - acc: 0.7882 - val_loss: 0.4991 - val_acc: 0.7448\n",
      "Epoch 483/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4385 - acc: 0.7865 - val_loss: 0.4991 - val_acc: 0.7448\n",
      "Epoch 484/1000\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4385 - acc: 0.7882 - val_loss: 0.4991 - val_acc: 0.7448\n",
      "Epoch 485/1000\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.4784 - acc: 0.781 - 0s 58us/step - loss: 0.4385 - acc: 0.7865 - val_loss: 0.4990 - val_acc: 0.7448\n",
      "Epoch 486/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4385 - acc: 0.7882 - val_loss: 0.4990 - val_acc: 0.7448\n",
      "Epoch 487/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4384 - acc: 0.7865 - val_loss: 0.4990 - val_acc: 0.7448\n",
      "Epoch 488/1000\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.4384 - acc: 0.7865 - val_loss: 0.4990 - val_acc: 0.7448\n",
      "Epoch 489/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4383 - acc: 0.7865 - val_loss: 0.4990 - val_acc: 0.7448\n",
      "Epoch 490/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4383 - acc: 0.7865 - val_loss: 0.4990 - val_acc: 0.7448\n",
      "Epoch 491/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4383 - acc: 0.7865 - val_loss: 0.4990 - val_acc: 0.7448\n",
      "Epoch 492/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4382 - acc: 0.7865 - val_loss: 0.4990 - val_acc: 0.7448\n",
      "Epoch 493/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4382 - acc: 0.7865 - val_loss: 0.4990 - val_acc: 0.7448\n",
      "Epoch 494/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4382 - acc: 0.7865 - val_loss: 0.4990 - val_acc: 0.7448\n",
      "Epoch 495/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4382 - acc: 0.7865 - val_loss: 0.4990 - val_acc: 0.7448\n",
      "Epoch 496/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4382 - acc: 0.7865 - val_loss: 0.4990 - val_acc: 0.7448\n",
      "Epoch 497/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4381 - acc: 0.7865 - val_loss: 0.4990 - val_acc: 0.7448\n",
      "Epoch 498/1000\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4381 - acc: 0.7865 - val_loss: 0.4990 - val_acc: 0.7448\n",
      "Epoch 499/1000\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4381 - acc: 0.7865 - val_loss: 0.4989 - val_acc: 0.7448\n",
      "Epoch 500/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4380 - acc: 0.7865 - val_loss: 0.4989 - val_acc: 0.7448\n",
      "Epoch 501/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4380 - acc: 0.7865 - val_loss: 0.4989 - val_acc: 0.7448\n",
      "Epoch 502/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4380 - acc: 0.7865 - val_loss: 0.4989 - val_acc: 0.7448\n",
      "Epoch 503/1000\n",
      "576/576 [==============================] - 0s 102us/step - loss: 0.4379 - acc: 0.7865 - val_loss: 0.4989 - val_acc: 0.7448\n",
      "Epoch 504/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4379 - acc: 0.7865 - val_loss: 0.4989 - val_acc: 0.7448\n",
      "Epoch 505/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4379 - acc: 0.7865 - val_loss: 0.4989 - val_acc: 0.7448\n",
      "Epoch 506/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4379 - acc: 0.7865 - val_loss: 0.4989 - val_acc: 0.7448\n",
      "Epoch 507/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4378 - acc: 0.7865 - val_loss: 0.4989 - val_acc: 0.7448\n",
      "Epoch 508/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4378 - acc: 0.7865 - val_loss: 0.4989 - val_acc: 0.7448\n",
      "Epoch 509/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4378 - acc: 0.7865 - val_loss: 0.4989 - val_acc: 0.7448\n",
      "Epoch 510/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4378 - acc: 0.7865 - val_loss: 0.4989 - val_acc: 0.7448\n",
      "Epoch 511/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4377 - acc: 0.7865 - val_loss: 0.4989 - val_acc: 0.7448\n",
      "Epoch 512/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4377 - acc: 0.7865 - val_loss: 0.4989 - val_acc: 0.7448\n",
      "Epoch 513/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4377 - acc: 0.7865 - val_loss: 0.4989 - val_acc: 0.7500\n",
      "Epoch 514/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4376 - acc: 0.7865 - val_loss: 0.4989 - val_acc: 0.7500\n",
      "Epoch 515/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4376 - acc: 0.7865 - val_loss: 0.4989 - val_acc: 0.7500\n",
      "Epoch 516/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4376 - acc: 0.7865 - val_loss: 0.4989 - val_acc: 0.7500\n",
      "Epoch 517/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4375 - acc: 0.7865 - val_loss: 0.4989 - val_acc: 0.7500\n",
      "Epoch 518/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4375 - acc: 0.7882 - val_loss: 0.4989 - val_acc: 0.7500\n",
      "Epoch 519/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4375 - acc: 0.7882 - val_loss: 0.4989 - val_acc: 0.7500\n",
      "Epoch 520/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4375 - acc: 0.7882 - val_loss: 0.4988 - val_acc: 0.7500\n",
      "Epoch 521/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4374 - acc: 0.7882 - val_loss: 0.4988 - val_acc: 0.7500\n",
      "Epoch 522/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4374 - acc: 0.7882 - val_loss: 0.4988 - val_acc: 0.7500\n",
      "Epoch 523/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4374 - acc: 0.7882 - val_loss: 0.4988 - val_acc: 0.7500\n",
      "Epoch 524/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4374 - acc: 0.7882 - val_loss: 0.4988 - val_acc: 0.7500\n",
      "Epoch 525/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4373 - acc: 0.7882 - val_loss: 0.4988 - val_acc: 0.7500\n",
      "Epoch 526/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4373 - acc: 0.7882 - val_loss: 0.4988 - val_acc: 0.7500\n",
      "Epoch 527/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4373 - acc: 0.7882 - val_loss: 0.4988 - val_acc: 0.7500\n",
      "Epoch 528/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4373 - acc: 0.7882 - val_loss: 0.4988 - val_acc: 0.7500\n",
      "Epoch 529/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4372 - acc: 0.7882 - val_loss: 0.4988 - val_acc: 0.7500\n",
      "Epoch 530/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4372 - acc: 0.7882 - val_loss: 0.4988 - val_acc: 0.7500\n",
      "Epoch 531/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4372 - acc: 0.7882 - val_loss: 0.4988 - val_acc: 0.7500\n",
      "Epoch 532/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4371 - acc: 0.7882 - val_loss: 0.4988 - val_acc: 0.7500\n",
      "Epoch 533/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4371 - acc: 0.7882 - val_loss: 0.4988 - val_acc: 0.7500\n",
      "Epoch 534/1000\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.4371 - acc: 0.7882 - val_loss: 0.4988 - val_acc: 0.7500\n",
      "Epoch 535/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4371 - acc: 0.7882 - val_loss: 0.4988 - val_acc: 0.7500\n",
      "Epoch 536/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4370 - acc: 0.7882 - val_loss: 0.4988 - val_acc: 0.7500\n",
      "Epoch 537/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4370 - acc: 0.7882 - val_loss: 0.4988 - val_acc: 0.7500\n",
      "Epoch 538/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4370 - acc: 0.7882 - val_loss: 0.4988 - val_acc: 0.7500\n",
      "Epoch 539/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 48us/step - loss: 0.4370 - acc: 0.7882 - val_loss: 0.4988 - val_acc: 0.7500\n",
      "Epoch 540/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4369 - acc: 0.7882 - val_loss: 0.4988 - val_acc: 0.7552\n",
      "Epoch 541/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4369 - acc: 0.7882 - val_loss: 0.4988 - val_acc: 0.7552\n",
      "Epoch 542/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4368 - acc: 0.7882 - val_loss: 0.4988 - val_acc: 0.7552\n",
      "Epoch 543/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4368 - acc: 0.7882 - val_loss: 0.4988 - val_acc: 0.7552\n",
      "Epoch 544/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4368 - acc: 0.7882 - val_loss: 0.4988 - val_acc: 0.7552\n",
      "Epoch 545/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4368 - acc: 0.7882 - val_loss: 0.4988 - val_acc: 0.7552\n",
      "Epoch 546/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4367 - acc: 0.7882 - val_loss: 0.4988 - val_acc: 0.7552\n",
      "Epoch 547/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4367 - acc: 0.7882 - val_loss: 0.4988 - val_acc: 0.7552\n",
      "Epoch 548/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4367 - acc: 0.7882 - val_loss: 0.4988 - val_acc: 0.7552\n",
      "Epoch 549/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4367 - acc: 0.7882 - val_loss: 0.4988 - val_acc: 0.7552\n",
      "Epoch 550/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4366 - acc: 0.7882 - val_loss: 0.4988 - val_acc: 0.7552\n",
      "Epoch 551/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4366 - acc: 0.7882 - val_loss: 0.4988 - val_acc: 0.7552\n",
      "Epoch 552/1000\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.4366 - acc: 0.7882 - val_loss: 0.4988 - val_acc: 0.7552\n",
      "Epoch 553/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4365 - acc: 0.7882 - val_loss: 0.4988 - val_acc: 0.7552\n",
      "Epoch 554/1000\n",
      "576/576 [==============================] - 0s 97us/step - loss: 0.4365 - acc: 0.7882 - val_loss: 0.4987 - val_acc: 0.7552\n",
      "Epoch 555/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4365 - acc: 0.7882 - val_loss: 0.4988 - val_acc: 0.7552\n",
      "Epoch 556/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4365 - acc: 0.7882 - val_loss: 0.4987 - val_acc: 0.7552\n",
      "Epoch 557/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4364 - acc: 0.7882 - val_loss: 0.4988 - val_acc: 0.7552\n",
      "Epoch 558/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4364 - acc: 0.7882 - val_loss: 0.4987 - val_acc: 0.7552\n",
      "Epoch 559/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4364 - acc: 0.7882 - val_loss: 0.4987 - val_acc: 0.7552\n",
      "Epoch 560/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4363 - acc: 0.7882 - val_loss: 0.4987 - val_acc: 0.7552\n",
      "Epoch 561/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4364 - acc: 0.7882 - val_loss: 0.4987 - val_acc: 0.7552\n",
      "Epoch 562/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4363 - acc: 0.7882 - val_loss: 0.4987 - val_acc: 0.7552\n",
      "Epoch 563/1000\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4363 - acc: 0.7882 - val_loss: 0.4987 - val_acc: 0.7552\n",
      "Epoch 564/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4362 - acc: 0.7882 - val_loss: 0.4987 - val_acc: 0.7552\n",
      "Epoch 565/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4362 - acc: 0.7882 - val_loss: 0.4987 - val_acc: 0.7552\n",
      "Epoch 566/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4362 - acc: 0.7882 - val_loss: 0.4987 - val_acc: 0.7552\n",
      "Epoch 567/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4362 - acc: 0.7882 - val_loss: 0.4987 - val_acc: 0.7552\n",
      "Epoch 568/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4361 - acc: 0.7882 - val_loss: 0.4987 - val_acc: 0.7552\n",
      "Epoch 569/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4361 - acc: 0.7882 - val_loss: 0.4987 - val_acc: 0.7552\n",
      "Epoch 570/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4361 - acc: 0.7882 - val_loss: 0.4987 - val_acc: 0.7552\n",
      "Epoch 571/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4360 - acc: 0.7882 - val_loss: 0.4987 - val_acc: 0.7552\n",
      "Epoch 572/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4360 - acc: 0.7882 - val_loss: 0.4987 - val_acc: 0.7552\n",
      "Epoch 573/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4360 - acc: 0.7882 - val_loss: 0.4987 - val_acc: 0.7552\n",
      "Epoch 574/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4360 - acc: 0.7882 - val_loss: 0.4987 - val_acc: 0.7552\n",
      "Epoch 575/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4359 - acc: 0.7882 - val_loss: 0.4987 - val_acc: 0.7552\n",
      "Epoch 576/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4359 - acc: 0.7882 - val_loss: 0.4987 - val_acc: 0.7500\n",
      "Epoch 577/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4359 - acc: 0.7882 - val_loss: 0.4987 - val_acc: 0.7500\n",
      "Epoch 578/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4359 - acc: 0.7882 - val_loss: 0.4987 - val_acc: 0.7500\n",
      "Epoch 579/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4358 - acc: 0.7882 - val_loss: 0.4987 - val_acc: 0.7500\n",
      "Epoch 580/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4358 - acc: 0.7882 - val_loss: 0.4987 - val_acc: 0.7500\n",
      "Epoch 581/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4358 - acc: 0.7882 - val_loss: 0.4987 - val_acc: 0.7500\n",
      "Epoch 582/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4358 - acc: 0.7882 - val_loss: 0.4987 - val_acc: 0.7500\n",
      "Epoch 583/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4357 - acc: 0.7882 - val_loss: 0.4987 - val_acc: 0.7500\n",
      "Epoch 584/1000\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4357 - acc: 0.7882 - val_loss: 0.4987 - val_acc: 0.7500\n",
      "Epoch 585/1000\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4357 - acc: 0.7882 - val_loss: 0.4987 - val_acc: 0.7500\n",
      "Epoch 586/1000\n",
      "576/576 [==============================] - 0s 82us/step - loss: 0.4356 - acc: 0.7882 - val_loss: 0.4987 - val_acc: 0.7500\n",
      "Epoch 587/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4356 - acc: 0.7882 - val_loss: 0.4987 - val_acc: 0.7500\n",
      "Epoch 588/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4356 - acc: 0.7882 - val_loss: 0.4987 - val_acc: 0.7500\n",
      "Epoch 589/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4356 - acc: 0.7882 - val_loss: 0.4987 - val_acc: 0.7500\n",
      "Epoch 590/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4355 - acc: 0.7882 - val_loss: 0.4987 - val_acc: 0.7500\n",
      "Epoch 591/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4355 - acc: 0.7882 - val_loss: 0.4987 - val_acc: 0.7500\n",
      "Epoch 592/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4355 - acc: 0.7882 - val_loss: 0.4987 - val_acc: 0.7500\n",
      "Epoch 593/1000\n",
      "576/576 [==============================] - 0s 88us/step - loss: 0.4355 - acc: 0.7882 - val_loss: 0.4987 - val_acc: 0.7500\n",
      "Epoch 594/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4354 - acc: 0.7882 - val_loss: 0.4987 - val_acc: 0.7500\n",
      "Epoch 595/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4354 - acc: 0.7882 - val_loss: 0.4987 - val_acc: 0.7500\n",
      "Epoch 596/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4354 - acc: 0.7882 - val_loss: 0.4987 - val_acc: 0.7500\n",
      "Epoch 597/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4354 - acc: 0.7882 - val_loss: 0.4987 - val_acc: 0.7500\n",
      "Epoch 598/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4353 - acc: 0.7882 - val_loss: 0.4987 - val_acc: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 599/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4353 - acc: 0.7882 - val_loss: 0.4987 - val_acc: 0.7500\n",
      "Epoch 600/1000\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.4353 - acc: 0.7882 - val_loss: 0.4986 - val_acc: 0.7500\n",
      "Epoch 601/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4353 - acc: 0.7882 - val_loss: 0.4987 - val_acc: 0.7500\n",
      "Epoch 602/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4352 - acc: 0.7882 - val_loss: 0.4986 - val_acc: 0.7500\n",
      "Epoch 603/1000\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4352 - acc: 0.7882 - val_loss: 0.4986 - val_acc: 0.7500\n",
      "Epoch 604/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4352 - acc: 0.7882 - val_loss: 0.4986 - val_acc: 0.7500\n",
      "Epoch 605/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4351 - acc: 0.7882 - val_loss: 0.4986 - val_acc: 0.7500\n",
      "Epoch 606/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4351 - acc: 0.7899 - val_loss: 0.4986 - val_acc: 0.7500\n",
      "Epoch 607/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4351 - acc: 0.7882 - val_loss: 0.4986 - val_acc: 0.7500\n",
      "Epoch 608/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4351 - acc: 0.7882 - val_loss: 0.4986 - val_acc: 0.7500\n",
      "Epoch 609/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4350 - acc: 0.7899 - val_loss: 0.4986 - val_acc: 0.7500\n",
      "Epoch 610/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4350 - acc: 0.7899 - val_loss: 0.4986 - val_acc: 0.7500\n",
      "Epoch 611/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4350 - acc: 0.7899 - val_loss: 0.4986 - val_acc: 0.7500\n",
      "Epoch 612/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4349 - acc: 0.7899 - val_loss: 0.4986 - val_acc: 0.7500\n",
      "Epoch 613/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4349 - acc: 0.7899 - val_loss: 0.4986 - val_acc: 0.7500\n",
      "Epoch 614/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4349 - acc: 0.7882 - val_loss: 0.4986 - val_acc: 0.7500\n",
      "Epoch 615/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4349 - acc: 0.7899 - val_loss: 0.4986 - val_acc: 0.7500\n",
      "Epoch 616/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4349 - acc: 0.7899 - val_loss: 0.4986 - val_acc: 0.7500\n",
      "Epoch 617/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4348 - acc: 0.7882 - val_loss: 0.4986 - val_acc: 0.7500\n",
      "Epoch 618/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4348 - acc: 0.7899 - val_loss: 0.4986 - val_acc: 0.7500\n",
      "Epoch 619/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4348 - acc: 0.7917 - val_loss: 0.4986 - val_acc: 0.7500\n",
      "Epoch 620/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4348 - acc: 0.7934 - val_loss: 0.4986 - val_acc: 0.7500\n",
      "Epoch 621/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4348 - acc: 0.7917 - val_loss: 0.4986 - val_acc: 0.7500\n",
      "Epoch 622/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4347 - acc: 0.7917 - val_loss: 0.4986 - val_acc: 0.7500\n",
      "Epoch 623/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4347 - acc: 0.7934 - val_loss: 0.4985 - val_acc: 0.7500\n",
      "Epoch 624/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4347 - acc: 0.7917 - val_loss: 0.4985 - val_acc: 0.7500\n",
      "Epoch 625/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4347 - acc: 0.7934 - val_loss: 0.4985 - val_acc: 0.7500\n",
      "Epoch 626/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4346 - acc: 0.7934 - val_loss: 0.4985 - val_acc: 0.7500\n",
      "Epoch 627/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4346 - acc: 0.7934 - val_loss: 0.4985 - val_acc: 0.7500\n",
      "Epoch 628/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4346 - acc: 0.7934 - val_loss: 0.4985 - val_acc: 0.7500\n",
      "Epoch 629/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4345 - acc: 0.7934 - val_loss: 0.4985 - val_acc: 0.7500\n",
      "Epoch 630/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4345 - acc: 0.7917 - val_loss: 0.4985 - val_acc: 0.7500\n",
      "Epoch 631/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4345 - acc: 0.7934 - val_loss: 0.4985 - val_acc: 0.7500\n",
      "Epoch 632/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4345 - acc: 0.7934 - val_loss: 0.4985 - val_acc: 0.7500\n",
      "Epoch 633/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4344 - acc: 0.7934 - val_loss: 0.4985 - val_acc: 0.7500\n",
      "Epoch 634/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4344 - acc: 0.7934 - val_loss: 0.4985 - val_acc: 0.7500\n",
      "Epoch 635/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4344 - acc: 0.7934 - val_loss: 0.4985 - val_acc: 0.7500\n",
      "Epoch 636/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4344 - acc: 0.7934 - val_loss: 0.4984 - val_acc: 0.7500\n",
      "Epoch 637/1000\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.4869 - acc: 0.812 - 0s 46us/step - loss: 0.4344 - acc: 0.7934 - val_loss: 0.4984 - val_acc: 0.7500\n",
      "Epoch 638/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4343 - acc: 0.7934 - val_loss: 0.4984 - val_acc: 0.7500\n",
      "Epoch 639/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4343 - acc: 0.7934 - val_loss: 0.4984 - val_acc: 0.7500\n",
      "Epoch 640/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4343 - acc: 0.7934 - val_loss: 0.4984 - val_acc: 0.7500\n",
      "Epoch 641/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4343 - acc: 0.7934 - val_loss: 0.4984 - val_acc: 0.7500\n",
      "Epoch 642/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4343 - acc: 0.7934 - val_loss: 0.4984 - val_acc: 0.7500\n",
      "Epoch 643/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4342 - acc: 0.7934 - val_loss: 0.4984 - val_acc: 0.7500\n",
      "Epoch 644/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4342 - acc: 0.7934 - val_loss: 0.4984 - val_acc: 0.7500\n",
      "Epoch 645/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4342 - acc: 0.7934 - val_loss: 0.4984 - val_acc: 0.7500\n",
      "Epoch 646/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4342 - acc: 0.7934 - val_loss: 0.4984 - val_acc: 0.7500\n",
      "Epoch 647/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4341 - acc: 0.7934 - val_loss: 0.4984 - val_acc: 0.7500\n",
      "Epoch 648/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4341 - acc: 0.7934 - val_loss: 0.4983 - val_acc: 0.7500\n",
      "Epoch 649/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4341 - acc: 0.7934 - val_loss: 0.4983 - val_acc: 0.7500\n",
      "Epoch 650/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4341 - acc: 0.7917 - val_loss: 0.4983 - val_acc: 0.7500\n",
      "Epoch 651/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4341 - acc: 0.7934 - val_loss: 0.4983 - val_acc: 0.7500\n",
      "Epoch 652/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4340 - acc: 0.7917 - val_loss: 0.4983 - val_acc: 0.7500\n",
      "Epoch 653/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4340 - acc: 0.7917 - val_loss: 0.4983 - val_acc: 0.7500\n",
      "Epoch 654/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4340 - acc: 0.7917 - val_loss: 0.4983 - val_acc: 0.7500\n",
      "Epoch 655/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4340 - acc: 0.7917 - val_loss: 0.4983 - val_acc: 0.7500\n",
      "Epoch 656/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4340 - acc: 0.7917 - val_loss: 0.4983 - val_acc: 0.7500\n",
      "Epoch 657/1000\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.4339 - acc: 0.7917 - val_loss: 0.4983 - val_acc: 0.7500\n",
      "Epoch 658/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 46us/step - loss: 0.4339 - acc: 0.7917 - val_loss: 0.4983 - val_acc: 0.7500\n",
      "Epoch 659/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4339 - acc: 0.7917 - val_loss: 0.4983 - val_acc: 0.7500\n",
      "Epoch 660/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4339 - acc: 0.7917 - val_loss: 0.4982 - val_acc: 0.7500\n",
      "Epoch 661/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4338 - acc: 0.7917 - val_loss: 0.4982 - val_acc: 0.7500\n",
      "Epoch 662/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4338 - acc: 0.7917 - val_loss: 0.4982 - val_acc: 0.7500\n",
      "Epoch 663/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4338 - acc: 0.7917 - val_loss: 0.4982 - val_acc: 0.7500\n",
      "Epoch 664/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4338 - acc: 0.7917 - val_loss: 0.4982 - val_acc: 0.7500\n",
      "Epoch 665/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4337 - acc: 0.7917 - val_loss: 0.4982 - val_acc: 0.7500\n",
      "Epoch 666/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4337 - acc: 0.7917 - val_loss: 0.4982 - val_acc: 0.7500\n",
      "Epoch 667/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4337 - acc: 0.7917 - val_loss: 0.4982 - val_acc: 0.7500\n",
      "Epoch 668/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4337 - acc: 0.7917 - val_loss: 0.4982 - val_acc: 0.7500\n",
      "Epoch 669/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4337 - acc: 0.7934 - val_loss: 0.4982 - val_acc: 0.7500\n",
      "Epoch 670/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4336 - acc: 0.7917 - val_loss: 0.4982 - val_acc: 0.7500\n",
      "Epoch 671/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4336 - acc: 0.7917 - val_loss: 0.4982 - val_acc: 0.7500\n",
      "Epoch 672/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4336 - acc: 0.7934 - val_loss: 0.4982 - val_acc: 0.7500\n",
      "Epoch 673/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4336 - acc: 0.7934 - val_loss: 0.4982 - val_acc: 0.7500\n",
      "Epoch 674/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4335 - acc: 0.7934 - val_loss: 0.4981 - val_acc: 0.7500\n",
      "Epoch 675/1000\n",
      "576/576 [==============================] - 0s 77us/step - loss: 0.4335 - acc: 0.7917 - val_loss: 0.4981 - val_acc: 0.7500\n",
      "Epoch 676/1000\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.4335 - acc: 0.7917 - val_loss: 0.4981 - val_acc: 0.7500\n",
      "Epoch 677/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4335 - acc: 0.7934 - val_loss: 0.4981 - val_acc: 0.7500\n",
      "Epoch 678/1000\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4335 - acc: 0.7934 - val_loss: 0.4981 - val_acc: 0.7500\n",
      "Epoch 679/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4334 - acc: 0.7934 - val_loss: 0.4981 - val_acc: 0.7500\n",
      "Epoch 680/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4334 - acc: 0.7934 - val_loss: 0.4981 - val_acc: 0.7500\n",
      "Epoch 681/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4334 - acc: 0.7934 - val_loss: 0.4981 - val_acc: 0.7500\n",
      "Epoch 682/1000\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4334 - acc: 0.7934 - val_loss: 0.4981 - val_acc: 0.7500\n",
      "Epoch 683/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4334 - acc: 0.7934 - val_loss: 0.4981 - val_acc: 0.7500\n",
      "Epoch 684/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4333 - acc: 0.7934 - val_loss: 0.4981 - val_acc: 0.7500\n",
      "Epoch 685/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4333 - acc: 0.7934 - val_loss: 0.4981 - val_acc: 0.7500\n",
      "Epoch 686/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4333 - acc: 0.7934 - val_loss: 0.4981 - val_acc: 0.7500\n",
      "Epoch 687/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4333 - acc: 0.7934 - val_loss: 0.4981 - val_acc: 0.7500\n",
      "Epoch 688/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4332 - acc: 0.7934 - val_loss: 0.4981 - val_acc: 0.7500\n",
      "Epoch 689/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4332 - acc: 0.7934 - val_loss: 0.4981 - val_acc: 0.7500\n",
      "Epoch 690/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4332 - acc: 0.7934 - val_loss: 0.4980 - val_acc: 0.7500\n",
      "Epoch 691/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4332 - acc: 0.7934 - val_loss: 0.4980 - val_acc: 0.7500\n",
      "Epoch 692/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4331 - acc: 0.7934 - val_loss: 0.4980 - val_acc: 0.7500\n",
      "Epoch 693/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4331 - acc: 0.7934 - val_loss: 0.4980 - val_acc: 0.7500\n",
      "Epoch 694/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4331 - acc: 0.7934 - val_loss: 0.4980 - val_acc: 0.7500\n",
      "Epoch 695/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4331 - acc: 0.7934 - val_loss: 0.4980 - val_acc: 0.7500\n",
      "Epoch 696/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4331 - acc: 0.7934 - val_loss: 0.4980 - val_acc: 0.7500\n",
      "Epoch 697/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4330 - acc: 0.7934 - val_loss: 0.4980 - val_acc: 0.7500\n",
      "Epoch 698/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4330 - acc: 0.7934 - val_loss: 0.4980 - val_acc: 0.7500\n",
      "Epoch 699/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4330 - acc: 0.7934 - val_loss: 0.4980 - val_acc: 0.7500\n",
      "Epoch 700/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4330 - acc: 0.7934 - val_loss: 0.4980 - val_acc: 0.7500\n",
      "Epoch 701/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4330 - acc: 0.7934 - val_loss: 0.4980 - val_acc: 0.7500\n",
      "Epoch 702/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4329 - acc: 0.7934 - val_loss: 0.4980 - val_acc: 0.7500\n",
      "Epoch 703/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4329 - acc: 0.7934 - val_loss: 0.4980 - val_acc: 0.7500\n",
      "Epoch 704/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4329 - acc: 0.7934 - val_loss: 0.4979 - val_acc: 0.7500\n",
      "Epoch 705/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4329 - acc: 0.7934 - val_loss: 0.4979 - val_acc: 0.7500\n",
      "Epoch 706/1000\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4329 - acc: 0.7934 - val_loss: 0.4979 - val_acc: 0.7500\n",
      "Epoch 707/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4329 - acc: 0.7934 - val_loss: 0.4979 - val_acc: 0.7500\n",
      "Epoch 708/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4328 - acc: 0.7934 - val_loss: 0.4979 - val_acc: 0.7500\n",
      "Epoch 709/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4328 - acc: 0.7934 - val_loss: 0.4979 - val_acc: 0.7500\n",
      "Epoch 710/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4328 - acc: 0.7934 - val_loss: 0.4979 - val_acc: 0.7500\n",
      "Epoch 711/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4328 - acc: 0.7934 - val_loss: 0.4979 - val_acc: 0.7500\n",
      "Epoch 712/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4328 - acc: 0.7934 - val_loss: 0.4979 - val_acc: 0.7500\n",
      "Epoch 713/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4327 - acc: 0.7934 - val_loss: 0.4979 - val_acc: 0.7500\n",
      "Epoch 714/1000\n",
      "576/576 [==============================] - 0s 90us/step - loss: 0.4327 - acc: 0.7934 - val_loss: 0.4979 - val_acc: 0.7500\n",
      "Epoch 715/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4327 - acc: 0.7934 - val_loss: 0.4979 - val_acc: 0.7500\n",
      "Epoch 716/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4327 - acc: 0.7934 - val_loss: 0.4979 - val_acc: 0.7500\n",
      "Epoch 717/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4327 - acc: 0.7934 - val_loss: 0.4978 - val_acc: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 718/1000\n",
      "576/576 [==============================] - 0s 85us/step - loss: 0.4326 - acc: 0.7934 - val_loss: 0.4978 - val_acc: 0.7500\n",
      "Epoch 719/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4326 - acc: 0.7934 - val_loss: 0.4978 - val_acc: 0.7500\n",
      "Epoch 720/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4326 - acc: 0.7934 - val_loss: 0.4978 - val_acc: 0.7500\n",
      "Epoch 721/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4326 - acc: 0.7934 - val_loss: 0.4978 - val_acc: 0.7500\n",
      "Epoch 722/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4326 - acc: 0.7934 - val_loss: 0.4978 - val_acc: 0.7500\n",
      "Epoch 723/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4325 - acc: 0.7934 - val_loss: 0.4978 - val_acc: 0.7500\n",
      "Epoch 724/1000\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4325 - acc: 0.7934 - val_loss: 0.4978 - val_acc: 0.7500\n",
      "Epoch 725/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4325 - acc: 0.7934 - val_loss: 0.4978 - val_acc: 0.7500\n",
      "Epoch 726/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4325 - acc: 0.7934 - val_loss: 0.4977 - val_acc: 0.7500\n",
      "Epoch 727/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4325 - acc: 0.7934 - val_loss: 0.4977 - val_acc: 0.7500\n",
      "Epoch 728/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4324 - acc: 0.7934 - val_loss: 0.4977 - val_acc: 0.7500\n",
      "Epoch 729/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4324 - acc: 0.7934 - val_loss: 0.4977 - val_acc: 0.7500\n",
      "Epoch 730/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4324 - acc: 0.7917 - val_loss: 0.4977 - val_acc: 0.7500\n",
      "Epoch 731/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4324 - acc: 0.7934 - val_loss: 0.4977 - val_acc: 0.7500\n",
      "Epoch 732/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4323 - acc: 0.7934 - val_loss: 0.4977 - val_acc: 0.7500\n",
      "Epoch 733/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4323 - acc: 0.7934 - val_loss: 0.4977 - val_acc: 0.7500\n",
      "Epoch 734/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4323 - acc: 0.7934 - val_loss: 0.4977 - val_acc: 0.7500\n",
      "Epoch 735/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4323 - acc: 0.7934 - val_loss: 0.4977 - val_acc: 0.7500\n",
      "Epoch 736/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4322 - acc: 0.7934 - val_loss: 0.4977 - val_acc: 0.7500\n",
      "Epoch 737/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4322 - acc: 0.7934 - val_loss: 0.4976 - val_acc: 0.7500\n",
      "Epoch 738/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4322 - acc: 0.7934 - val_loss: 0.4976 - val_acc: 0.7500\n",
      "Epoch 739/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4322 - acc: 0.7934 - val_loss: 0.4976 - val_acc: 0.7500\n",
      "Epoch 740/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4321 - acc: 0.7934 - val_loss: 0.4976 - val_acc: 0.7500\n",
      "Epoch 741/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4321 - acc: 0.7934 - val_loss: 0.4976 - val_acc: 0.7500\n",
      "Epoch 742/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4321 - acc: 0.7934 - val_loss: 0.4976 - val_acc: 0.7500\n",
      "Epoch 743/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4321 - acc: 0.7934 - val_loss: 0.4976 - val_acc: 0.7500\n",
      "Epoch 744/1000\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4321 - acc: 0.7934 - val_loss: 0.4976 - val_acc: 0.7500\n",
      "Epoch 745/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4321 - acc: 0.7934 - val_loss: 0.4976 - val_acc: 0.7500\n",
      "Epoch 746/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4321 - acc: 0.7934 - val_loss: 0.4975 - val_acc: 0.7500\n",
      "Epoch 747/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4320 - acc: 0.7934 - val_loss: 0.4975 - val_acc: 0.7500\n",
      "Epoch 748/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4320 - acc: 0.7934 - val_loss: 0.4975 - val_acc: 0.7500\n",
      "Epoch 749/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4320 - acc: 0.7917 - val_loss: 0.4975 - val_acc: 0.7500\n",
      "Epoch 750/1000\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4319 - acc: 0.7917 - val_loss: 0.4975 - val_acc: 0.7500\n",
      "Epoch 751/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4319 - acc: 0.7934 - val_loss: 0.4975 - val_acc: 0.7500\n",
      "Epoch 752/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4319 - acc: 0.7934 - val_loss: 0.4975 - val_acc: 0.7500\n",
      "Epoch 753/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4319 - acc: 0.7934 - val_loss: 0.4975 - val_acc: 0.7500\n",
      "Epoch 754/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4319 - acc: 0.7917 - val_loss: 0.4975 - val_acc: 0.7500\n",
      "Epoch 755/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4319 - acc: 0.7934 - val_loss: 0.4975 - val_acc: 0.7500\n",
      "Epoch 756/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4318 - acc: 0.7934 - val_loss: 0.4974 - val_acc: 0.7500\n",
      "Epoch 757/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4318 - acc: 0.7934 - val_loss: 0.4974 - val_acc: 0.7500\n",
      "Epoch 758/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4318 - acc: 0.7934 - val_loss: 0.4974 - val_acc: 0.7500\n",
      "Epoch 759/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4318 - acc: 0.7934 - val_loss: 0.4974 - val_acc: 0.7500\n",
      "Epoch 760/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4317 - acc: 0.7934 - val_loss: 0.4974 - val_acc: 0.7500\n",
      "Epoch 761/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4317 - acc: 0.7934 - val_loss: 0.4974 - val_acc: 0.7500\n",
      "Epoch 762/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4317 - acc: 0.7934 - val_loss: 0.4974 - val_acc: 0.7500\n",
      "Epoch 763/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4317 - acc: 0.7934 - val_loss: 0.4974 - val_acc: 0.7500\n",
      "Epoch 764/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4317 - acc: 0.7934 - val_loss: 0.4974 - val_acc: 0.7500\n",
      "Epoch 765/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4316 - acc: 0.7934 - val_loss: 0.4974 - val_acc: 0.7500\n",
      "Epoch 766/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4316 - acc: 0.7934 - val_loss: 0.4973 - val_acc: 0.7500\n",
      "Epoch 767/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4316 - acc: 0.7934 - val_loss: 0.4973 - val_acc: 0.7500\n",
      "Epoch 768/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4316 - acc: 0.7934 - val_loss: 0.4973 - val_acc: 0.7500\n",
      "Epoch 769/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4316 - acc: 0.7934 - val_loss: 0.4973 - val_acc: 0.7500\n",
      "Epoch 770/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4315 - acc: 0.7934 - val_loss: 0.4973 - val_acc: 0.7500\n",
      "Epoch 771/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4315 - acc: 0.7934 - val_loss: 0.4973 - val_acc: 0.7552\n",
      "Epoch 772/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4315 - acc: 0.7934 - val_loss: 0.4973 - val_acc: 0.7552\n",
      "Epoch 773/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4315 - acc: 0.7934 - val_loss: 0.4973 - val_acc: 0.7552\n",
      "Epoch 774/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4315 - acc: 0.7934 - val_loss: 0.4973 - val_acc: 0.7552\n",
      "Epoch 775/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4314 - acc: 0.7934 - val_loss: 0.4973 - val_acc: 0.7552\n",
      "Epoch 776/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4314 - acc: 0.7934 - val_loss: 0.4973 - val_acc: 0.7552\n",
      "Epoch 777/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4314 - acc: 0.7934 - val_loss: 0.4973 - val_acc: 0.7552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 778/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4314 - acc: 0.7934 - val_loss: 0.4972 - val_acc: 0.7552\n",
      "Epoch 779/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4314 - acc: 0.7934 - val_loss: 0.4972 - val_acc: 0.7552\n",
      "Epoch 780/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4314 - acc: 0.7934 - val_loss: 0.4972 - val_acc: 0.7552\n",
      "Epoch 781/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4313 - acc: 0.7934 - val_loss: 0.4972 - val_acc: 0.7552\n",
      "Epoch 782/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4313 - acc: 0.7934 - val_loss: 0.4972 - val_acc: 0.7552\n",
      "Epoch 783/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4313 - acc: 0.7934 - val_loss: 0.4972 - val_acc: 0.7552\n",
      "Epoch 784/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4313 - acc: 0.7934 - val_loss: 0.4972 - val_acc: 0.7552\n",
      "Epoch 785/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4313 - acc: 0.7934 - val_loss: 0.4972 - val_acc: 0.7552\n",
      "Epoch 786/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4313 - acc: 0.7934 - val_loss: 0.4972 - val_acc: 0.7552\n",
      "Epoch 787/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4312 - acc: 0.7934 - val_loss: 0.4972 - val_acc: 0.7552\n",
      "Epoch 788/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4312 - acc: 0.7934 - val_loss: 0.4972 - val_acc: 0.7552\n",
      "Epoch 789/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4312 - acc: 0.7934 - val_loss: 0.4971 - val_acc: 0.7552\n",
      "Epoch 790/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4312 - acc: 0.7934 - val_loss: 0.4971 - val_acc: 0.7552\n",
      "Epoch 791/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4312 - acc: 0.7934 - val_loss: 0.4971 - val_acc: 0.7552\n",
      "Epoch 792/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4311 - acc: 0.7934 - val_loss: 0.4971 - val_acc: 0.7552\n",
      "Epoch 793/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4311 - acc: 0.7934 - val_loss: 0.4971 - val_acc: 0.7552\n",
      "Epoch 794/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4311 - acc: 0.7934 - val_loss: 0.4971 - val_acc: 0.7552\n",
      "Epoch 795/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4311 - acc: 0.7934 - val_loss: 0.4971 - val_acc: 0.7552\n",
      "Epoch 796/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4311 - acc: 0.7934 - val_loss: 0.4971 - val_acc: 0.7552\n",
      "Epoch 797/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4311 - acc: 0.7934 - val_loss: 0.4971 - val_acc: 0.7552\n",
      "Epoch 798/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4310 - acc: 0.7934 - val_loss: 0.4971 - val_acc: 0.7552\n",
      "Epoch 799/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4310 - acc: 0.7934 - val_loss: 0.4971 - val_acc: 0.7552\n",
      "Epoch 800/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4310 - acc: 0.7934 - val_loss: 0.4971 - val_acc: 0.7552\n",
      "Epoch 801/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4310 - acc: 0.7934 - val_loss: 0.4970 - val_acc: 0.7552\n",
      "Epoch 802/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4310 - acc: 0.7934 - val_loss: 0.4970 - val_acc: 0.7552\n",
      "Epoch 803/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4309 - acc: 0.7934 - val_loss: 0.4970 - val_acc: 0.7552\n",
      "Epoch 804/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4309 - acc: 0.7934 - val_loss: 0.4970 - val_acc: 0.7552\n",
      "Epoch 805/1000\n",
      "576/576 [==============================] - 0s 77us/step - loss: 0.4309 - acc: 0.7934 - val_loss: 0.4970 - val_acc: 0.7552\n",
      "Epoch 806/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4309 - acc: 0.7934 - val_loss: 0.4970 - val_acc: 0.7552\n",
      "Epoch 807/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4309 - acc: 0.7934 - val_loss: 0.4970 - val_acc: 0.7552\n",
      "Epoch 808/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4309 - acc: 0.7934 - val_loss: 0.4970 - val_acc: 0.7552\n",
      "Epoch 809/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4308 - acc: 0.7934 - val_loss: 0.4970 - val_acc: 0.7552\n",
      "Epoch 810/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4308 - acc: 0.7934 - val_loss: 0.4970 - val_acc: 0.7552\n",
      "Epoch 811/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4308 - acc: 0.7934 - val_loss: 0.4970 - val_acc: 0.7552\n",
      "Epoch 812/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4308 - acc: 0.7934 - val_loss: 0.4970 - val_acc: 0.7552\n",
      "Epoch 813/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4308 - acc: 0.7934 - val_loss: 0.4970 - val_acc: 0.7552\n",
      "Epoch 814/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4308 - acc: 0.7934 - val_loss: 0.4970 - val_acc: 0.7552\n",
      "Epoch 815/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4307 - acc: 0.7934 - val_loss: 0.4970 - val_acc: 0.7552\n",
      "Epoch 816/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4307 - acc: 0.7934 - val_loss: 0.4969 - val_acc: 0.7552\n",
      "Epoch 817/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4307 - acc: 0.7934 - val_loss: 0.4969 - val_acc: 0.7552\n",
      "Epoch 818/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4307 - acc: 0.7934 - val_loss: 0.4969 - val_acc: 0.7552\n",
      "Epoch 819/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4307 - acc: 0.7934 - val_loss: 0.4969 - val_acc: 0.7552\n",
      "Epoch 820/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4306 - acc: 0.7934 - val_loss: 0.4969 - val_acc: 0.7552\n",
      "Epoch 821/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4306 - acc: 0.7934 - val_loss: 0.4969 - val_acc: 0.7552\n",
      "Epoch 822/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4306 - acc: 0.7934 - val_loss: 0.4969 - val_acc: 0.7552\n",
      "Epoch 823/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4306 - acc: 0.7934 - val_loss: 0.4969 - val_acc: 0.7552\n",
      "Epoch 824/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4306 - acc: 0.7934 - val_loss: 0.4969 - val_acc: 0.7552\n",
      "Epoch 825/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4306 - acc: 0.7934 - val_loss: 0.4969 - val_acc: 0.7552\n",
      "Epoch 826/1000\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.2388 - acc: 0.937 - 0s 46us/step - loss: 0.4305 - acc: 0.7934 - val_loss: 0.4969 - val_acc: 0.7552\n",
      "Epoch 827/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4305 - acc: 0.7934 - val_loss: 0.4969 - val_acc: 0.7552\n",
      "Epoch 828/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4305 - acc: 0.7934 - val_loss: 0.4969 - val_acc: 0.7552\n",
      "Epoch 829/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4305 - acc: 0.7934 - val_loss: 0.4969 - val_acc: 0.7552\n",
      "Epoch 830/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4305 - acc: 0.7934 - val_loss: 0.4968 - val_acc: 0.7552\n",
      "Epoch 831/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4304 - acc: 0.7934 - val_loss: 0.4968 - val_acc: 0.7552\n",
      "Epoch 832/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4304 - acc: 0.7934 - val_loss: 0.4968 - val_acc: 0.7552\n",
      "Epoch 833/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4304 - acc: 0.7934 - val_loss: 0.4968 - val_acc: 0.7552\n",
      "Epoch 834/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4304 - acc: 0.7934 - val_loss: 0.4968 - val_acc: 0.7552\n",
      "Epoch 835/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4304 - acc: 0.7934 - val_loss: 0.4968 - val_acc: 0.7552\n",
      "Epoch 836/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4304 - acc: 0.7934 - val_loss: 0.4968 - val_acc: 0.7552\n",
      "Epoch 837/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 50us/step - loss: 0.4304 - acc: 0.7934 - val_loss: 0.4968 - val_acc: 0.7552\n",
      "Epoch 838/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4303 - acc: 0.7934 - val_loss: 0.4968 - val_acc: 0.7552\n",
      "Epoch 839/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4303 - acc: 0.7934 - val_loss: 0.4968 - val_acc: 0.7552\n",
      "Epoch 840/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4303 - acc: 0.7934 - val_loss: 0.4968 - val_acc: 0.7552\n",
      "Epoch 841/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4303 - acc: 0.7934 - val_loss: 0.4968 - val_acc: 0.7552\n",
      "Epoch 842/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4303 - acc: 0.7934 - val_loss: 0.4968 - val_acc: 0.7552\n",
      "Epoch 843/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4303 - acc: 0.7934 - val_loss: 0.4968 - val_acc: 0.7552\n",
      "Epoch 844/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4302 - acc: 0.7934 - val_loss: 0.4967 - val_acc: 0.7552\n",
      "Epoch 845/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4302 - acc: 0.7934 - val_loss: 0.4967 - val_acc: 0.7552\n",
      "Epoch 846/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4302 - acc: 0.7934 - val_loss: 0.4967 - val_acc: 0.7552\n",
      "Epoch 847/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4302 - acc: 0.7934 - val_loss: 0.4967 - val_acc: 0.7552\n",
      "Epoch 848/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4302 - acc: 0.7934 - val_loss: 0.4967 - val_acc: 0.7552\n",
      "Epoch 849/1000\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4302 - acc: 0.7917 - val_loss: 0.4967 - val_acc: 0.7552\n",
      "Epoch 850/1000\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.4301 - acc: 0.7934 - val_loss: 0.4967 - val_acc: 0.7552\n",
      "Epoch 851/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4301 - acc: 0.7934 - val_loss: 0.4967 - val_acc: 0.7552\n",
      "Epoch 852/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4301 - acc: 0.7934 - val_loss: 0.4967 - val_acc: 0.7552\n",
      "Epoch 853/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4301 - acc: 0.7934 - val_loss: 0.4967 - val_acc: 0.7552\n",
      "Epoch 854/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4300 - acc: 0.7917 - val_loss: 0.4967 - val_acc: 0.7552\n",
      "Epoch 855/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4301 - acc: 0.7934 - val_loss: 0.4967 - val_acc: 0.7552\n",
      "Epoch 856/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4300 - acc: 0.7934 - val_loss: 0.4967 - val_acc: 0.7552\n",
      "Epoch 857/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4300 - acc: 0.7934 - val_loss: 0.4967 - val_acc: 0.7552\n",
      "Epoch 858/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4300 - acc: 0.7917 - val_loss: 0.4967 - val_acc: 0.7552\n",
      "Epoch 859/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4300 - acc: 0.7917 - val_loss: 0.4967 - val_acc: 0.7552\n",
      "Epoch 860/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4300 - acc: 0.7917 - val_loss: 0.4966 - val_acc: 0.7552\n",
      "Epoch 861/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4300 - acc: 0.7934 - val_loss: 0.4966 - val_acc: 0.7552\n",
      "Epoch 862/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4300 - acc: 0.7934 - val_loss: 0.4966 - val_acc: 0.7552\n",
      "Epoch 863/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4299 - acc: 0.7934 - val_loss: 0.4966 - val_acc: 0.7552\n",
      "Epoch 864/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4299 - acc: 0.7934 - val_loss: 0.4966 - val_acc: 0.7552\n",
      "Epoch 865/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4299 - acc: 0.7917 - val_loss: 0.4966 - val_acc: 0.7552\n",
      "Epoch 866/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4299 - acc: 0.7934 - val_loss: 0.4966 - val_acc: 0.7552\n",
      "Epoch 867/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4299 - acc: 0.7934 - val_loss: 0.4966 - val_acc: 0.7552\n",
      "Epoch 868/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4298 - acc: 0.7934 - val_loss: 0.4966 - val_acc: 0.7552\n",
      "Epoch 869/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4299 - acc: 0.7934 - val_loss: 0.4966 - val_acc: 0.7552\n",
      "Epoch 870/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4298 - acc: 0.7917 - val_loss: 0.4966 - val_acc: 0.7552\n",
      "Epoch 871/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4298 - acc: 0.7934 - val_loss: 0.4966 - val_acc: 0.7552\n",
      "Epoch 872/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4298 - acc: 0.7917 - val_loss: 0.4966 - val_acc: 0.7552\n",
      "Epoch 873/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4298 - acc: 0.7934 - val_loss: 0.4966 - val_acc: 0.7552\n",
      "Epoch 874/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4298 - acc: 0.7934 - val_loss: 0.4966 - val_acc: 0.7552\n",
      "Epoch 875/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4297 - acc: 0.7917 - val_loss: 0.4966 - val_acc: 0.7552\n",
      "Epoch 876/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4297 - acc: 0.7917 - val_loss: 0.4965 - val_acc: 0.7552\n",
      "Epoch 877/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4297 - acc: 0.7917 - val_loss: 0.4965 - val_acc: 0.7552\n",
      "Epoch 878/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4297 - acc: 0.7917 - val_loss: 0.4965 - val_acc: 0.7552\n",
      "Epoch 879/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4297 - acc: 0.7917 - val_loss: 0.4965 - val_acc: 0.7552\n",
      "Epoch 880/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4297 - acc: 0.7917 - val_loss: 0.4965 - val_acc: 0.7552\n",
      "Epoch 881/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4297 - acc: 0.7934 - val_loss: 0.4965 - val_acc: 0.7552\n",
      "Epoch 882/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4296 - acc: 0.7917 - val_loss: 0.4965 - val_acc: 0.7552\n",
      "Epoch 883/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4296 - acc: 0.7917 - val_loss: 0.4965 - val_acc: 0.7552\n",
      "Epoch 884/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4296 - acc: 0.7917 - val_loss: 0.4965 - val_acc: 0.7552\n",
      "Epoch 885/1000\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.4296 - acc: 0.7917 - val_loss: 0.4965 - val_acc: 0.7552\n",
      "Epoch 886/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4296 - acc: 0.7917 - val_loss: 0.4965 - val_acc: 0.7552\n",
      "Epoch 887/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4295 - acc: 0.7917 - val_loss: 0.4965 - val_acc: 0.7552\n",
      "Epoch 888/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4296 - acc: 0.7917 - val_loss: 0.4965 - val_acc: 0.7552\n",
      "Epoch 889/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4295 - acc: 0.7917 - val_loss: 0.4964 - val_acc: 0.7552\n",
      "Epoch 890/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4295 - acc: 0.7917 - val_loss: 0.4964 - val_acc: 0.7552\n",
      "Epoch 891/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4295 - acc: 0.7934 - val_loss: 0.4964 - val_acc: 0.7552\n",
      "Epoch 892/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4295 - acc: 0.7917 - val_loss: 0.4964 - val_acc: 0.7552\n",
      "Epoch 893/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4294 - acc: 0.7917 - val_loss: 0.4964 - val_acc: 0.7552\n",
      "Epoch 894/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4294 - acc: 0.7917 - val_loss: 0.4964 - val_acc: 0.7552\n",
      "Epoch 895/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4294 - acc: 0.7917 - val_loss: 0.4964 - val_acc: 0.7552\n",
      "Epoch 896/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4294 - acc: 0.7917 - val_loss: 0.4964 - val_acc: 0.7552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 897/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4294 - acc: 0.7934 - val_loss: 0.4964 - val_acc: 0.7552\n",
      "Epoch 898/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4294 - acc: 0.7917 - val_loss: 0.4964 - val_acc: 0.7552\n",
      "Epoch 899/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4293 - acc: 0.7917 - val_loss: 0.4964 - val_acc: 0.7552\n",
      "Epoch 900/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4294 - acc: 0.7917 - val_loss: 0.4964 - val_acc: 0.7552\n",
      "Epoch 901/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4293 - acc: 0.7917 - val_loss: 0.4964 - val_acc: 0.7552\n",
      "Epoch 902/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4293 - acc: 0.7917 - val_loss: 0.4964 - val_acc: 0.7552\n",
      "Epoch 903/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4293 - acc: 0.7917 - val_loss: 0.4964 - val_acc: 0.7552\n",
      "Epoch 904/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4293 - acc: 0.7917 - val_loss: 0.4964 - val_acc: 0.7552\n",
      "Epoch 905/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4292 - acc: 0.7917 - val_loss: 0.4963 - val_acc: 0.7552\n",
      "Epoch 906/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4292 - acc: 0.7917 - val_loss: 0.4963 - val_acc: 0.7552\n",
      "Epoch 907/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4292 - acc: 0.7917 - val_loss: 0.4963 - val_acc: 0.7552\n",
      "Epoch 908/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4292 - acc: 0.7917 - val_loss: 0.4963 - val_acc: 0.7552\n",
      "Epoch 909/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4292 - acc: 0.7917 - val_loss: 0.4963 - val_acc: 0.7552\n",
      "Epoch 910/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4291 - acc: 0.7917 - val_loss: 0.4963 - val_acc: 0.7552\n",
      "Epoch 911/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4291 - acc: 0.7917 - val_loss: 0.4963 - val_acc: 0.7552\n",
      "Epoch 912/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4291 - acc: 0.7917 - val_loss: 0.4963 - val_acc: 0.7552\n",
      "Epoch 913/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4291 - acc: 0.7917 - val_loss: 0.4963 - val_acc: 0.7552\n",
      "Epoch 914/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4291 - acc: 0.7917 - val_loss: 0.4963 - val_acc: 0.7552\n",
      "Epoch 915/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4291 - acc: 0.7917 - val_loss: 0.4963 - val_acc: 0.7552\n",
      "Epoch 916/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4291 - acc: 0.7917 - val_loss: 0.4963 - val_acc: 0.7552\n",
      "Epoch 917/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4290 - acc: 0.7917 - val_loss: 0.4963 - val_acc: 0.7552\n",
      "Epoch 918/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4290 - acc: 0.7917 - val_loss: 0.4963 - val_acc: 0.7552\n",
      "Epoch 919/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4290 - acc: 0.7917 - val_loss: 0.4963 - val_acc: 0.7552\n",
      "Epoch 920/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4290 - acc: 0.7917 - val_loss: 0.4963 - val_acc: 0.7552\n",
      "Epoch 921/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4290 - acc: 0.7917 - val_loss: 0.4962 - val_acc: 0.7552\n",
      "Epoch 922/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4289 - acc: 0.7917 - val_loss: 0.4962 - val_acc: 0.7552\n",
      "Epoch 923/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4290 - acc: 0.7917 - val_loss: 0.4962 - val_acc: 0.7552\n",
      "Epoch 924/1000\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.4289 - acc: 0.7917 - val_loss: 0.4962 - val_acc: 0.7552\n",
      "Epoch 925/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4289 - acc: 0.7917 - val_loss: 0.4962 - val_acc: 0.7552\n",
      "Epoch 926/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4289 - acc: 0.7917 - val_loss: 0.4962 - val_acc: 0.7552\n",
      "Epoch 927/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4289 - acc: 0.7917 - val_loss: 0.4962 - val_acc: 0.7552\n",
      "Epoch 928/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4288 - acc: 0.7917 - val_loss: 0.4962 - val_acc: 0.7552\n",
      "Epoch 929/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4289 - acc: 0.7917 - val_loss: 0.4962 - val_acc: 0.7552\n",
      "Epoch 930/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4288 - acc: 0.7917 - val_loss: 0.4962 - val_acc: 0.7552\n",
      "Epoch 931/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4288 - acc: 0.7917 - val_loss: 0.4962 - val_acc: 0.7552\n",
      "Epoch 932/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4288 - acc: 0.7917 - val_loss: 0.4962 - val_acc: 0.7552\n",
      "Epoch 933/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4288 - acc: 0.7917 - val_loss: 0.4962 - val_acc: 0.7552\n",
      "Epoch 934/1000\n",
      "576/576 [==============================] - 0s 79us/step - loss: 0.4288 - acc: 0.7917 - val_loss: 0.4962 - val_acc: 0.7552\n",
      "Epoch 935/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4288 - acc: 0.7917 - val_loss: 0.4962 - val_acc: 0.7552\n",
      "Epoch 936/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4287 - acc: 0.7917 - val_loss: 0.4962 - val_acc: 0.7552\n",
      "Epoch 937/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4287 - acc: 0.7917 - val_loss: 0.4962 - val_acc: 0.7552\n",
      "Epoch 938/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4287 - acc: 0.7917 - val_loss: 0.4962 - val_acc: 0.7552\n",
      "Epoch 939/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4287 - acc: 0.7917 - val_loss: 0.4962 - val_acc: 0.7552\n",
      "Epoch 940/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4287 - acc: 0.7917 - val_loss: 0.4962 - val_acc: 0.7552\n",
      "Epoch 941/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4287 - acc: 0.7917 - val_loss: 0.4961 - val_acc: 0.7552\n",
      "Epoch 942/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4287 - acc: 0.7917 - val_loss: 0.4961 - val_acc: 0.7552\n",
      "Epoch 943/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4286 - acc: 0.7917 - val_loss: 0.4961 - val_acc: 0.7552\n",
      "Epoch 944/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4286 - acc: 0.7917 - val_loss: 0.4961 - val_acc: 0.7552\n",
      "Epoch 945/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4286 - acc: 0.7917 - val_loss: 0.4961 - val_acc: 0.7552\n",
      "Epoch 946/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4286 - acc: 0.7917 - val_loss: 0.4961 - val_acc: 0.7552\n",
      "Epoch 947/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4286 - acc: 0.7917 - val_loss: 0.4961 - val_acc: 0.7552\n",
      "Epoch 948/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4285 - acc: 0.7917 - val_loss: 0.4961 - val_acc: 0.7552\n",
      "Epoch 949/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4285 - acc: 0.7917 - val_loss: 0.4961 - val_acc: 0.7552\n",
      "Epoch 950/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4285 - acc: 0.7917 - val_loss: 0.4961 - val_acc: 0.7552\n",
      "Epoch 951/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4285 - acc: 0.7917 - val_loss: 0.4961 - val_acc: 0.7552\n",
      "Epoch 952/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4285 - acc: 0.7917 - val_loss: 0.4961 - val_acc: 0.7552\n",
      "Epoch 953/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4285 - acc: 0.7917 - val_loss: 0.4961 - val_acc: 0.7552\n",
      "Epoch 954/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4284 - acc: 0.7917 - val_loss: 0.4961 - val_acc: 0.7552\n",
      "Epoch 955/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4285 - acc: 0.7917 - val_loss: 0.4961 - val_acc: 0.7552\n",
      "Epoch 956/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4284 - acc: 0.7917 - val_loss: 0.4961 - val_acc: 0.7552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 957/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4284 - acc: 0.7917 - val_loss: 0.4961 - val_acc: 0.7552\n",
      "Epoch 958/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4284 - acc: 0.7917 - val_loss: 0.4961 - val_acc: 0.7552\n",
      "Epoch 959/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4284 - acc: 0.7917 - val_loss: 0.4961 - val_acc: 0.7552\n",
      "Epoch 960/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4283 - acc: 0.7917 - val_loss: 0.4961 - val_acc: 0.7552\n",
      "Epoch 961/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4284 - acc: 0.7917 - val_loss: 0.4961 - val_acc: 0.7552\n",
      "Epoch 962/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4283 - acc: 0.7917 - val_loss: 0.4961 - val_acc: 0.7552\n",
      "Epoch 963/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4283 - acc: 0.7917 - val_loss: 0.4961 - val_acc: 0.7552\n",
      "Epoch 964/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4283 - acc: 0.7917 - val_loss: 0.4961 - val_acc: 0.7552\n",
      "Epoch 965/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4283 - acc: 0.7917 - val_loss: 0.4961 - val_acc: 0.7552\n",
      "Epoch 966/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4283 - acc: 0.7917 - val_loss: 0.4961 - val_acc: 0.7552\n",
      "Epoch 967/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4282 - acc: 0.7917 - val_loss: 0.4960 - val_acc: 0.7552\n",
      "Epoch 968/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4283 - acc: 0.7917 - val_loss: 0.4960 - val_acc: 0.7552\n",
      "Epoch 969/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4282 - acc: 0.7917 - val_loss: 0.4960 - val_acc: 0.7552\n",
      "Epoch 970/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4282 - acc: 0.7917 - val_loss: 0.4960 - val_acc: 0.7552\n",
      "Epoch 971/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4282 - acc: 0.7917 - val_loss: 0.4960 - val_acc: 0.7552\n",
      "Epoch 972/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4282 - acc: 0.7917 - val_loss: 0.4960 - val_acc: 0.7552\n",
      "Epoch 973/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4282 - acc: 0.7917 - val_loss: 0.4960 - val_acc: 0.7552\n",
      "Epoch 974/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4282 - acc: 0.7917 - val_loss: 0.4960 - val_acc: 0.7552\n",
      "Epoch 975/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4281 - acc: 0.7917 - val_loss: 0.4960 - val_acc: 0.7552\n",
      "Epoch 976/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4281 - acc: 0.7917 - val_loss: 0.4960 - val_acc: 0.7552\n",
      "Epoch 977/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4281 - acc: 0.7917 - val_loss: 0.4960 - val_acc: 0.7552\n",
      "Epoch 978/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4281 - acc: 0.7917 - val_loss: 0.4960 - val_acc: 0.7552\n",
      "Epoch 979/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4281 - acc: 0.7917 - val_loss: 0.4960 - val_acc: 0.7552\n",
      "Epoch 980/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4280 - acc: 0.7917 - val_loss: 0.4960 - val_acc: 0.7552\n",
      "Epoch 981/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4280 - acc: 0.7917 - val_loss: 0.4960 - val_acc: 0.7552\n",
      "Epoch 982/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4280 - acc: 0.7917 - val_loss: 0.4960 - val_acc: 0.7552\n",
      "Epoch 983/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4280 - acc: 0.7917 - val_loss: 0.4960 - val_acc: 0.7552\n",
      "Epoch 984/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4280 - acc: 0.7917 - val_loss: 0.4960 - val_acc: 0.7552\n",
      "Epoch 985/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4280 - acc: 0.7917 - val_loss: 0.4960 - val_acc: 0.7552\n",
      "Epoch 986/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4279 - acc: 0.7917 - val_loss: 0.4960 - val_acc: 0.7552\n",
      "Epoch 987/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4279 - acc: 0.7917 - val_loss: 0.4960 - val_acc: 0.7552\n",
      "Epoch 988/1000\n",
      "576/576 [==============================] - 0s 86us/step - loss: 0.4279 - acc: 0.7917 - val_loss: 0.4960 - val_acc: 0.7552\n",
      "Epoch 989/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4279 - acc: 0.7917 - val_loss: 0.4960 - val_acc: 0.7552\n",
      "Epoch 990/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4279 - acc: 0.7917 - val_loss: 0.4960 - val_acc: 0.7552\n",
      "Epoch 991/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4279 - acc: 0.7917 - val_loss: 0.4960 - val_acc: 0.7552\n",
      "Epoch 992/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4279 - acc: 0.7917 - val_loss: 0.4960 - val_acc: 0.7552\n",
      "Epoch 993/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4278 - acc: 0.7917 - val_loss: 0.4960 - val_acc: 0.7552\n",
      "Epoch 994/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4278 - acc: 0.7917 - val_loss: 0.4960 - val_acc: 0.7552\n",
      "Epoch 995/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4278 - acc: 0.7917 - val_loss: 0.4960 - val_acc: 0.7552\n",
      "Epoch 996/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4278 - acc: 0.7917 - val_loss: 0.4960 - val_acc: 0.7552\n",
      "Epoch 997/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4278 - acc: 0.7917 - val_loss: 0.4960 - val_acc: 0.7552\n",
      "Epoch 998/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4278 - acc: 0.7917 - val_loss: 0.4960 - val_acc: 0.7552\n",
      "Epoch 999/1000\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.4277 - acc: 0.7934 - val_loss: 0.4960 - val_acc: 0.7552\n",
      "Epoch 1000/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4277 - acc: 0.7934 - val_loss: 0.4960 - val_acc: 0.7552\n"
     ]
    }
   ],
   "source": [
    "## Note that when we call \"fit\" again, it picks up where it left off\n",
    "run_hist_1b = model_1.fit(X_train_norm, y_train, validation_data=(X_test_norm, y_test), epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1163f30f0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA6sAAAHVCAYAAAAXVW0dAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSZXz/wFaDH71qrOJOgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1163bf630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = len(run_hist_1.history[\"loss\"])\n",
    "m = len(run_hist_1b.history['loss'])\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "\n",
    "ax.plot(range(n), run_hist_1.history[\"loss\"],'r', marker='.', label=\"Train Loss - Run 1\")\n",
    "ax.plot(range(n, n+m), run_hist_1b.history[\"loss\"], 'hotpink', marker='.', label=\"Train Loss - Run 2\")\n",
    "\n",
    "ax.plot(range(n), run_hist_1.history[\"val_loss\"],'b', marker='.', label=\"Validation Loss - Run 1\")\n",
    "ax.plot(range(n, n+m), run_hist_1b.history[\"val_loss\"], 'LightSkyBlue', marker='.',  label=\"Validation Loss - Run 2\")\n",
    "\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this graph begins where the other left off.  While the training loss is still going down, it looks like the validation loss has stabilized (or even gotten worse!).  This suggests that our network will not benefit from further training.  What is the appropriate number of epochs?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "I think 400 is a good number of epoch's as the validation loss function is minimized around there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "Now it's your turn.  Do the following in the cells below:\n",
    "- Build a model with two hidden layers, each with 6 nodes\n",
    "- Use the \"relu\" activation function for the hidden layers, and \"sigmoid\" for the final layer\n",
    "- Use a learning rate of .003 and train for 1500 epochs\n",
    "- Graph the trajectory of the loss functions, accuracy on both train and test set\n",
    "- Plot the roc curve for the predictions\n",
    "\n",
    "Experiment with different learning rates, numbers of epochs, and network structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_8 (Dense)              (None, 6)                 54        \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 6)                 42        \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 7         \n",
      "=================================================================\n",
      "Total params: 103\n",
      "Trainable params: 103\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Type your code here to with layer 1,2 having activation relu and layer 3 with activation sigmoid\n",
    "\n",
    "model_2 = Sequential([\n",
    "    Dense(6, input_shape=(8,), activation=\"relu\"),\n",
    "    Dense(6, input_shape=(8,), activation=\"relu\"),\n",
    "    Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 576 samples, validate on 192 samples\n",
      "Epoch 1/1500\n",
      "576/576 [==============================] - 0s 742us/step - loss: 0.7387 - acc: 0.5642 - val_loss: 0.7075 - val_acc: 0.5625\n",
      "Epoch 2/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.7330 - acc: 0.5816 - val_loss: 0.7029 - val_acc: 0.5781\n",
      "Epoch 3/1500\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.7278 - acc: 0.5903 - val_loss: 0.6985 - val_acc: 0.5885\n",
      "Epoch 4/1500\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.7228 - acc: 0.5920 - val_loss: 0.6945 - val_acc: 0.6042\n",
      "Epoch 5/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.7181 - acc: 0.5955 - val_loss: 0.6907 - val_acc: 0.5833\n",
      "Epoch 6/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.7138 - acc: 0.5955 - val_loss: 0.6871 - val_acc: 0.6042\n",
      "Epoch 7/1500\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.7096 - acc: 0.6076 - val_loss: 0.6838 - val_acc: 0.6094\n",
      "Epoch 8/1500\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.7057 - acc: 0.6146 - val_loss: 0.6806 - val_acc: 0.6146\n",
      "Epoch 9/1500\n",
      "576/576 [==============================] - 0s 79us/step - loss: 0.7020 - acc: 0.6198 - val_loss: 0.6776 - val_acc: 0.6250\n",
      "Epoch 10/1500\n",
      "576/576 [==============================] - 0s 115us/step - loss: 0.6985 - acc: 0.6337 - val_loss: 0.6748 - val_acc: 0.6302\n",
      "Epoch 11/1500\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.6952 - acc: 0.6424 - val_loss: 0.6722 - val_acc: 0.6354\n",
      "Epoch 12/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.6920 - acc: 0.6476 - val_loss: 0.6697 - val_acc: 0.6302\n",
      "Epoch 13/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.6890 - acc: 0.6406 - val_loss: 0.6673 - val_acc: 0.6302\n",
      "Epoch 14/1500\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.6862 - acc: 0.6389 - val_loss: 0.6651 - val_acc: 0.6302\n",
      "Epoch 15/1500\n",
      "576/576 [==============================] - 0s 83us/step - loss: 0.6835 - acc: 0.6424 - val_loss: 0.6629 - val_acc: 0.6198\n",
      "Epoch 16/1500\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.6810 - acc: 0.6424 - val_loss: 0.6609 - val_acc: 0.6250\n",
      "Epoch 17/1500\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.6786 - acc: 0.6441 - val_loss: 0.6590 - val_acc: 0.6406\n",
      "Epoch 18/1500\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.6762 - acc: 0.6424 - val_loss: 0.6571 - val_acc: 0.6458\n",
      "Epoch 19/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.6740 - acc: 0.6493 - val_loss: 0.6554 - val_acc: 0.6562\n",
      "Epoch 20/1500\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.6719 - acc: 0.6562 - val_loss: 0.6537 - val_acc: 0.6719\n",
      "Epoch 21/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.6698 - acc: 0.6545 - val_loss: 0.6520 - val_acc: 0.6719\n",
      "Epoch 22/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.6678 - acc: 0.6458 - val_loss: 0.6504 - val_acc: 0.6667\n",
      "Epoch 23/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.6658 - acc: 0.6476 - val_loss: 0.6488 - val_acc: 0.6615\n",
      "Epoch 24/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.6639 - acc: 0.6441 - val_loss: 0.6474 - val_acc: 0.6667\n",
      "Epoch 25/1500\n",
      "576/576 [==============================] - 0s 88us/step - loss: 0.6621 - acc: 0.6406 - val_loss: 0.6460 - val_acc: 0.6615\n",
      "Epoch 26/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.6603 - acc: 0.6441 - val_loss: 0.6446 - val_acc: 0.6667\n",
      "Epoch 27/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.6586 - acc: 0.6424 - val_loss: 0.6433 - val_acc: 0.6719\n",
      "Epoch 28/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.6569 - acc: 0.6441 - val_loss: 0.6420 - val_acc: 0.6875\n",
      "Epoch 29/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.6553 - acc: 0.6406 - val_loss: 0.6407 - val_acc: 0.6823\n",
      "Epoch 30/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.6537 - acc: 0.6458 - val_loss: 0.6395 - val_acc: 0.6875\n",
      "Epoch 31/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.6522 - acc: 0.6476 - val_loss: 0.6383 - val_acc: 0.6875\n",
      "Epoch 32/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.6507 - acc: 0.6476 - val_loss: 0.6372 - val_acc: 0.6927\n",
      "Epoch 33/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.6492 - acc: 0.6458 - val_loss: 0.6361 - val_acc: 0.6875\n",
      "Epoch 34/1500\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.6479 - acc: 0.6476 - val_loss: 0.6350 - val_acc: 0.6875\n",
      "Epoch 35/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.6465 - acc: 0.6458 - val_loss: 0.6339 - val_acc: 0.6875\n",
      "Epoch 36/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.6452 - acc: 0.6458 - val_loss: 0.6329 - val_acc: 0.6823\n",
      "Epoch 37/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.6439 - acc: 0.6441 - val_loss: 0.6319 - val_acc: 0.6875\n",
      "Epoch 38/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.6426 - acc: 0.6441 - val_loss: 0.6309 - val_acc: 0.6875\n",
      "Epoch 39/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.6414 - acc: 0.6476 - val_loss: 0.6299 - val_acc: 0.6875\n",
      "Epoch 40/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.6402 - acc: 0.6493 - val_loss: 0.6290 - val_acc: 0.6823\n",
      "Epoch 41/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.6391 - acc: 0.6493 - val_loss: 0.6281 - val_acc: 0.6823\n",
      "Epoch 42/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.6380 - acc: 0.6528 - val_loss: 0.6272 - val_acc: 0.6823\n",
      "Epoch 43/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.6368 - acc: 0.6528 - val_loss: 0.6263 - val_acc: 0.6823\n",
      "Epoch 44/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.6358 - acc: 0.6528 - val_loss: 0.6255 - val_acc: 0.6823\n",
      "Epoch 45/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.6347 - acc: 0.6528 - val_loss: 0.6246 - val_acc: 0.6823\n",
      "Epoch 46/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.6337 - acc: 0.6528 - val_loss: 0.6238 - val_acc: 0.6823\n",
      "Epoch 47/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.6326 - acc: 0.6476 - val_loss: 0.6230 - val_acc: 0.6823\n",
      "Epoch 48/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.6316 - acc: 0.6510 - val_loss: 0.6222 - val_acc: 0.6823\n",
      "Epoch 49/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.6306 - acc: 0.6510 - val_loss: 0.6215 - val_acc: 0.6823\n",
      "Epoch 50/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.6296 - acc: 0.6510 - val_loss: 0.6207 - val_acc: 0.6771\n",
      "Epoch 51/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.6286 - acc: 0.6528 - val_loss: 0.6200 - val_acc: 0.6823\n",
      "Epoch 52/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.6276 - acc: 0.6493 - val_loss: 0.6193 - val_acc: 0.6823\n",
      "Epoch 53/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.6267 - acc: 0.6476 - val_loss: 0.6185 - val_acc: 0.6823\n",
      "Epoch 54/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.6257 - acc: 0.6458 - val_loss: 0.6178 - val_acc: 0.6823\n",
      "Epoch 55/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.6248 - acc: 0.6476 - val_loss: 0.6171 - val_acc: 0.6823\n",
      "Epoch 56/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.6238 - acc: 0.6476 - val_loss: 0.6164 - val_acc: 0.6823\n",
      "Epoch 57/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.6229 - acc: 0.6476 - val_loss: 0.6157 - val_acc: 0.6823\n",
      "Epoch 58/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.6220 - acc: 0.6510 - val_loss: 0.6150 - val_acc: 0.6823\n",
      "Epoch 59/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.6210 - acc: 0.6545 - val_loss: 0.6143 - val_acc: 0.6875\n",
      "Epoch 60/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.6202 - acc: 0.6528 - val_loss: 0.6137 - val_acc: 0.6875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/1500\n",
      "576/576 [==============================] - 0s 88us/step - loss: 0.6192 - acc: 0.6562 - val_loss: 0.6130 - val_acc: 0.6875\n",
      "Epoch 62/1500\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.6183 - acc: 0.6562 - val_loss: 0.6123 - val_acc: 0.6875\n",
      "Epoch 63/1500\n",
      "576/576 [==============================] - 0s 80us/step - loss: 0.6174 - acc: 0.6562 - val_loss: 0.6117 - val_acc: 0.6875\n",
      "Epoch 64/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.6166 - acc: 0.6562 - val_loss: 0.6110 - val_acc: 0.6875\n",
      "Epoch 65/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.6157 - acc: 0.6562 - val_loss: 0.6104 - val_acc: 0.6875\n",
      "Epoch 66/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.6148 - acc: 0.6580 - val_loss: 0.6097 - val_acc: 0.6875\n",
      "Epoch 67/1500\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.6140 - acc: 0.6580 - val_loss: 0.6091 - val_acc: 0.6875\n",
      "Epoch 68/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.6132 - acc: 0.6580 - val_loss: 0.6085 - val_acc: 0.6875\n",
      "Epoch 69/1500\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.6012 - acc: 0.656 - 0s 73us/step - loss: 0.6123 - acc: 0.6580 - val_loss: 0.6079 - val_acc: 0.6875\n",
      "Epoch 70/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.6115 - acc: 0.6597 - val_loss: 0.6072 - val_acc: 0.6875\n",
      "Epoch 71/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.6107 - acc: 0.6580 - val_loss: 0.6066 - val_acc: 0.6823\n",
      "Epoch 72/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.6099 - acc: 0.6597 - val_loss: 0.6060 - val_acc: 0.6823\n",
      "Epoch 73/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.6091 - acc: 0.6615 - val_loss: 0.6054 - val_acc: 0.6823\n",
      "Epoch 74/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.6083 - acc: 0.6615 - val_loss: 0.6048 - val_acc: 0.6823\n",
      "Epoch 75/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.6075 - acc: 0.6615 - val_loss: 0.6041 - val_acc: 0.6823\n",
      "Epoch 76/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.6067 - acc: 0.6615 - val_loss: 0.6035 - val_acc: 0.6823\n",
      "Epoch 77/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.6059 - acc: 0.6615 - val_loss: 0.6028 - val_acc: 0.6823\n",
      "Epoch 78/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.6052 - acc: 0.6615 - val_loss: 0.6022 - val_acc: 0.6823\n",
      "Epoch 79/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.6044 - acc: 0.6615 - val_loss: 0.6015 - val_acc: 0.6823\n",
      "Epoch 80/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.6036 - acc: 0.6615 - val_loss: 0.6009 - val_acc: 0.6823\n",
      "Epoch 81/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.6028 - acc: 0.6615 - val_loss: 0.6002 - val_acc: 0.6875\n",
      "Epoch 82/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.6021 - acc: 0.6615 - val_loss: 0.5995 - val_acc: 0.6875\n",
      "Epoch 83/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.6013 - acc: 0.6615 - val_loss: 0.5989 - val_acc: 0.6875\n",
      "Epoch 84/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.6006 - acc: 0.6615 - val_loss: 0.5982 - val_acc: 0.6875\n",
      "Epoch 85/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.5998 - acc: 0.6615 - val_loss: 0.5975 - val_acc: 0.6875\n",
      "Epoch 86/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.5990 - acc: 0.6615 - val_loss: 0.5969 - val_acc: 0.6875\n",
      "Epoch 87/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.5983 - acc: 0.6632 - val_loss: 0.5962 - val_acc: 0.6927\n",
      "Epoch 88/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.5975 - acc: 0.6632 - val_loss: 0.5956 - val_acc: 0.6927\n",
      "Epoch 89/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.5968 - acc: 0.6632 - val_loss: 0.5949 - val_acc: 0.6927\n",
      "Epoch 90/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.5960 - acc: 0.6632 - val_loss: 0.5943 - val_acc: 0.6927\n",
      "Epoch 91/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.5953 - acc: 0.6632 - val_loss: 0.5937 - val_acc: 0.6927\n",
      "Epoch 92/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.5946 - acc: 0.6649 - val_loss: 0.5930 - val_acc: 0.6927\n",
      "Epoch 93/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.5938 - acc: 0.6649 - val_loss: 0.5924 - val_acc: 0.6927\n",
      "Epoch 94/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.5931 - acc: 0.6667 - val_loss: 0.5918 - val_acc: 0.6927\n",
      "Epoch 95/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.5924 - acc: 0.6649 - val_loss: 0.5911 - val_acc: 0.6927\n",
      "Epoch 96/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.5916 - acc: 0.6667 - val_loss: 0.5905 - val_acc: 0.6927\n",
      "Epoch 97/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.5909 - acc: 0.6649 - val_loss: 0.5899 - val_acc: 0.6927\n",
      "Epoch 98/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.5902 - acc: 0.6667 - val_loss: 0.5892 - val_acc: 0.6927\n",
      "Epoch 99/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5895 - acc: 0.6684 - val_loss: 0.5886 - val_acc: 0.6927\n",
      "Epoch 100/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.5888 - acc: 0.6684 - val_loss: 0.5879 - val_acc: 0.6927\n",
      "Epoch 101/1500\n",
      "576/576 [==============================] - 0s 80us/step - loss: 0.5881 - acc: 0.6684 - val_loss: 0.5873 - val_acc: 0.6927\n",
      "Epoch 102/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.5874 - acc: 0.6684 - val_loss: 0.5866 - val_acc: 0.6927\n",
      "Epoch 103/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.5867 - acc: 0.6667 - val_loss: 0.5860 - val_acc: 0.6927\n",
      "Epoch 104/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.5860 - acc: 0.6667 - val_loss: 0.5853 - val_acc: 0.6927\n",
      "Epoch 105/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.5853 - acc: 0.6684 - val_loss: 0.5847 - val_acc: 0.6927\n",
      "Epoch 106/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.5846 - acc: 0.6684 - val_loss: 0.5840 - val_acc: 0.6927\n",
      "Epoch 107/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.5839 - acc: 0.6684 - val_loss: 0.5834 - val_acc: 0.6979\n",
      "Epoch 108/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.5832 - acc: 0.6701 - val_loss: 0.5827 - val_acc: 0.6979\n",
      "Epoch 109/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.5824 - acc: 0.6684 - val_loss: 0.5820 - val_acc: 0.6979\n",
      "Epoch 110/1500\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.6475 - acc: 0.562 - 0s 45us/step - loss: 0.5817 - acc: 0.6684 - val_loss: 0.5813 - val_acc: 0.6979\n",
      "Epoch 111/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.5810 - acc: 0.6701 - val_loss: 0.5806 - val_acc: 0.6979\n",
      "Epoch 112/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.5803 - acc: 0.6701 - val_loss: 0.5799 - val_acc: 0.6979\n",
      "Epoch 113/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.5796 - acc: 0.6701 - val_loss: 0.5793 - val_acc: 0.6927\n",
      "Epoch 114/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.5789 - acc: 0.6701 - val_loss: 0.5786 - val_acc: 0.6927\n",
      "Epoch 115/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.5782 - acc: 0.6701 - val_loss: 0.5779 - val_acc: 0.6927\n",
      "Epoch 116/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.5775 - acc: 0.6719 - val_loss: 0.5772 - val_acc: 0.6927\n",
      "Epoch 117/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.5767 - acc: 0.6719 - val_loss: 0.5765 - val_acc: 0.6979\n",
      "Epoch 118/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.5760 - acc: 0.6736 - val_loss: 0.5758 - val_acc: 0.6979\n",
      "Epoch 119/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.5752 - acc: 0.6753 - val_loss: 0.5751 - val_acc: 0.6979\n",
      "Epoch 120/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 47us/step - loss: 0.5744 - acc: 0.6753 - val_loss: 0.5743 - val_acc: 0.7031\n",
      "Epoch 121/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.5737 - acc: 0.6753 - val_loss: 0.5737 - val_acc: 0.7031\n",
      "Epoch 122/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.5729 - acc: 0.6753 - val_loss: 0.5730 - val_acc: 0.7031\n",
      "Epoch 123/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.5722 - acc: 0.6788 - val_loss: 0.5723 - val_acc: 0.7083\n",
      "Epoch 124/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.5715 - acc: 0.6788 - val_loss: 0.5716 - val_acc: 0.7083\n",
      "Epoch 125/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.5707 - acc: 0.6788 - val_loss: 0.5709 - val_acc: 0.7083\n",
      "Epoch 126/1500\n",
      "576/576 [==============================] - 0s 83us/step - loss: 0.5700 - acc: 0.6806 - val_loss: 0.5702 - val_acc: 0.7083\n",
      "Epoch 127/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.5693 - acc: 0.6788 - val_loss: 0.5695 - val_acc: 0.7083\n",
      "Epoch 128/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.5685 - acc: 0.6788 - val_loss: 0.5688 - val_acc: 0.7135\n",
      "Epoch 129/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.5678 - acc: 0.6823 - val_loss: 0.5681 - val_acc: 0.7135\n",
      "Epoch 130/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.5671 - acc: 0.6840 - val_loss: 0.5675 - val_acc: 0.7135\n",
      "Epoch 131/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.5664 - acc: 0.6823 - val_loss: 0.5668 - val_acc: 0.7135\n",
      "Epoch 132/1500\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.5656 - acc: 0.6823 - val_loss: 0.5661 - val_acc: 0.7135\n",
      "Epoch 133/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.5649 - acc: 0.6840 - val_loss: 0.5654 - val_acc: 0.7135\n",
      "Epoch 134/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.5642 - acc: 0.6840 - val_loss: 0.5648 - val_acc: 0.7135\n",
      "Epoch 135/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.5635 - acc: 0.6858 - val_loss: 0.5641 - val_acc: 0.7135\n",
      "Epoch 136/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.5628 - acc: 0.6840 - val_loss: 0.5635 - val_acc: 0.7135\n",
      "Epoch 137/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.5621 - acc: 0.6858 - val_loss: 0.5629 - val_acc: 0.7135\n",
      "Epoch 138/1500\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.5373 - acc: 0.656 - 0s 55us/step - loss: 0.5615 - acc: 0.6875 - val_loss: 0.5622 - val_acc: 0.7135\n",
      "Epoch 139/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.5608 - acc: 0.6892 - val_loss: 0.5616 - val_acc: 0.7135\n",
      "Epoch 140/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.5601 - acc: 0.6892 - val_loss: 0.5610 - val_acc: 0.7135\n",
      "Epoch 141/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.5594 - acc: 0.6910 - val_loss: 0.5603 - val_acc: 0.7135\n",
      "Epoch 142/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.5588 - acc: 0.6910 - val_loss: 0.5597 - val_acc: 0.7135\n",
      "Epoch 143/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.5581 - acc: 0.6910 - val_loss: 0.5591 - val_acc: 0.7135\n",
      "Epoch 144/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.5575 - acc: 0.6927 - val_loss: 0.5585 - val_acc: 0.7135\n",
      "Epoch 145/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.5568 - acc: 0.6944 - val_loss: 0.5579 - val_acc: 0.7188\n",
      "Epoch 146/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.5561 - acc: 0.6979 - val_loss: 0.5573 - val_acc: 0.7188\n",
      "Epoch 147/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.5555 - acc: 0.6962 - val_loss: 0.5567 - val_acc: 0.7188\n",
      "Epoch 148/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.5548 - acc: 0.7014 - val_loss: 0.5561 - val_acc: 0.7188\n",
      "Epoch 149/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.5542 - acc: 0.7014 - val_loss: 0.5555 - val_acc: 0.7188\n",
      "Epoch 150/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.5535 - acc: 0.7014 - val_loss: 0.5549 - val_acc: 0.7188\n",
      "Epoch 151/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.5529 - acc: 0.7014 - val_loss: 0.5542 - val_acc: 0.7188\n",
      "Epoch 152/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.5522 - acc: 0.7014 - val_loss: 0.5536 - val_acc: 0.7188\n",
      "Epoch 153/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.5516 - acc: 0.7031 - val_loss: 0.5530 - val_acc: 0.7188\n",
      "Epoch 154/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.5510 - acc: 0.7049 - val_loss: 0.5523 - val_acc: 0.7188\n",
      "Epoch 155/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.5504 - acc: 0.7049 - val_loss: 0.5517 - val_acc: 0.7188\n",
      "Epoch 156/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.5497 - acc: 0.7031 - val_loss: 0.5511 - val_acc: 0.7188\n",
      "Epoch 157/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.5491 - acc: 0.7031 - val_loss: 0.5505 - val_acc: 0.7188\n",
      "Epoch 158/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.5485 - acc: 0.7049 - val_loss: 0.5499 - val_acc: 0.7188\n",
      "Epoch 159/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.5479 - acc: 0.7066 - val_loss: 0.5493 - val_acc: 0.7188\n",
      "Epoch 160/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.5473 - acc: 0.7083 - val_loss: 0.5488 - val_acc: 0.7188\n",
      "Epoch 161/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.5467 - acc: 0.7066 - val_loss: 0.5482 - val_acc: 0.7240\n",
      "Epoch 162/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.5461 - acc: 0.7101 - val_loss: 0.5475 - val_acc: 0.7240\n",
      "Epoch 163/1500\n",
      "576/576 [==============================] - 0s 89us/step - loss: 0.5455 - acc: 0.7135 - val_loss: 0.5469 - val_acc: 0.7240\n",
      "Epoch 164/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.5448 - acc: 0.7135 - val_loss: 0.5463 - val_acc: 0.7292\n",
      "Epoch 165/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.5442 - acc: 0.7153 - val_loss: 0.5457 - val_acc: 0.7292\n",
      "Epoch 166/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5436 - acc: 0.7153 - val_loss: 0.5451 - val_acc: 0.7292\n",
      "Epoch 167/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.5430 - acc: 0.7153 - val_loss: 0.5446 - val_acc: 0.7292\n",
      "Epoch 168/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.5423 - acc: 0.7135 - val_loss: 0.5440 - val_acc: 0.7292\n",
      "Epoch 169/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.5417 - acc: 0.7135 - val_loss: 0.5434 - val_acc: 0.7292\n",
      "Epoch 170/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.5411 - acc: 0.7153 - val_loss: 0.5428 - val_acc: 0.7292\n",
      "Epoch 171/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.5405 - acc: 0.7153 - val_loss: 0.5423 - val_acc: 0.7292\n",
      "Epoch 172/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.5399 - acc: 0.7170 - val_loss: 0.5417 - val_acc: 0.7292\n",
      "Epoch 173/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.5393 - acc: 0.7170 - val_loss: 0.5411 - val_acc: 0.7292\n",
      "Epoch 174/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.5387 - acc: 0.7188 - val_loss: 0.5406 - val_acc: 0.7344\n",
      "Epoch 175/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.5381 - acc: 0.7205 - val_loss: 0.5400 - val_acc: 0.7344\n",
      "Epoch 176/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.5375 - acc: 0.7222 - val_loss: 0.5395 - val_acc: 0.7344\n",
      "Epoch 177/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.5369 - acc: 0.7274 - val_loss: 0.5389 - val_acc: 0.7344\n",
      "Epoch 178/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.5364 - acc: 0.7292 - val_loss: 0.5384 - val_acc: 0.7344\n",
      "Epoch 179/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 44us/step - loss: 0.5358 - acc: 0.7292 - val_loss: 0.5379 - val_acc: 0.7344\n",
      "Epoch 180/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.5352 - acc: 0.7292 - val_loss: 0.5373 - val_acc: 0.7344\n",
      "Epoch 181/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.5346 - acc: 0.7292 - val_loss: 0.5368 - val_acc: 0.7292\n",
      "Epoch 182/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.5341 - acc: 0.7292 - val_loss: 0.5363 - val_acc: 0.7292\n",
      "Epoch 183/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.5335 - acc: 0.7292 - val_loss: 0.5358 - val_acc: 0.7344\n",
      "Epoch 184/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.5329 - acc: 0.7326 - val_loss: 0.5352 - val_acc: 0.7344\n",
      "Epoch 185/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.5324 - acc: 0.7326 - val_loss: 0.5347 - val_acc: 0.7344\n",
      "Epoch 186/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.5318 - acc: 0.7292 - val_loss: 0.5342 - val_acc: 0.7344\n",
      "Epoch 187/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.5313 - acc: 0.7292 - val_loss: 0.5337 - val_acc: 0.7344\n",
      "Epoch 188/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.5307 - acc: 0.7309 - val_loss: 0.5332 - val_acc: 0.7396\n",
      "Epoch 189/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.5302 - acc: 0.7309 - val_loss: 0.5327 - val_acc: 0.7396\n",
      "Epoch 190/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.5297 - acc: 0.7309 - val_loss: 0.5323 - val_acc: 0.7500\n",
      "Epoch 191/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.5292 - acc: 0.7326 - val_loss: 0.5318 - val_acc: 0.7500\n",
      "Epoch 192/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.5287 - acc: 0.7344 - val_loss: 0.5314 - val_acc: 0.7500\n",
      "Epoch 193/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.5282 - acc: 0.7326 - val_loss: 0.5309 - val_acc: 0.7500\n",
      "Epoch 194/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.5276 - acc: 0.7344 - val_loss: 0.5305 - val_acc: 0.7448\n",
      "Epoch 195/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.5272 - acc: 0.7326 - val_loss: 0.5300 - val_acc: 0.7448\n",
      "Epoch 196/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.5266 - acc: 0.7326 - val_loss: 0.5296 - val_acc: 0.7396\n",
      "Epoch 197/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.5261 - acc: 0.7326 - val_loss: 0.5292 - val_acc: 0.7396\n",
      "Epoch 198/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.5257 - acc: 0.7326 - val_loss: 0.5288 - val_acc: 0.7552\n",
      "Epoch 199/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.5252 - acc: 0.7326 - val_loss: 0.5284 - val_acc: 0.7552\n",
      "Epoch 200/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5247 - acc: 0.7344 - val_loss: 0.5279 - val_acc: 0.7552\n",
      "Epoch 201/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.5242 - acc: 0.7326 - val_loss: 0.5275 - val_acc: 0.7604\n",
      "Epoch 202/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.5238 - acc: 0.7326 - val_loss: 0.5272 - val_acc: 0.7656\n",
      "Epoch 203/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.5233 - acc: 0.7326 - val_loss: 0.5268 - val_acc: 0.7656\n",
      "Epoch 204/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.5228 - acc: 0.7326 - val_loss: 0.5264 - val_acc: 0.7604\n",
      "Epoch 205/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.5224 - acc: 0.7344 - val_loss: 0.5260 - val_acc: 0.7552\n",
      "Epoch 206/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.5219 - acc: 0.7344 - val_loss: 0.5256 - val_acc: 0.7552\n",
      "Epoch 207/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.5215 - acc: 0.7361 - val_loss: 0.5252 - val_acc: 0.7552\n",
      "Epoch 208/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.5210 - acc: 0.7344 - val_loss: 0.5249 - val_acc: 0.7552\n",
      "Epoch 209/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5206 - acc: 0.7344 - val_loss: 0.5245 - val_acc: 0.7552\n",
      "Epoch 210/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5202 - acc: 0.7361 - val_loss: 0.5241 - val_acc: 0.7552\n",
      "Epoch 211/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.5197 - acc: 0.7361 - val_loss: 0.5237 - val_acc: 0.7552\n",
      "Epoch 212/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.5193 - acc: 0.7361 - val_loss: 0.5234 - val_acc: 0.7500\n",
      "Epoch 213/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.5188 - acc: 0.7344 - val_loss: 0.5230 - val_acc: 0.7500\n",
      "Epoch 214/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.5184 - acc: 0.7344 - val_loss: 0.5226 - val_acc: 0.7500\n",
      "Epoch 215/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.5180 - acc: 0.7344 - val_loss: 0.5223 - val_acc: 0.7500\n",
      "Epoch 216/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5176 - acc: 0.7326 - val_loss: 0.5219 - val_acc: 0.7500\n",
      "Epoch 217/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.5171 - acc: 0.7344 - val_loss: 0.5216 - val_acc: 0.7500\n",
      "Epoch 218/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.5167 - acc: 0.7326 - val_loss: 0.5212 - val_acc: 0.7500\n",
      "Epoch 219/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5163 - acc: 0.7326 - val_loss: 0.5208 - val_acc: 0.7500\n",
      "Epoch 220/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.5159 - acc: 0.7326 - val_loss: 0.5205 - val_acc: 0.7500\n",
      "Epoch 221/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.5155 - acc: 0.7309 - val_loss: 0.5201 - val_acc: 0.7500\n",
      "Epoch 222/1500\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.5151 - acc: 0.7309 - val_loss: 0.5198 - val_acc: 0.7448\n",
      "Epoch 223/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.5148 - acc: 0.7292 - val_loss: 0.5195 - val_acc: 0.7448\n",
      "Epoch 224/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.5144 - acc: 0.7326 - val_loss: 0.5191 - val_acc: 0.7448\n",
      "Epoch 225/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.5140 - acc: 0.7344 - val_loss: 0.5188 - val_acc: 0.7448\n",
      "Epoch 226/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.5137 - acc: 0.7326 - val_loss: 0.5184 - val_acc: 0.7448\n",
      "Epoch 227/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.5134 - acc: 0.7326 - val_loss: 0.5181 - val_acc: 0.7500\n",
      "Epoch 228/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5130 - acc: 0.7344 - val_loss: 0.5178 - val_acc: 0.7500\n",
      "Epoch 229/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.5127 - acc: 0.7344 - val_loss: 0.5175 - val_acc: 0.7500\n",
      "Epoch 230/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.5124 - acc: 0.7344 - val_loss: 0.5172 - val_acc: 0.7552\n",
      "Epoch 231/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.5120 - acc: 0.7361 - val_loss: 0.5168 - val_acc: 0.7552\n",
      "Epoch 232/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.5117 - acc: 0.7361 - val_loss: 0.5165 - val_acc: 0.7552\n",
      "Epoch 233/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5114 - acc: 0.7361 - val_loss: 0.5162 - val_acc: 0.7552\n",
      "Epoch 234/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.5111 - acc: 0.7361 - val_loss: 0.5159 - val_acc: 0.7552\n",
      "Epoch 235/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.5108 - acc: 0.7361 - val_loss: 0.5157 - val_acc: 0.7552\n",
      "Epoch 236/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5105 - acc: 0.7361 - val_loss: 0.5154 - val_acc: 0.7552\n",
      "Epoch 237/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.5102 - acc: 0.7361 - val_loss: 0.5151 - val_acc: 0.7552\n",
      "Epoch 238/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.5099 - acc: 0.7361 - val_loss: 0.5148 - val_acc: 0.7552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 239/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.5096 - acc: 0.7361 - val_loss: 0.5145 - val_acc: 0.7552\n",
      "Epoch 240/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.5093 - acc: 0.7361 - val_loss: 0.5142 - val_acc: 0.7552\n",
      "Epoch 241/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.5090 - acc: 0.7361 - val_loss: 0.5139 - val_acc: 0.7552\n",
      "Epoch 242/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.5088 - acc: 0.7344 - val_loss: 0.5137 - val_acc: 0.7552\n",
      "Epoch 243/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.5085 - acc: 0.7361 - val_loss: 0.5134 - val_acc: 0.7552\n",
      "Epoch 244/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.5082 - acc: 0.7361 - val_loss: 0.5131 - val_acc: 0.7552\n",
      "Epoch 245/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5079 - acc: 0.7361 - val_loss: 0.5129 - val_acc: 0.7552\n",
      "Epoch 246/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.5077 - acc: 0.7361 - val_loss: 0.5126 - val_acc: 0.7552\n",
      "Epoch 247/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.5074 - acc: 0.7361 - val_loss: 0.5123 - val_acc: 0.7552\n",
      "Epoch 248/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.5072 - acc: 0.7361 - val_loss: 0.5121 - val_acc: 0.7552\n",
      "Epoch 249/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.5069 - acc: 0.7361 - val_loss: 0.5118 - val_acc: 0.7604\n",
      "Epoch 250/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.5066 - acc: 0.7361 - val_loss: 0.5116 - val_acc: 0.7604\n",
      "Epoch 251/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.5064 - acc: 0.7378 - val_loss: 0.5113 - val_acc: 0.7604\n",
      "Epoch 252/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.5061 - acc: 0.7378 - val_loss: 0.5110 - val_acc: 0.7604\n",
      "Epoch 253/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.5058 - acc: 0.7396 - val_loss: 0.5108 - val_acc: 0.7604\n",
      "Epoch 254/1500\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.5056 - acc: 0.7396 - val_loss: 0.5105 - val_acc: 0.7604\n",
      "Epoch 255/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.5053 - acc: 0.7396 - val_loss: 0.5103 - val_acc: 0.7604\n",
      "Epoch 256/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.5050 - acc: 0.7396 - val_loss: 0.5100 - val_acc: 0.7604\n",
      "Epoch 257/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.5048 - acc: 0.7396 - val_loss: 0.5098 - val_acc: 0.7604\n",
      "Epoch 258/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.5045 - acc: 0.7396 - val_loss: 0.5095 - val_acc: 0.7604\n",
      "Epoch 259/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.5043 - acc: 0.7413 - val_loss: 0.5093 - val_acc: 0.7604\n",
      "Epoch 260/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.5040 - acc: 0.7413 - val_loss: 0.5090 - val_acc: 0.7604\n",
      "Epoch 261/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.5037 - acc: 0.7396 - val_loss: 0.5088 - val_acc: 0.7604\n",
      "Epoch 262/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.5035 - acc: 0.7378 - val_loss: 0.5086 - val_acc: 0.7604\n",
      "Epoch 263/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.5032 - acc: 0.7396 - val_loss: 0.5083 - val_acc: 0.7656\n",
      "Epoch 264/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.5030 - acc: 0.7361 - val_loss: 0.5081 - val_acc: 0.7708\n",
      "Epoch 265/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.5027 - acc: 0.7396 - val_loss: 0.5078 - val_acc: 0.7708\n",
      "Epoch 266/1500\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.4072 - acc: 0.812 - 0s 50us/step - loss: 0.5025 - acc: 0.7396 - val_loss: 0.5076 - val_acc: 0.7708\n",
      "Epoch 267/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.5022 - acc: 0.7378 - val_loss: 0.5073 - val_acc: 0.7708\n",
      "Epoch 268/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.5020 - acc: 0.7378 - val_loss: 0.5071 - val_acc: 0.7708\n",
      "Epoch 269/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.5018 - acc: 0.7378 - val_loss: 0.5069 - val_acc: 0.7708\n",
      "Epoch 270/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.5015 - acc: 0.7378 - val_loss: 0.5066 - val_acc: 0.7656\n",
      "Epoch 271/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.5012 - acc: 0.7361 - val_loss: 0.5064 - val_acc: 0.7656\n",
      "Epoch 272/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.5010 - acc: 0.7361 - val_loss: 0.5062 - val_acc: 0.7708\n",
      "Epoch 273/1500\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.5007 - acc: 0.7361 - val_loss: 0.5060 - val_acc: 0.7708\n",
      "Epoch 274/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.5005 - acc: 0.7361 - val_loss: 0.5057 - val_acc: 0.7760\n",
      "Epoch 275/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5003 - acc: 0.7361 - val_loss: 0.5055 - val_acc: 0.7760\n",
      "Epoch 276/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.5000 - acc: 0.7361 - val_loss: 0.5053 - val_acc: 0.7760\n",
      "Epoch 277/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4998 - acc: 0.7344 - val_loss: 0.5051 - val_acc: 0.7760\n",
      "Epoch 278/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4996 - acc: 0.7344 - val_loss: 0.5049 - val_acc: 0.7760\n",
      "Epoch 279/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4993 - acc: 0.7344 - val_loss: 0.5047 - val_acc: 0.7760\n",
      "Epoch 280/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4991 - acc: 0.7326 - val_loss: 0.5045 - val_acc: 0.7760\n",
      "Epoch 281/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4989 - acc: 0.7326 - val_loss: 0.5043 - val_acc: 0.7760\n",
      "Epoch 282/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4986 - acc: 0.7326 - val_loss: 0.5041 - val_acc: 0.7760\n",
      "Epoch 283/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4984 - acc: 0.7344 - val_loss: 0.5039 - val_acc: 0.7760\n",
      "Epoch 284/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4982 - acc: 0.7344 - val_loss: 0.5037 - val_acc: 0.7760\n",
      "Epoch 285/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4980 - acc: 0.7344 - val_loss: 0.5035 - val_acc: 0.7760\n",
      "Epoch 286/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4978 - acc: 0.7344 - val_loss: 0.5033 - val_acc: 0.7760\n",
      "Epoch 287/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4975 - acc: 0.7326 - val_loss: 0.5031 - val_acc: 0.7760\n",
      "Epoch 288/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4973 - acc: 0.7326 - val_loss: 0.5029 - val_acc: 0.7708\n",
      "Epoch 289/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4971 - acc: 0.7326 - val_loss: 0.5027 - val_acc: 0.7708\n",
      "Epoch 290/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4969 - acc: 0.7326 - val_loss: 0.5026 - val_acc: 0.7708\n",
      "Epoch 291/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4967 - acc: 0.7344 - val_loss: 0.5024 - val_acc: 0.7708\n",
      "Epoch 292/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4965 - acc: 0.7344 - val_loss: 0.5022 - val_acc: 0.7708\n",
      "Epoch 293/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4963 - acc: 0.7344 - val_loss: 0.5020 - val_acc: 0.7656\n",
      "Epoch 294/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4961 - acc: 0.7378 - val_loss: 0.5018 - val_acc: 0.7656\n",
      "Epoch 295/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4959 - acc: 0.7378 - val_loss: 0.5017 - val_acc: 0.7656\n",
      "Epoch 296/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4957 - acc: 0.7378 - val_loss: 0.5015 - val_acc: 0.7656\n",
      "Epoch 297/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4955 - acc: 0.7378 - val_loss: 0.5014 - val_acc: 0.7656\n",
      "Epoch 298/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 55us/step - loss: 0.4953 - acc: 0.7378 - val_loss: 0.5012 - val_acc: 0.7656\n",
      "Epoch 299/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4951 - acc: 0.7378 - val_loss: 0.5010 - val_acc: 0.7656\n",
      "Epoch 300/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4949 - acc: 0.7378 - val_loss: 0.5009 - val_acc: 0.7656\n",
      "Epoch 301/1500\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4947 - acc: 0.7378 - val_loss: 0.5007 - val_acc: 0.7656\n",
      "Epoch 302/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4945 - acc: 0.7378 - val_loss: 0.5006 - val_acc: 0.7708\n",
      "Epoch 303/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4943 - acc: 0.7396 - val_loss: 0.5004 - val_acc: 0.7708\n",
      "Epoch 304/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4941 - acc: 0.7396 - val_loss: 0.5003 - val_acc: 0.7708\n",
      "Epoch 305/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4939 - acc: 0.7413 - val_loss: 0.5001 - val_acc: 0.7708\n",
      "Epoch 306/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4937 - acc: 0.7413 - val_loss: 0.4999 - val_acc: 0.7708\n",
      "Epoch 307/1500\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4935 - acc: 0.7413 - val_loss: 0.4998 - val_acc: 0.7708\n",
      "Epoch 308/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4933 - acc: 0.7413 - val_loss: 0.4996 - val_acc: 0.7708\n",
      "Epoch 309/1500\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4932 - acc: 0.7413 - val_loss: 0.4995 - val_acc: 0.7708\n",
      "Epoch 310/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4930 - acc: 0.7413 - val_loss: 0.4993 - val_acc: 0.7708\n",
      "Epoch 311/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4928 - acc: 0.7413 - val_loss: 0.4992 - val_acc: 0.7708\n",
      "Epoch 312/1500\n",
      "576/576 [==============================] - 0s 92us/step - loss: 0.4926 - acc: 0.7413 - val_loss: 0.4991 - val_acc: 0.7708\n",
      "Epoch 313/1500\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4924 - acc: 0.7413 - val_loss: 0.4989 - val_acc: 0.7708\n",
      "Epoch 314/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4923 - acc: 0.7431 - val_loss: 0.4988 - val_acc: 0.7708\n",
      "Epoch 315/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4921 - acc: 0.7431 - val_loss: 0.4986 - val_acc: 0.7708\n",
      "Epoch 316/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4919 - acc: 0.7431 - val_loss: 0.4985 - val_acc: 0.7708\n",
      "Epoch 317/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4917 - acc: 0.7431 - val_loss: 0.4983 - val_acc: 0.7708\n",
      "Epoch 318/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4915 - acc: 0.7431 - val_loss: 0.4982 - val_acc: 0.7760\n",
      "Epoch 319/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4913 - acc: 0.7431 - val_loss: 0.4981 - val_acc: 0.7760\n",
      "Epoch 320/1500\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4912 - acc: 0.7448 - val_loss: 0.4979 - val_acc: 0.7760\n",
      "Epoch 321/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4910 - acc: 0.7448 - val_loss: 0.4978 - val_acc: 0.7760\n",
      "Epoch 322/1500\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4908 - acc: 0.7448 - val_loss: 0.4976 - val_acc: 0.7760\n",
      "Epoch 323/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4906 - acc: 0.7448 - val_loss: 0.4975 - val_acc: 0.7760\n",
      "Epoch 324/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4904 - acc: 0.7465 - val_loss: 0.4974 - val_acc: 0.7760\n",
      "Epoch 325/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4903 - acc: 0.7465 - val_loss: 0.4972 - val_acc: 0.7760\n",
      "Epoch 326/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4901 - acc: 0.7465 - val_loss: 0.4971 - val_acc: 0.7760\n",
      "Epoch 327/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4899 - acc: 0.7465 - val_loss: 0.4969 - val_acc: 0.7760\n",
      "Epoch 328/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4897 - acc: 0.7465 - val_loss: 0.4968 - val_acc: 0.7760\n",
      "Epoch 329/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4895 - acc: 0.7465 - val_loss: 0.4967 - val_acc: 0.7760\n",
      "Epoch 330/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4894 - acc: 0.7465 - val_loss: 0.4965 - val_acc: 0.7760\n",
      "Epoch 331/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4892 - acc: 0.7465 - val_loss: 0.4964 - val_acc: 0.7760\n",
      "Epoch 332/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4890 - acc: 0.7483 - val_loss: 0.4963 - val_acc: 0.7760\n",
      "Epoch 333/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4889 - acc: 0.7483 - val_loss: 0.4962 - val_acc: 0.7760\n",
      "Epoch 334/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4887 - acc: 0.7483 - val_loss: 0.4960 - val_acc: 0.7760\n",
      "Epoch 335/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4885 - acc: 0.7483 - val_loss: 0.4959 - val_acc: 0.7760\n",
      "Epoch 336/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4883 - acc: 0.7500 - val_loss: 0.4958 - val_acc: 0.7760\n",
      "Epoch 337/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4882 - acc: 0.7500 - val_loss: 0.4957 - val_acc: 0.7760\n",
      "Epoch 338/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4880 - acc: 0.7517 - val_loss: 0.4956 - val_acc: 0.7760\n",
      "Epoch 339/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4878 - acc: 0.7517 - val_loss: 0.4955 - val_acc: 0.7760\n",
      "Epoch 340/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4876 - acc: 0.7517 - val_loss: 0.4953 - val_acc: 0.7760\n",
      "Epoch 341/1500\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4875 - acc: 0.7517 - val_loss: 0.4952 - val_acc: 0.7760\n",
      "Epoch 342/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4873 - acc: 0.7517 - val_loss: 0.4951 - val_acc: 0.7760\n",
      "Epoch 343/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4871 - acc: 0.7517 - val_loss: 0.4950 - val_acc: 0.7760\n",
      "Epoch 344/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4870 - acc: 0.7517 - val_loss: 0.4949 - val_acc: 0.7812\n",
      "Epoch 345/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4868 - acc: 0.7517 - val_loss: 0.4947 - val_acc: 0.7812\n",
      "Epoch 346/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4866 - acc: 0.7517 - val_loss: 0.4946 - val_acc: 0.7865\n",
      "Epoch 347/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4864 - acc: 0.7535 - val_loss: 0.4945 - val_acc: 0.7865\n",
      "Epoch 348/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4863 - acc: 0.7535 - val_loss: 0.4944 - val_acc: 0.7865\n",
      "Epoch 349/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4861 - acc: 0.7535 - val_loss: 0.4943 - val_acc: 0.7865\n",
      "Epoch 350/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4860 - acc: 0.7535 - val_loss: 0.4942 - val_acc: 0.7865\n",
      "Epoch 351/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4858 - acc: 0.7535 - val_loss: 0.4941 - val_acc: 0.7865\n",
      "Epoch 352/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4856 - acc: 0.7535 - val_loss: 0.4940 - val_acc: 0.7865\n",
      "Epoch 353/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4855 - acc: 0.7552 - val_loss: 0.4939 - val_acc: 0.7865\n",
      "Epoch 354/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4853 - acc: 0.7552 - val_loss: 0.4938 - val_acc: 0.7865\n",
      "Epoch 355/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4852 - acc: 0.7552 - val_loss: 0.4937 - val_acc: 0.7865\n",
      "Epoch 356/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4850 - acc: 0.7535 - val_loss: 0.4935 - val_acc: 0.7865\n",
      "Epoch 357/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4849 - acc: 0.7552 - val_loss: 0.4935 - val_acc: 0.7865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 358/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4847 - acc: 0.7552 - val_loss: 0.4934 - val_acc: 0.7865\n",
      "Epoch 359/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4846 - acc: 0.7535 - val_loss: 0.4933 - val_acc: 0.7865\n",
      "Epoch 360/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4844 - acc: 0.7535 - val_loss: 0.4932 - val_acc: 0.7865\n",
      "Epoch 361/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4843 - acc: 0.7517 - val_loss: 0.4931 - val_acc: 0.7865\n",
      "Epoch 362/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4841 - acc: 0.7517 - val_loss: 0.4930 - val_acc: 0.7865\n",
      "Epoch 363/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4840 - acc: 0.7517 - val_loss: 0.4929 - val_acc: 0.7865\n",
      "Epoch 364/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4838 - acc: 0.7517 - val_loss: 0.4928 - val_acc: 0.7865\n",
      "Epoch 365/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4836 - acc: 0.7517 - val_loss: 0.4927 - val_acc: 0.7865\n",
      "Epoch 366/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4835 - acc: 0.7517 - val_loss: 0.4926 - val_acc: 0.7865\n",
      "Epoch 367/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4833 - acc: 0.7517 - val_loss: 0.4925 - val_acc: 0.7865\n",
      "Epoch 368/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4832 - acc: 0.7517 - val_loss: 0.4924 - val_acc: 0.7865\n",
      "Epoch 369/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4830 - acc: 0.7517 - val_loss: 0.4923 - val_acc: 0.7865\n",
      "Epoch 370/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4829 - acc: 0.7517 - val_loss: 0.4922 - val_acc: 0.7865\n",
      "Epoch 371/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4828 - acc: 0.7535 - val_loss: 0.4921 - val_acc: 0.7865\n",
      "Epoch 372/1500\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.4826 - acc: 0.7535 - val_loss: 0.4920 - val_acc: 0.7865\n",
      "Epoch 373/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4824 - acc: 0.7535 - val_loss: 0.4919 - val_acc: 0.7865\n",
      "Epoch 374/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4823 - acc: 0.7535 - val_loss: 0.4918 - val_acc: 0.7865\n",
      "Epoch 375/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4821 - acc: 0.7535 - val_loss: 0.4918 - val_acc: 0.7865\n",
      "Epoch 376/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4820 - acc: 0.7535 - val_loss: 0.4917 - val_acc: 0.7865\n",
      "Epoch 377/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4818 - acc: 0.7535 - val_loss: 0.4916 - val_acc: 0.7865\n",
      "Epoch 378/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4817 - acc: 0.7535 - val_loss: 0.4915 - val_acc: 0.7865\n",
      "Epoch 379/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4815 - acc: 0.7535 - val_loss: 0.4914 - val_acc: 0.7865\n",
      "Epoch 380/1500\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4814 - acc: 0.7535 - val_loss: 0.4913 - val_acc: 0.7865\n",
      "Epoch 381/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4813 - acc: 0.7535 - val_loss: 0.4912 - val_acc: 0.7865\n",
      "Epoch 382/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4811 - acc: 0.7535 - val_loss: 0.4912 - val_acc: 0.7865\n",
      "Epoch 383/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4810 - acc: 0.7535 - val_loss: 0.4911 - val_acc: 0.7865\n",
      "Epoch 384/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4809 - acc: 0.7535 - val_loss: 0.4910 - val_acc: 0.7865\n",
      "Epoch 385/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4807 - acc: 0.7535 - val_loss: 0.4909 - val_acc: 0.7865\n",
      "Epoch 386/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4806 - acc: 0.7535 - val_loss: 0.4909 - val_acc: 0.7865\n",
      "Epoch 387/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4804 - acc: 0.7535 - val_loss: 0.4908 - val_acc: 0.7865\n",
      "Epoch 388/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4803 - acc: 0.7535 - val_loss: 0.4907 - val_acc: 0.7865\n",
      "Epoch 389/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4802 - acc: 0.7552 - val_loss: 0.4907 - val_acc: 0.7865\n",
      "Epoch 390/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4801 - acc: 0.7552 - val_loss: 0.4906 - val_acc: 0.7865\n",
      "Epoch 391/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4799 - acc: 0.7552 - val_loss: 0.4905 - val_acc: 0.7865\n",
      "Epoch 392/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4798 - acc: 0.7552 - val_loss: 0.4905 - val_acc: 0.7865\n",
      "Epoch 393/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4797 - acc: 0.7552 - val_loss: 0.4904 - val_acc: 0.7865\n",
      "Epoch 394/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4796 - acc: 0.7552 - val_loss: 0.4904 - val_acc: 0.7865\n",
      "Epoch 395/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4794 - acc: 0.7552 - val_loss: 0.4903 - val_acc: 0.7865\n",
      "Epoch 396/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4793 - acc: 0.7552 - val_loss: 0.4902 - val_acc: 0.7865\n",
      "Epoch 397/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4792 - acc: 0.7552 - val_loss: 0.4902 - val_acc: 0.7865\n",
      "Epoch 398/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4791 - acc: 0.7535 - val_loss: 0.4901 - val_acc: 0.7865\n",
      "Epoch 399/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4790 - acc: 0.7552 - val_loss: 0.4901 - val_acc: 0.7865\n",
      "Epoch 400/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4789 - acc: 0.7552 - val_loss: 0.4900 - val_acc: 0.7865\n",
      "Epoch 401/1500\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.4787 - acc: 0.7535 - val_loss: 0.4900 - val_acc: 0.7865\n",
      "Epoch 402/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4786 - acc: 0.7552 - val_loss: 0.4899 - val_acc: 0.7865\n",
      "Epoch 403/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4785 - acc: 0.7552 - val_loss: 0.4899 - val_acc: 0.7865\n",
      "Epoch 404/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4784 - acc: 0.7552 - val_loss: 0.4898 - val_acc: 0.7865\n",
      "Epoch 405/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4783 - acc: 0.7552 - val_loss: 0.4897 - val_acc: 0.7865\n",
      "Epoch 406/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4782 - acc: 0.7552 - val_loss: 0.4897 - val_acc: 0.7865\n",
      "Epoch 407/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4780 - acc: 0.7535 - val_loss: 0.4896 - val_acc: 0.7865\n",
      "Epoch 408/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4779 - acc: 0.7535 - val_loss: 0.4896 - val_acc: 0.7865\n",
      "Epoch 409/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4778 - acc: 0.7535 - val_loss: 0.4895 - val_acc: 0.7865\n",
      "Epoch 410/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4777 - acc: 0.7535 - val_loss: 0.4894 - val_acc: 0.7865\n",
      "Epoch 411/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4775 - acc: 0.7535 - val_loss: 0.4894 - val_acc: 0.7865\n",
      "Epoch 412/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4774 - acc: 0.7535 - val_loss: 0.4893 - val_acc: 0.7865\n",
      "Epoch 413/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4773 - acc: 0.7552 - val_loss: 0.4893 - val_acc: 0.7865\n",
      "Epoch 414/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4772 - acc: 0.7552 - val_loss: 0.4892 - val_acc: 0.7865\n",
      "Epoch 415/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4771 - acc: 0.7552 - val_loss: 0.4892 - val_acc: 0.7865\n",
      "Epoch 416/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4769 - acc: 0.7552 - val_loss: 0.4891 - val_acc: 0.7917\n",
      "Epoch 417/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4768 - acc: 0.7552 - val_loss: 0.4890 - val_acc: 0.7917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 418/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4767 - acc: 0.7552 - val_loss: 0.4890 - val_acc: 0.7917\n",
      "Epoch 419/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4766 - acc: 0.7552 - val_loss: 0.4890 - val_acc: 0.7969\n",
      "Epoch 420/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4765 - acc: 0.7552 - val_loss: 0.4889 - val_acc: 0.7969\n",
      "Epoch 421/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4764 - acc: 0.7552 - val_loss: 0.4889 - val_acc: 0.7969\n",
      "Epoch 422/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4763 - acc: 0.7552 - val_loss: 0.4888 - val_acc: 0.7969\n",
      "Epoch 423/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4762 - acc: 0.7552 - val_loss: 0.4888 - val_acc: 0.7969\n",
      "Epoch 424/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4761 - acc: 0.7552 - val_loss: 0.4887 - val_acc: 0.7969\n",
      "Epoch 425/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4760 - acc: 0.7552 - val_loss: 0.4887 - val_acc: 0.7969\n",
      "Epoch 426/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4759 - acc: 0.7552 - val_loss: 0.4886 - val_acc: 0.7969\n",
      "Epoch 427/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4758 - acc: 0.7552 - val_loss: 0.4885 - val_acc: 0.7969\n",
      "Epoch 428/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4757 - acc: 0.7552 - val_loss: 0.4885 - val_acc: 0.7969\n",
      "Epoch 429/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4756 - acc: 0.7552 - val_loss: 0.4885 - val_acc: 0.7969\n",
      "Epoch 430/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4755 - acc: 0.7552 - val_loss: 0.4884 - val_acc: 0.7969\n",
      "Epoch 431/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4754 - acc: 0.7552 - val_loss: 0.4884 - val_acc: 0.7969\n",
      "Epoch 432/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4753 - acc: 0.7535 - val_loss: 0.4883 - val_acc: 0.7969\n",
      "Epoch 433/1500\n",
      "576/576 [==============================] - 0s 83us/step - loss: 0.4752 - acc: 0.7535 - val_loss: 0.4883 - val_acc: 0.7969\n",
      "Epoch 434/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4751 - acc: 0.7535 - val_loss: 0.4882 - val_acc: 0.7969\n",
      "Epoch 435/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4750 - acc: 0.7535 - val_loss: 0.4882 - val_acc: 0.7969\n",
      "Epoch 436/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4749 - acc: 0.7535 - val_loss: 0.4881 - val_acc: 0.7969\n",
      "Epoch 437/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4748 - acc: 0.7535 - val_loss: 0.4881 - val_acc: 0.7969\n",
      "Epoch 438/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4747 - acc: 0.7535 - val_loss: 0.4881 - val_acc: 0.7969\n",
      "Epoch 439/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4746 - acc: 0.7535 - val_loss: 0.4880 - val_acc: 0.7969\n",
      "Epoch 440/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4745 - acc: 0.7552 - val_loss: 0.4880 - val_acc: 0.7969\n",
      "Epoch 441/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4744 - acc: 0.7535 - val_loss: 0.4879 - val_acc: 0.7969\n",
      "Epoch 442/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4743 - acc: 0.7552 - val_loss: 0.4879 - val_acc: 0.7969\n",
      "Epoch 443/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4742 - acc: 0.7535 - val_loss: 0.4879 - val_acc: 0.7969\n",
      "Epoch 444/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4742 - acc: 0.7552 - val_loss: 0.4878 - val_acc: 0.7969\n",
      "Epoch 445/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4741 - acc: 0.7552 - val_loss: 0.4878 - val_acc: 0.7969\n",
      "Epoch 446/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4740 - acc: 0.7552 - val_loss: 0.4878 - val_acc: 0.7969\n",
      "Epoch 447/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4739 - acc: 0.7569 - val_loss: 0.4877 - val_acc: 0.7969\n",
      "Epoch 448/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4738 - acc: 0.7587 - val_loss: 0.4877 - val_acc: 0.7969\n",
      "Epoch 449/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4737 - acc: 0.7569 - val_loss: 0.4876 - val_acc: 0.7969\n",
      "Epoch 450/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4736 - acc: 0.7569 - val_loss: 0.4876 - val_acc: 0.7969\n",
      "Epoch 451/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4735 - acc: 0.7587 - val_loss: 0.4876 - val_acc: 0.7969\n",
      "Epoch 452/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4734 - acc: 0.7569 - val_loss: 0.4875 - val_acc: 0.7969\n",
      "Epoch 453/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4733 - acc: 0.7569 - val_loss: 0.4875 - val_acc: 0.7969\n",
      "Epoch 454/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4732 - acc: 0.7569 - val_loss: 0.4875 - val_acc: 0.7969\n",
      "Epoch 455/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4731 - acc: 0.7587 - val_loss: 0.4874 - val_acc: 0.7969\n",
      "Epoch 456/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4730 - acc: 0.7569 - val_loss: 0.4874 - val_acc: 0.7969\n",
      "Epoch 457/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4729 - acc: 0.7569 - val_loss: 0.4874 - val_acc: 0.7969\n",
      "Epoch 458/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4728 - acc: 0.7587 - val_loss: 0.4873 - val_acc: 0.7969\n",
      "Epoch 459/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4727 - acc: 0.7587 - val_loss: 0.4873 - val_acc: 0.7969\n",
      "Epoch 460/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4726 - acc: 0.7587 - val_loss: 0.4872 - val_acc: 0.7969\n",
      "Epoch 461/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4725 - acc: 0.7569 - val_loss: 0.4872 - val_acc: 0.7969\n",
      "Epoch 462/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4724 - acc: 0.7587 - val_loss: 0.4872 - val_acc: 0.7969\n",
      "Epoch 463/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4723 - acc: 0.7569 - val_loss: 0.4871 - val_acc: 0.7969\n",
      "Epoch 464/1500\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.4723 - acc: 0.7569 - val_loss: 0.4871 - val_acc: 0.7969\n",
      "Epoch 465/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4722 - acc: 0.7587 - val_loss: 0.4871 - val_acc: 0.7969\n",
      "Epoch 466/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4721 - acc: 0.7587 - val_loss: 0.4870 - val_acc: 0.7969\n",
      "Epoch 467/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4720 - acc: 0.7587 - val_loss: 0.4870 - val_acc: 0.7969\n",
      "Epoch 468/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4719 - acc: 0.7587 - val_loss: 0.4869 - val_acc: 0.7969\n",
      "Epoch 469/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4718 - acc: 0.7587 - val_loss: 0.4869 - val_acc: 0.7969\n",
      "Epoch 470/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4717 - acc: 0.7587 - val_loss: 0.4869 - val_acc: 0.7969\n",
      "Epoch 471/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4716 - acc: 0.7587 - val_loss: 0.4868 - val_acc: 0.7969\n",
      "Epoch 472/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4715 - acc: 0.7587 - val_loss: 0.4868 - val_acc: 0.7969\n",
      "Epoch 473/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4714 - acc: 0.7587 - val_loss: 0.4868 - val_acc: 0.7969\n",
      "Epoch 474/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4713 - acc: 0.7587 - val_loss: 0.4867 - val_acc: 0.7969\n",
      "Epoch 475/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4713 - acc: 0.7587 - val_loss: 0.4867 - val_acc: 0.7969\n",
      "Epoch 476/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4711 - acc: 0.7587 - val_loss: 0.4867 - val_acc: 0.7969\n",
      "Epoch 477/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4711 - acc: 0.7587 - val_loss: 0.4866 - val_acc: 0.7969\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 478/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4710 - acc: 0.7587 - val_loss: 0.4866 - val_acc: 0.7969\n",
      "Epoch 479/1500\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4709 - acc: 0.7587 - val_loss: 0.4866 - val_acc: 0.7969\n",
      "Epoch 480/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4708 - acc: 0.7587 - val_loss: 0.4865 - val_acc: 0.7969\n",
      "Epoch 481/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4707 - acc: 0.7587 - val_loss: 0.4865 - val_acc: 0.7969\n",
      "Epoch 482/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4706 - acc: 0.7587 - val_loss: 0.4865 - val_acc: 0.7969\n",
      "Epoch 483/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4705 - acc: 0.7587 - val_loss: 0.4864 - val_acc: 0.7969\n",
      "Epoch 484/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4704 - acc: 0.7604 - val_loss: 0.4864 - val_acc: 0.7969\n",
      "Epoch 485/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4703 - acc: 0.7604 - val_loss: 0.4864 - val_acc: 0.7969\n",
      "Epoch 486/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4702 - acc: 0.7604 - val_loss: 0.4863 - val_acc: 0.7969\n",
      "Epoch 487/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4701 - acc: 0.7604 - val_loss: 0.4863 - val_acc: 0.7969\n",
      "Epoch 488/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4700 - acc: 0.7587 - val_loss: 0.4863 - val_acc: 0.7969\n",
      "Epoch 489/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4699 - acc: 0.7587 - val_loss: 0.4862 - val_acc: 0.7969\n",
      "Epoch 490/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4698 - acc: 0.7587 - val_loss: 0.4862 - val_acc: 0.7917\n",
      "Epoch 491/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4697 - acc: 0.7604 - val_loss: 0.4861 - val_acc: 0.7917\n",
      "Epoch 492/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4696 - acc: 0.7604 - val_loss: 0.4861 - val_acc: 0.7917\n",
      "Epoch 493/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4695 - acc: 0.7622 - val_loss: 0.4860 - val_acc: 0.7917\n",
      "Epoch 494/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4695 - acc: 0.7604 - val_loss: 0.4860 - val_acc: 0.7917\n",
      "Epoch 495/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4693 - acc: 0.7622 - val_loss: 0.4859 - val_acc: 0.7917\n",
      "Epoch 496/1500\n",
      "576/576 [==============================] - 0s 82us/step - loss: 0.4693 - acc: 0.7622 - val_loss: 0.4859 - val_acc: 0.7917\n",
      "Epoch 497/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4691 - acc: 0.7622 - val_loss: 0.4858 - val_acc: 0.7917\n",
      "Epoch 498/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4691 - acc: 0.7604 - val_loss: 0.4858 - val_acc: 0.7917\n",
      "Epoch 499/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4690 - acc: 0.7622 - val_loss: 0.4857 - val_acc: 0.7917\n",
      "Epoch 500/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4689 - acc: 0.7622 - val_loss: 0.4857 - val_acc: 0.7917\n",
      "Epoch 501/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4688 - acc: 0.7622 - val_loss: 0.4856 - val_acc: 0.7917\n",
      "Epoch 502/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4687 - acc: 0.7604 - val_loss: 0.4856 - val_acc: 0.7917\n",
      "Epoch 503/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4686 - acc: 0.7604 - val_loss: 0.4856 - val_acc: 0.7917\n",
      "Epoch 504/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4685 - acc: 0.7604 - val_loss: 0.4855 - val_acc: 0.7917\n",
      "Epoch 505/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4684 - acc: 0.7604 - val_loss: 0.4855 - val_acc: 0.7917\n",
      "Epoch 506/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4683 - acc: 0.7604 - val_loss: 0.4854 - val_acc: 0.7917\n",
      "Epoch 507/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4682 - acc: 0.7604 - val_loss: 0.4854 - val_acc: 0.7917\n",
      "Epoch 508/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4682 - acc: 0.7604 - val_loss: 0.4853 - val_acc: 0.7917\n",
      "Epoch 509/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4681 - acc: 0.7604 - val_loss: 0.4853 - val_acc: 0.7917\n",
      "Epoch 510/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4680 - acc: 0.7604 - val_loss: 0.4852 - val_acc: 0.7865\n",
      "Epoch 511/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4679 - acc: 0.7604 - val_loss: 0.4852 - val_acc: 0.7865\n",
      "Epoch 512/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4678 - acc: 0.7604 - val_loss: 0.4851 - val_acc: 0.7865\n",
      "Epoch 513/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4677 - acc: 0.7604 - val_loss: 0.4851 - val_acc: 0.7865\n",
      "Epoch 514/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4676 - acc: 0.7604 - val_loss: 0.4851 - val_acc: 0.7865\n",
      "Epoch 515/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4675 - acc: 0.7604 - val_loss: 0.4850 - val_acc: 0.7865\n",
      "Epoch 516/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4674 - acc: 0.7604 - val_loss: 0.4850 - val_acc: 0.7865\n",
      "Epoch 517/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4673 - acc: 0.7604 - val_loss: 0.4849 - val_acc: 0.7865\n",
      "Epoch 518/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4672 - acc: 0.7604 - val_loss: 0.4849 - val_acc: 0.7865\n",
      "Epoch 519/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4671 - acc: 0.7622 - val_loss: 0.4848 - val_acc: 0.7865\n",
      "Epoch 520/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4670 - acc: 0.7604 - val_loss: 0.4848 - val_acc: 0.7865\n",
      "Epoch 521/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4670 - acc: 0.7622 - val_loss: 0.4847 - val_acc: 0.7865\n",
      "Epoch 522/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4669 - acc: 0.7639 - val_loss: 0.4847 - val_acc: 0.7865\n",
      "Epoch 523/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4667 - acc: 0.7639 - val_loss: 0.4847 - val_acc: 0.7865\n",
      "Epoch 524/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4667 - acc: 0.7639 - val_loss: 0.4846 - val_acc: 0.7865\n",
      "Epoch 525/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4666 - acc: 0.7639 - val_loss: 0.4846 - val_acc: 0.7865\n",
      "Epoch 526/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4665 - acc: 0.7656 - val_loss: 0.4845 - val_acc: 0.7865\n",
      "Epoch 527/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4664 - acc: 0.7656 - val_loss: 0.4845 - val_acc: 0.7865\n",
      "Epoch 528/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4663 - acc: 0.7656 - val_loss: 0.4844 - val_acc: 0.7865\n",
      "Epoch 529/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4662 - acc: 0.7656 - val_loss: 0.4844 - val_acc: 0.7865\n",
      "Epoch 530/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4661 - acc: 0.7656 - val_loss: 0.4844 - val_acc: 0.7865\n",
      "Epoch 531/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4660 - acc: 0.7656 - val_loss: 0.4843 - val_acc: 0.7865\n",
      "Epoch 532/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4660 - acc: 0.7674 - val_loss: 0.4843 - val_acc: 0.7865\n",
      "Epoch 533/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4658 - acc: 0.7674 - val_loss: 0.4843 - val_acc: 0.7865\n",
      "Epoch 534/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4657 - acc: 0.7674 - val_loss: 0.4842 - val_acc: 0.7865\n",
      "Epoch 535/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4657 - acc: 0.7674 - val_loss: 0.4842 - val_acc: 0.7865\n",
      "Epoch 536/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4656 - acc: 0.7691 - val_loss: 0.4842 - val_acc: 0.7865\n",
      "Epoch 537/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4655 - acc: 0.7691 - val_loss: 0.4841 - val_acc: 0.7865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 538/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4654 - acc: 0.7691 - val_loss: 0.4841 - val_acc: 0.7917\n",
      "Epoch 539/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4653 - acc: 0.7691 - val_loss: 0.4840 - val_acc: 0.7917\n",
      "Epoch 540/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4652 - acc: 0.7691 - val_loss: 0.4840 - val_acc: 0.7917\n",
      "Epoch 541/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4651 - acc: 0.7674 - val_loss: 0.4840 - val_acc: 0.7917\n",
      "Epoch 542/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4650 - acc: 0.7691 - val_loss: 0.4840 - val_acc: 0.7917\n",
      "Epoch 543/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4649 - acc: 0.7691 - val_loss: 0.4839 - val_acc: 0.7917\n",
      "Epoch 544/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4648 - acc: 0.7691 - val_loss: 0.4839 - val_acc: 0.7917\n",
      "Epoch 545/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4647 - acc: 0.7691 - val_loss: 0.4839 - val_acc: 0.7917\n",
      "Epoch 546/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4646 - acc: 0.7691 - val_loss: 0.4838 - val_acc: 0.7917\n",
      "Epoch 547/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4645 - acc: 0.7691 - val_loss: 0.4838 - val_acc: 0.7917\n",
      "Epoch 548/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4644 - acc: 0.7708 - val_loss: 0.4838 - val_acc: 0.7917\n",
      "Epoch 549/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4643 - acc: 0.7708 - val_loss: 0.4837 - val_acc: 0.7917\n",
      "Epoch 550/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4642 - acc: 0.7726 - val_loss: 0.4837 - val_acc: 0.7917\n",
      "Epoch 551/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4641 - acc: 0.7726 - val_loss: 0.4837 - val_acc: 0.7865\n",
      "Epoch 552/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4640 - acc: 0.7726 - val_loss: 0.4837 - val_acc: 0.7812\n",
      "Epoch 553/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4640 - acc: 0.7726 - val_loss: 0.4836 - val_acc: 0.7865\n",
      "Epoch 554/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4639 - acc: 0.7726 - val_loss: 0.4836 - val_acc: 0.7865\n",
      "Epoch 555/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4637 - acc: 0.7726 - val_loss: 0.4836 - val_acc: 0.7865\n",
      "Epoch 556/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4636 - acc: 0.7726 - val_loss: 0.4836 - val_acc: 0.7865\n",
      "Epoch 557/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4636 - acc: 0.7726 - val_loss: 0.4835 - val_acc: 0.7865\n",
      "Epoch 558/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4635 - acc: 0.7708 - val_loss: 0.4835 - val_acc: 0.7865\n",
      "Epoch 559/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4634 - acc: 0.7708 - val_loss: 0.4835 - val_acc: 0.7865\n",
      "Epoch 560/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4633 - acc: 0.7726 - val_loss: 0.4835 - val_acc: 0.7865\n",
      "Epoch 561/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4632 - acc: 0.7708 - val_loss: 0.4834 - val_acc: 0.7865\n",
      "Epoch 562/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4631 - acc: 0.7708 - val_loss: 0.4834 - val_acc: 0.7865\n",
      "Epoch 563/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4631 - acc: 0.7691 - val_loss: 0.4834 - val_acc: 0.7865\n",
      "Epoch 564/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4629 - acc: 0.7708 - val_loss: 0.4834 - val_acc: 0.7865\n",
      "Epoch 565/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4629 - acc: 0.7708 - val_loss: 0.4834 - val_acc: 0.7865\n",
      "Epoch 566/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4628 - acc: 0.7691 - val_loss: 0.4834 - val_acc: 0.7865\n",
      "Epoch 567/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4627 - acc: 0.7691 - val_loss: 0.4833 - val_acc: 0.7865\n",
      "Epoch 568/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4627 - acc: 0.7691 - val_loss: 0.4834 - val_acc: 0.7865\n",
      "Epoch 569/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4626 - acc: 0.7708 - val_loss: 0.4834 - val_acc: 0.7865\n",
      "Epoch 570/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4625 - acc: 0.7708 - val_loss: 0.4834 - val_acc: 0.7865\n",
      "Epoch 571/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4624 - acc: 0.7708 - val_loss: 0.4834 - val_acc: 0.7865\n",
      "Epoch 572/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4623 - acc: 0.7708 - val_loss: 0.4833 - val_acc: 0.7865\n",
      "Epoch 573/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4622 - acc: 0.7708 - val_loss: 0.4834 - val_acc: 0.7812\n",
      "Epoch 574/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4621 - acc: 0.7708 - val_loss: 0.4833 - val_acc: 0.7812\n",
      "Epoch 575/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4621 - acc: 0.7708 - val_loss: 0.4834 - val_acc: 0.7812\n",
      "Epoch 576/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4620 - acc: 0.7691 - val_loss: 0.4833 - val_acc: 0.7812\n",
      "Epoch 577/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4619 - acc: 0.7708 - val_loss: 0.4833 - val_acc: 0.7812\n",
      "Epoch 578/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4618 - acc: 0.7691 - val_loss: 0.4833 - val_acc: 0.7812\n",
      "Epoch 579/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4617 - acc: 0.7691 - val_loss: 0.4834 - val_acc: 0.7812\n",
      "Epoch 580/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4617 - acc: 0.7708 - val_loss: 0.4833 - val_acc: 0.7812\n",
      "Epoch 581/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4616 - acc: 0.7691 - val_loss: 0.4834 - val_acc: 0.7812\n",
      "Epoch 582/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4615 - acc: 0.7691 - val_loss: 0.4834 - val_acc: 0.7812\n",
      "Epoch 583/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4614 - acc: 0.7691 - val_loss: 0.4834 - val_acc: 0.7812\n",
      "Epoch 584/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4613 - acc: 0.7691 - val_loss: 0.4834 - val_acc: 0.7812\n",
      "Epoch 585/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4613 - acc: 0.7691 - val_loss: 0.4834 - val_acc: 0.7812\n",
      "Epoch 586/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4612 - acc: 0.7691 - val_loss: 0.4834 - val_acc: 0.7760\n",
      "Epoch 587/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4612 - acc: 0.7691 - val_loss: 0.4834 - val_acc: 0.7760\n",
      "Epoch 588/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4610 - acc: 0.7691 - val_loss: 0.4834 - val_acc: 0.7760\n",
      "Epoch 589/1500\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.5208 - acc: 0.687 - 0s 50us/step - loss: 0.4610 - acc: 0.7691 - val_loss: 0.4834 - val_acc: 0.7760\n",
      "Epoch 590/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4609 - acc: 0.7691 - val_loss: 0.4834 - val_acc: 0.7760\n",
      "Epoch 591/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4608 - acc: 0.7691 - val_loss: 0.4834 - val_acc: 0.7760\n",
      "Epoch 592/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4607 - acc: 0.7691 - val_loss: 0.4834 - val_acc: 0.7760\n",
      "Epoch 593/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4607 - acc: 0.7691 - val_loss: 0.4834 - val_acc: 0.7760\n",
      "Epoch 594/1500\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.4606 - acc: 0.7691 - val_loss: 0.4834 - val_acc: 0.7760\n",
      "Epoch 595/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4605 - acc: 0.7691 - val_loss: 0.4834 - val_acc: 0.7760\n",
      "Epoch 596/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4605 - acc: 0.7674 - val_loss: 0.4835 - val_acc: 0.7760\n",
      "Epoch 597/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 50us/step - loss: 0.4604 - acc: 0.7674 - val_loss: 0.4835 - val_acc: 0.7760\n",
      "Epoch 598/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4603 - acc: 0.7674 - val_loss: 0.4835 - val_acc: 0.7760\n",
      "Epoch 599/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4603 - acc: 0.7674 - val_loss: 0.4835 - val_acc: 0.7760\n",
      "Epoch 600/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4602 - acc: 0.7674 - val_loss: 0.4835 - val_acc: 0.7760\n",
      "Epoch 601/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4601 - acc: 0.7691 - val_loss: 0.4835 - val_acc: 0.7760\n",
      "Epoch 602/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4600 - acc: 0.7691 - val_loss: 0.4835 - val_acc: 0.7760\n",
      "Epoch 603/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4599 - acc: 0.7691 - val_loss: 0.4835 - val_acc: 0.7760\n",
      "Epoch 604/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4599 - acc: 0.7691 - val_loss: 0.4835 - val_acc: 0.7760\n",
      "Epoch 605/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4598 - acc: 0.7691 - val_loss: 0.4835 - val_acc: 0.7760\n",
      "Epoch 606/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4597 - acc: 0.7691 - val_loss: 0.4836 - val_acc: 0.7760\n",
      "Epoch 607/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4597 - acc: 0.7691 - val_loss: 0.4836 - val_acc: 0.7760\n",
      "Epoch 608/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4596 - acc: 0.7691 - val_loss: 0.4836 - val_acc: 0.7760\n",
      "Epoch 609/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4595 - acc: 0.7691 - val_loss: 0.4836 - val_acc: 0.7812\n",
      "Epoch 610/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4595 - acc: 0.7691 - val_loss: 0.4836 - val_acc: 0.7812\n",
      "Epoch 611/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4594 - acc: 0.7691 - val_loss: 0.4836 - val_acc: 0.7812\n",
      "Epoch 612/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4593 - acc: 0.7691 - val_loss: 0.4837 - val_acc: 0.7812\n",
      "Epoch 613/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4592 - acc: 0.7691 - val_loss: 0.4837 - val_acc: 0.7812\n",
      "Epoch 614/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4592 - acc: 0.7691 - val_loss: 0.4837 - val_acc: 0.7812\n",
      "Epoch 615/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4591 - acc: 0.7691 - val_loss: 0.4837 - val_acc: 0.7812\n",
      "Epoch 616/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4590 - acc: 0.7691 - val_loss: 0.4837 - val_acc: 0.7812\n",
      "Epoch 617/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4590 - acc: 0.7691 - val_loss: 0.4837 - val_acc: 0.7812\n",
      "Epoch 618/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4589 - acc: 0.7691 - val_loss: 0.4838 - val_acc: 0.7812\n",
      "Epoch 619/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4588 - acc: 0.7691 - val_loss: 0.4838 - val_acc: 0.7812\n",
      "Epoch 620/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4587 - acc: 0.7691 - val_loss: 0.4838 - val_acc: 0.7812\n",
      "Epoch 621/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4587 - acc: 0.7691 - val_loss: 0.4838 - val_acc: 0.7812\n",
      "Epoch 622/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4586 - acc: 0.7708 - val_loss: 0.4838 - val_acc: 0.7812\n",
      "Epoch 623/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4586 - acc: 0.7708 - val_loss: 0.4839 - val_acc: 0.7812\n",
      "Epoch 624/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4585 - acc: 0.7726 - val_loss: 0.4839 - val_acc: 0.7812\n",
      "Epoch 625/1500\n",
      "576/576 [==============================] - 0s 83us/step - loss: 0.4584 - acc: 0.7726 - val_loss: 0.4839 - val_acc: 0.7812\n",
      "Epoch 626/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4583 - acc: 0.7726 - val_loss: 0.4839 - val_acc: 0.7812\n",
      "Epoch 627/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4583 - acc: 0.7726 - val_loss: 0.4839 - val_acc: 0.7812\n",
      "Epoch 628/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4582 - acc: 0.7726 - val_loss: 0.4840 - val_acc: 0.7812\n",
      "Epoch 629/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4581 - acc: 0.7726 - val_loss: 0.4840 - val_acc: 0.7812\n",
      "Epoch 630/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4581 - acc: 0.7726 - val_loss: 0.4840 - val_acc: 0.7812\n",
      "Epoch 631/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4580 - acc: 0.7726 - val_loss: 0.4841 - val_acc: 0.7812\n",
      "Epoch 632/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4579 - acc: 0.7726 - val_loss: 0.4841 - val_acc: 0.7812\n",
      "Epoch 633/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4578 - acc: 0.7726 - val_loss: 0.4841 - val_acc: 0.7812\n",
      "Epoch 634/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4578 - acc: 0.7726 - val_loss: 0.4841 - val_acc: 0.7812\n",
      "Epoch 635/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4577 - acc: 0.7726 - val_loss: 0.4842 - val_acc: 0.7812\n",
      "Epoch 636/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4577 - acc: 0.7726 - val_loss: 0.4842 - val_acc: 0.7812\n",
      "Epoch 637/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4576 - acc: 0.7726 - val_loss: 0.4842 - val_acc: 0.7812\n",
      "Epoch 638/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4575 - acc: 0.7726 - val_loss: 0.4842 - val_acc: 0.7812\n",
      "Epoch 639/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4575 - acc: 0.7726 - val_loss: 0.4843 - val_acc: 0.7760\n",
      "Epoch 640/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4574 - acc: 0.7726 - val_loss: 0.4843 - val_acc: 0.7760\n",
      "Epoch 641/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4573 - acc: 0.7726 - val_loss: 0.4843 - val_acc: 0.7760\n",
      "Epoch 642/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4573 - acc: 0.7726 - val_loss: 0.4844 - val_acc: 0.7760\n",
      "Epoch 643/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4572 - acc: 0.7726 - val_loss: 0.4844 - val_acc: 0.7760\n",
      "Epoch 644/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4571 - acc: 0.7726 - val_loss: 0.4844 - val_acc: 0.7760\n",
      "Epoch 645/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4571 - acc: 0.7726 - val_loss: 0.4845 - val_acc: 0.7760\n",
      "Epoch 646/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4570 - acc: 0.7726 - val_loss: 0.4845 - val_acc: 0.7760\n",
      "Epoch 647/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4569 - acc: 0.7726 - val_loss: 0.4845 - val_acc: 0.7760\n",
      "Epoch 648/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4569 - acc: 0.7726 - val_loss: 0.4846 - val_acc: 0.7708\n",
      "Epoch 649/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4568 - acc: 0.7726 - val_loss: 0.4846 - val_acc: 0.7708\n",
      "Epoch 650/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4568 - acc: 0.7726 - val_loss: 0.4846 - val_acc: 0.7708\n",
      "Epoch 651/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4567 - acc: 0.7726 - val_loss: 0.4847 - val_acc: 0.7708\n",
      "Epoch 652/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4566 - acc: 0.7726 - val_loss: 0.4847 - val_acc: 0.7708\n",
      "Epoch 653/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4566 - acc: 0.7726 - val_loss: 0.4847 - val_acc: 0.7708\n",
      "Epoch 654/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4565 - acc: 0.7726 - val_loss: 0.4848 - val_acc: 0.7708\n",
      "Epoch 655/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4565 - acc: 0.7726 - val_loss: 0.4848 - val_acc: 0.7708\n",
      "Epoch 656/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4564 - acc: 0.7726 - val_loss: 0.4849 - val_acc: 0.7708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 657/1500\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4563 - acc: 0.7726 - val_loss: 0.4849 - val_acc: 0.7708\n",
      "Epoch 658/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4563 - acc: 0.7726 - val_loss: 0.4849 - val_acc: 0.7708\n",
      "Epoch 659/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4562 - acc: 0.7726 - val_loss: 0.4850 - val_acc: 0.7708\n",
      "Epoch 660/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4561 - acc: 0.7726 - val_loss: 0.4850 - val_acc: 0.7708\n",
      "Epoch 661/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4561 - acc: 0.7726 - val_loss: 0.4850 - val_acc: 0.7708\n",
      "Epoch 662/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4560 - acc: 0.7726 - val_loss: 0.4851 - val_acc: 0.7656\n",
      "Epoch 663/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4560 - acc: 0.7726 - val_loss: 0.4851 - val_acc: 0.7656\n",
      "Epoch 664/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4559 - acc: 0.7726 - val_loss: 0.4851 - val_acc: 0.7656\n",
      "Epoch 665/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4559 - acc: 0.7726 - val_loss: 0.4852 - val_acc: 0.7656\n",
      "Epoch 666/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4558 - acc: 0.7726 - val_loss: 0.4852 - val_acc: 0.7656\n",
      "Epoch 667/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4557 - acc: 0.7726 - val_loss: 0.4852 - val_acc: 0.7656\n",
      "Epoch 668/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4557 - acc: 0.7726 - val_loss: 0.4853 - val_acc: 0.7656\n",
      "Epoch 669/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4556 - acc: 0.7743 - val_loss: 0.4853 - val_acc: 0.7656\n",
      "Epoch 670/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4556 - acc: 0.7726 - val_loss: 0.4853 - val_acc: 0.7656\n",
      "Epoch 671/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4555 - acc: 0.7743 - val_loss: 0.4854 - val_acc: 0.7656\n",
      "Epoch 672/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4555 - acc: 0.7743 - val_loss: 0.4854 - val_acc: 0.7656\n",
      "Epoch 673/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4554 - acc: 0.7743 - val_loss: 0.4854 - val_acc: 0.7656\n",
      "Epoch 674/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4554 - acc: 0.7743 - val_loss: 0.4854 - val_acc: 0.7656\n",
      "Epoch 675/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4553 - acc: 0.7743 - val_loss: 0.4855 - val_acc: 0.7656\n",
      "Epoch 676/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4553 - acc: 0.7743 - val_loss: 0.4855 - val_acc: 0.7656\n",
      "Epoch 677/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4552 - acc: 0.7743 - val_loss: 0.4855 - val_acc: 0.7656\n",
      "Epoch 678/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4551 - acc: 0.7743 - val_loss: 0.4856 - val_acc: 0.7656\n",
      "Epoch 679/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4550 - acc: 0.7743 - val_loss: 0.4856 - val_acc: 0.7656\n",
      "Epoch 680/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4550 - acc: 0.7743 - val_loss: 0.4857 - val_acc: 0.7656\n",
      "Epoch 681/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4550 - acc: 0.7743 - val_loss: 0.4857 - val_acc: 0.7656\n",
      "Epoch 682/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4549 - acc: 0.7743 - val_loss: 0.4857 - val_acc: 0.7656\n",
      "Epoch 683/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4548 - acc: 0.7743 - val_loss: 0.4858 - val_acc: 0.7656\n",
      "Epoch 684/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4548 - acc: 0.7743 - val_loss: 0.4858 - val_acc: 0.7656\n",
      "Epoch 685/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4547 - acc: 0.7743 - val_loss: 0.4858 - val_acc: 0.7656\n",
      "Epoch 686/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4547 - acc: 0.7743 - val_loss: 0.4858 - val_acc: 0.7656\n",
      "Epoch 687/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4546 - acc: 0.7743 - val_loss: 0.4859 - val_acc: 0.7656\n",
      "Epoch 688/1500\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.4545 - acc: 0.7743 - val_loss: 0.4859 - val_acc: 0.7656\n",
      "Epoch 689/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4545 - acc: 0.7743 - val_loss: 0.4859 - val_acc: 0.7656\n",
      "Epoch 690/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4544 - acc: 0.7743 - val_loss: 0.4860 - val_acc: 0.7656\n",
      "Epoch 691/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4544 - acc: 0.7743 - val_loss: 0.4860 - val_acc: 0.7708\n",
      "Epoch 692/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4543 - acc: 0.7743 - val_loss: 0.4860 - val_acc: 0.7708\n",
      "Epoch 693/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4542 - acc: 0.7743 - val_loss: 0.4860 - val_acc: 0.7708\n",
      "Epoch 694/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4542 - acc: 0.7743 - val_loss: 0.4861 - val_acc: 0.7708\n",
      "Epoch 695/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4541 - acc: 0.7743 - val_loss: 0.4861 - val_acc: 0.7708\n",
      "Epoch 696/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4541 - acc: 0.7743 - val_loss: 0.4861 - val_acc: 0.7708\n",
      "Epoch 697/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4540 - acc: 0.7743 - val_loss: 0.4862 - val_acc: 0.7708\n",
      "Epoch 698/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4540 - acc: 0.7743 - val_loss: 0.4862 - val_acc: 0.7708\n",
      "Epoch 699/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4539 - acc: 0.7743 - val_loss: 0.4862 - val_acc: 0.7708\n",
      "Epoch 700/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4539 - acc: 0.7726 - val_loss: 0.4863 - val_acc: 0.7708\n",
      "Epoch 701/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4538 - acc: 0.7743 - val_loss: 0.4863 - val_acc: 0.7708\n",
      "Epoch 702/1500\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4538 - acc: 0.7726 - val_loss: 0.4863 - val_acc: 0.7708\n",
      "Epoch 703/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4537 - acc: 0.7743 - val_loss: 0.4864 - val_acc: 0.7708\n",
      "Epoch 704/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4536 - acc: 0.7743 - val_loss: 0.4864 - val_acc: 0.7708\n",
      "Epoch 705/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4536 - acc: 0.7726 - val_loss: 0.4864 - val_acc: 0.7708\n",
      "Epoch 706/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4535 - acc: 0.7726 - val_loss: 0.4865 - val_acc: 0.7708\n",
      "Epoch 707/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4535 - acc: 0.7726 - val_loss: 0.4865 - val_acc: 0.7708\n",
      "Epoch 708/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4534 - acc: 0.7726 - val_loss: 0.4866 - val_acc: 0.7708\n",
      "Epoch 709/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4534 - acc: 0.7726 - val_loss: 0.4866 - val_acc: 0.7708\n",
      "Epoch 710/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4533 - acc: 0.7726 - val_loss: 0.4866 - val_acc: 0.7708\n",
      "Epoch 711/1500\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4532 - acc: 0.7726 - val_loss: 0.4867 - val_acc: 0.7708\n",
      "Epoch 712/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4532 - acc: 0.7726 - val_loss: 0.4867 - val_acc: 0.7708\n",
      "Epoch 713/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4531 - acc: 0.7726 - val_loss: 0.4867 - val_acc: 0.7708\n",
      "Epoch 714/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4531 - acc: 0.7726 - val_loss: 0.4868 - val_acc: 0.7708\n",
      "Epoch 715/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4531 - acc: 0.7726 - val_loss: 0.4868 - val_acc: 0.7708\n",
      "Epoch 716/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4530 - acc: 0.7726 - val_loss: 0.4868 - val_acc: 0.7708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 717/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4529 - acc: 0.7726 - val_loss: 0.4869 - val_acc: 0.7708\n",
      "Epoch 718/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4529 - acc: 0.7726 - val_loss: 0.4869 - val_acc: 0.7708\n",
      "Epoch 719/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4528 - acc: 0.7726 - val_loss: 0.4869 - val_acc: 0.7708\n",
      "Epoch 720/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4528 - acc: 0.7726 - val_loss: 0.4870 - val_acc: 0.7708\n",
      "Epoch 721/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4527 - acc: 0.7726 - val_loss: 0.4870 - val_acc: 0.7708\n",
      "Epoch 722/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4527 - acc: 0.7726 - val_loss: 0.4870 - val_acc: 0.7708\n",
      "Epoch 723/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4527 - acc: 0.7726 - val_loss: 0.4871 - val_acc: 0.7708\n",
      "Epoch 724/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4526 - acc: 0.7726 - val_loss: 0.4871 - val_acc: 0.7708\n",
      "Epoch 725/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4526 - acc: 0.7726 - val_loss: 0.4871 - val_acc: 0.7708\n",
      "Epoch 726/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4525 - acc: 0.7726 - val_loss: 0.4872 - val_acc: 0.7708\n",
      "Epoch 727/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4525 - acc: 0.7726 - val_loss: 0.4872 - val_acc: 0.7708\n",
      "Epoch 728/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4524 - acc: 0.7726 - val_loss: 0.4873 - val_acc: 0.7708\n",
      "Epoch 729/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4524 - acc: 0.7726 - val_loss: 0.4873 - val_acc: 0.7708\n",
      "Epoch 730/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4523 - acc: 0.7726 - val_loss: 0.4873 - val_acc: 0.7708\n",
      "Epoch 731/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4523 - acc: 0.7726 - val_loss: 0.4874 - val_acc: 0.7708\n",
      "Epoch 732/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4522 - acc: 0.7743 - val_loss: 0.4874 - val_acc: 0.7708\n",
      "Epoch 733/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4522 - acc: 0.7743 - val_loss: 0.4874 - val_acc: 0.7708\n",
      "Epoch 734/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4522 - acc: 0.7708 - val_loss: 0.4875 - val_acc: 0.7708\n",
      "Epoch 735/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4521 - acc: 0.7708 - val_loss: 0.4875 - val_acc: 0.7708\n",
      "Epoch 736/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4521 - acc: 0.7743 - val_loss: 0.4875 - val_acc: 0.7708\n",
      "Epoch 737/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4520 - acc: 0.7708 - val_loss: 0.4876 - val_acc: 0.7708\n",
      "Epoch 738/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4520 - acc: 0.7726 - val_loss: 0.4876 - val_acc: 0.7708\n",
      "Epoch 739/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4519 - acc: 0.7726 - val_loss: 0.4876 - val_acc: 0.7708\n",
      "Epoch 740/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4519 - acc: 0.7726 - val_loss: 0.4877 - val_acc: 0.7708\n",
      "Epoch 741/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4518 - acc: 0.7726 - val_loss: 0.4877 - val_acc: 0.7708\n",
      "Epoch 742/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4518 - acc: 0.7726 - val_loss: 0.4877 - val_acc: 0.7708\n",
      "Epoch 743/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4517 - acc: 0.7743 - val_loss: 0.4878 - val_acc: 0.7708\n",
      "Epoch 744/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4517 - acc: 0.7726 - val_loss: 0.4878 - val_acc: 0.7708\n",
      "Epoch 745/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4516 - acc: 0.7726 - val_loss: 0.4878 - val_acc: 0.7708\n",
      "Epoch 746/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4516 - acc: 0.7726 - val_loss: 0.4879 - val_acc: 0.7708\n",
      "Epoch 747/1500\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.4516 - acc: 0.7743 - val_loss: 0.4879 - val_acc: 0.7708\n",
      "Epoch 748/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4515 - acc: 0.7726 - val_loss: 0.4879 - val_acc: 0.7708\n",
      "Epoch 749/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4515 - acc: 0.7726 - val_loss: 0.4880 - val_acc: 0.7708\n",
      "Epoch 750/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4514 - acc: 0.7726 - val_loss: 0.4880 - val_acc: 0.7708\n",
      "Epoch 751/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4514 - acc: 0.7726 - val_loss: 0.4880 - val_acc: 0.7708\n",
      "Epoch 752/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4514 - acc: 0.7726 - val_loss: 0.4881 - val_acc: 0.7708\n",
      "Epoch 753/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4513 - acc: 0.7726 - val_loss: 0.4881 - val_acc: 0.7708\n",
      "Epoch 754/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4513 - acc: 0.7743 - val_loss: 0.4881 - val_acc: 0.7708\n",
      "Epoch 755/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4513 - acc: 0.7743 - val_loss: 0.4882 - val_acc: 0.7708\n",
      "Epoch 756/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4512 - acc: 0.7726 - val_loss: 0.4882 - val_acc: 0.7708\n",
      "Epoch 757/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4512 - acc: 0.7743 - val_loss: 0.4883 - val_acc: 0.7708\n",
      "Epoch 758/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4511 - acc: 0.7743 - val_loss: 0.4883 - val_acc: 0.7708\n",
      "Epoch 759/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4511 - acc: 0.7743 - val_loss: 0.4883 - val_acc: 0.7708\n",
      "Epoch 760/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4511 - acc: 0.7743 - val_loss: 0.4884 - val_acc: 0.7708\n",
      "Epoch 761/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4510 - acc: 0.7743 - val_loss: 0.4884 - val_acc: 0.7708\n",
      "Epoch 762/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4510 - acc: 0.7743 - val_loss: 0.4884 - val_acc: 0.7708\n",
      "Epoch 763/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4509 - acc: 0.7743 - val_loss: 0.4885 - val_acc: 0.7708\n",
      "Epoch 764/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4509 - acc: 0.7743 - val_loss: 0.4885 - val_acc: 0.7708\n",
      "Epoch 765/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4509 - acc: 0.7760 - val_loss: 0.4886 - val_acc: 0.7708\n",
      "Epoch 766/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4508 - acc: 0.7760 - val_loss: 0.4886 - val_acc: 0.7708\n",
      "Epoch 767/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4508 - acc: 0.7760 - val_loss: 0.4886 - val_acc: 0.7708\n",
      "Epoch 768/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4508 - acc: 0.7760 - val_loss: 0.4887 - val_acc: 0.7708\n",
      "Epoch 769/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4507 - acc: 0.7760 - val_loss: 0.4887 - val_acc: 0.7708\n",
      "Epoch 770/1500\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4507 - acc: 0.7760 - val_loss: 0.4887 - val_acc: 0.7708\n",
      "Epoch 771/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4507 - acc: 0.7760 - val_loss: 0.4888 - val_acc: 0.7708\n",
      "Epoch 772/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4506 - acc: 0.7760 - val_loss: 0.4888 - val_acc: 0.7708\n",
      "Epoch 773/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4506 - acc: 0.7760 - val_loss: 0.4889 - val_acc: 0.7708\n",
      "Epoch 774/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4505 - acc: 0.7760 - val_loss: 0.4889 - val_acc: 0.7708\n",
      "Epoch 775/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4505 - acc: 0.7760 - val_loss: 0.4889 - val_acc: 0.7708\n",
      "Epoch 776/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4505 - acc: 0.7760 - val_loss: 0.4890 - val_acc: 0.7708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 777/1500\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.3792 - acc: 0.843 - 0s 55us/step - loss: 0.4504 - acc: 0.7760 - val_loss: 0.4890 - val_acc: 0.7708\n",
      "Epoch 778/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4504 - acc: 0.7760 - val_loss: 0.4891 - val_acc: 0.7708\n",
      "Epoch 779/1500\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.4503 - acc: 0.7760 - val_loss: 0.4891 - val_acc: 0.7708\n",
      "Epoch 780/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4503 - acc: 0.7760 - val_loss: 0.4891 - val_acc: 0.7708\n",
      "Epoch 781/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4503 - acc: 0.7760 - val_loss: 0.4892 - val_acc: 0.7708\n",
      "Epoch 782/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4502 - acc: 0.7760 - val_loss: 0.4892 - val_acc: 0.7708\n",
      "Epoch 783/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4502 - acc: 0.7760 - val_loss: 0.4893 - val_acc: 0.7708\n",
      "Epoch 784/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4501 - acc: 0.7760 - val_loss: 0.4893 - val_acc: 0.7708\n",
      "Epoch 785/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4501 - acc: 0.7760 - val_loss: 0.4893 - val_acc: 0.7708\n",
      "Epoch 786/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4501 - acc: 0.7760 - val_loss: 0.4894 - val_acc: 0.7708\n",
      "Epoch 787/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4500 - acc: 0.7760 - val_loss: 0.4894 - val_acc: 0.7708\n",
      "Epoch 788/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4500 - acc: 0.7760 - val_loss: 0.4895 - val_acc: 0.7708\n",
      "Epoch 789/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4500 - acc: 0.7760 - val_loss: 0.4895 - val_acc: 0.7708\n",
      "Epoch 790/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4499 - acc: 0.7760 - val_loss: 0.4895 - val_acc: 0.7708\n",
      "Epoch 791/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4499 - acc: 0.7760 - val_loss: 0.4896 - val_acc: 0.7708\n",
      "Epoch 792/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4498 - acc: 0.7760 - val_loss: 0.4896 - val_acc: 0.7708\n",
      "Epoch 793/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4498 - acc: 0.7760 - val_loss: 0.4897 - val_acc: 0.7708\n",
      "Epoch 794/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4498 - acc: 0.7760 - val_loss: 0.4897 - val_acc: 0.7708\n",
      "Epoch 795/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4497 - acc: 0.7760 - val_loss: 0.4897 - val_acc: 0.7708\n",
      "Epoch 796/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4497 - acc: 0.7760 - val_loss: 0.4898 - val_acc: 0.7708\n",
      "Epoch 797/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4496 - acc: 0.7760 - val_loss: 0.4898 - val_acc: 0.7708\n",
      "Epoch 798/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4497 - acc: 0.7760 - val_loss: 0.4898 - val_acc: 0.7708\n",
      "Epoch 799/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4496 - acc: 0.7760 - val_loss: 0.4898 - val_acc: 0.7708\n",
      "Epoch 800/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4495 - acc: 0.7760 - val_loss: 0.4899 - val_acc: 0.7708\n",
      "Epoch 801/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4495 - acc: 0.7760 - val_loss: 0.4899 - val_acc: 0.7708\n",
      "Epoch 802/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4495 - acc: 0.7743 - val_loss: 0.4899 - val_acc: 0.7708\n",
      "Epoch 803/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4494 - acc: 0.7760 - val_loss: 0.4899 - val_acc: 0.7708\n",
      "Epoch 804/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4494 - acc: 0.7760 - val_loss: 0.4899 - val_acc: 0.7708\n",
      "Epoch 805/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4494 - acc: 0.7760 - val_loss: 0.4900 - val_acc: 0.7708\n",
      "Epoch 806/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4494 - acc: 0.7743 - val_loss: 0.4900 - val_acc: 0.7708\n",
      "Epoch 807/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4493 - acc: 0.7743 - val_loss: 0.4900 - val_acc: 0.7708\n",
      "Epoch 808/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4493 - acc: 0.7760 - val_loss: 0.4900 - val_acc: 0.7708\n",
      "Epoch 809/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4492 - acc: 0.7743 - val_loss: 0.4901 - val_acc: 0.7708\n",
      "Epoch 810/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4492 - acc: 0.7743 - val_loss: 0.4901 - val_acc: 0.7708\n",
      "Epoch 811/1500\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4492 - acc: 0.7743 - val_loss: 0.4901 - val_acc: 0.7708\n",
      "Epoch 812/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4491 - acc: 0.7743 - val_loss: 0.4901 - val_acc: 0.7708\n",
      "Epoch 813/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4491 - acc: 0.7743 - val_loss: 0.4901 - val_acc: 0.7708\n",
      "Epoch 814/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4491 - acc: 0.7760 - val_loss: 0.4901 - val_acc: 0.7708\n",
      "Epoch 815/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4490 - acc: 0.7743 - val_loss: 0.4902 - val_acc: 0.7708\n",
      "Epoch 816/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4490 - acc: 0.7760 - val_loss: 0.4902 - val_acc: 0.7708\n",
      "Epoch 817/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4490 - acc: 0.7743 - val_loss: 0.4902 - val_acc: 0.7708\n",
      "Epoch 818/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4489 - acc: 0.7743 - val_loss: 0.4902 - val_acc: 0.7708\n",
      "Epoch 819/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4489 - acc: 0.7743 - val_loss: 0.4902 - val_acc: 0.7708\n",
      "Epoch 820/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4489 - acc: 0.7760 - val_loss: 0.4903 - val_acc: 0.7708\n",
      "Epoch 821/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4488 - acc: 0.7743 - val_loss: 0.4903 - val_acc: 0.7708\n",
      "Epoch 822/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4488 - acc: 0.7760 - val_loss: 0.4903 - val_acc: 0.7708\n",
      "Epoch 823/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4488 - acc: 0.7743 - val_loss: 0.4903 - val_acc: 0.7708\n",
      "Epoch 824/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4488 - acc: 0.7743 - val_loss: 0.4903 - val_acc: 0.7708\n",
      "Epoch 825/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4487 - acc: 0.7760 - val_loss: 0.4904 - val_acc: 0.7708\n",
      "Epoch 826/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4486 - acc: 0.7760 - val_loss: 0.4904 - val_acc: 0.7708\n",
      "Epoch 827/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4486 - acc: 0.7760 - val_loss: 0.4904 - val_acc: 0.7708\n",
      "Epoch 828/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4486 - acc: 0.7743 - val_loss: 0.4904 - val_acc: 0.7708\n",
      "Epoch 829/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4486 - acc: 0.7760 - val_loss: 0.4904 - val_acc: 0.7708\n",
      "Epoch 830/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4485 - acc: 0.7760 - val_loss: 0.4905 - val_acc: 0.7708\n",
      "Epoch 831/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4485 - acc: 0.7760 - val_loss: 0.4905 - val_acc: 0.7708\n",
      "Epoch 832/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4485 - acc: 0.7760 - val_loss: 0.4905 - val_acc: 0.7708\n",
      "Epoch 833/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4484 - acc: 0.7760 - val_loss: 0.4905 - val_acc: 0.7708\n",
      "Epoch 834/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4484 - acc: 0.7760 - val_loss: 0.4905 - val_acc: 0.7708\n",
      "Epoch 835/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4484 - acc: 0.7760 - val_loss: 0.4905 - val_acc: 0.7708\n",
      "Epoch 836/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 59us/step - loss: 0.4484 - acc: 0.7760 - val_loss: 0.4905 - val_acc: 0.7708\n",
      "Epoch 837/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4483 - acc: 0.7760 - val_loss: 0.4906 - val_acc: 0.7708\n",
      "Epoch 838/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4483 - acc: 0.7760 - val_loss: 0.4906 - val_acc: 0.7708\n",
      "Epoch 839/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4482 - acc: 0.7760 - val_loss: 0.4906 - val_acc: 0.7708\n",
      "Epoch 840/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4482 - acc: 0.7760 - val_loss: 0.4906 - val_acc: 0.7708\n",
      "Epoch 841/1500\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4482 - acc: 0.7760 - val_loss: 0.4907 - val_acc: 0.7708\n",
      "Epoch 842/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4482 - acc: 0.7760 - val_loss: 0.4907 - val_acc: 0.7708\n",
      "Epoch 843/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4481 - acc: 0.7760 - val_loss: 0.4907 - val_acc: 0.7708\n",
      "Epoch 844/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4481 - acc: 0.7760 - val_loss: 0.4907 - val_acc: 0.7708\n",
      "Epoch 845/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4481 - acc: 0.7760 - val_loss: 0.4907 - val_acc: 0.7708\n",
      "Epoch 846/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4480 - acc: 0.7760 - val_loss: 0.4908 - val_acc: 0.7708\n",
      "Epoch 847/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4480 - acc: 0.7743 - val_loss: 0.4908 - val_acc: 0.7708\n",
      "Epoch 848/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4479 - acc: 0.7760 - val_loss: 0.4908 - val_acc: 0.7708\n",
      "Epoch 849/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4479 - acc: 0.7743 - val_loss: 0.4908 - val_acc: 0.7708\n",
      "Epoch 850/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4479 - acc: 0.7743 - val_loss: 0.4908 - val_acc: 0.7708\n",
      "Epoch 851/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4478 - acc: 0.7743 - val_loss: 0.4908 - val_acc: 0.7708\n",
      "Epoch 852/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4478 - acc: 0.7743 - val_loss: 0.4909 - val_acc: 0.7708\n",
      "Epoch 853/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4478 - acc: 0.7743 - val_loss: 0.4909 - val_acc: 0.7708\n",
      "Epoch 854/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4478 - acc: 0.7743 - val_loss: 0.4909 - val_acc: 0.7708\n",
      "Epoch 855/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4477 - acc: 0.7743 - val_loss: 0.4909 - val_acc: 0.7708\n",
      "Epoch 856/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4477 - acc: 0.7743 - val_loss: 0.4909 - val_acc: 0.7708\n",
      "Epoch 857/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4476 - acc: 0.7743 - val_loss: 0.4910 - val_acc: 0.7708\n",
      "Epoch 858/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4476 - acc: 0.7743 - val_loss: 0.4910 - val_acc: 0.7708\n",
      "Epoch 859/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4476 - acc: 0.7743 - val_loss: 0.4910 - val_acc: 0.7708\n",
      "Epoch 860/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4475 - acc: 0.7743 - val_loss: 0.4910 - val_acc: 0.7708\n",
      "Epoch 861/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4475 - acc: 0.7726 - val_loss: 0.4910 - val_acc: 0.7708\n",
      "Epoch 862/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4475 - acc: 0.7743 - val_loss: 0.4911 - val_acc: 0.7708\n",
      "Epoch 863/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4475 - acc: 0.7726 - val_loss: 0.4911 - val_acc: 0.7708\n",
      "Epoch 864/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4474 - acc: 0.7726 - val_loss: 0.4911 - val_acc: 0.7708\n",
      "Epoch 865/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4473 - acc: 0.7726 - val_loss: 0.4911 - val_acc: 0.7708\n",
      "Epoch 866/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4474 - acc: 0.7726 - val_loss: 0.4911 - val_acc: 0.7708\n",
      "Epoch 867/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4473 - acc: 0.7726 - val_loss: 0.4912 - val_acc: 0.7708\n",
      "Epoch 868/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4473 - acc: 0.7726 - val_loss: 0.4912 - val_acc: 0.7708\n",
      "Epoch 869/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4472 - acc: 0.7726 - val_loss: 0.4912 - val_acc: 0.7708\n",
      "Epoch 870/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4472 - acc: 0.7726 - val_loss: 0.4912 - val_acc: 0.7708\n",
      "Epoch 871/1500\n",
      "576/576 [==============================] - 0s 77us/step - loss: 0.4472 - acc: 0.7726 - val_loss: 0.4913 - val_acc: 0.7708\n",
      "Epoch 872/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4471 - acc: 0.7726 - val_loss: 0.4913 - val_acc: 0.7708\n",
      "Epoch 873/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4472 - acc: 0.7726 - val_loss: 0.4913 - val_acc: 0.7708\n",
      "Epoch 874/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4471 - acc: 0.7726 - val_loss: 0.4913 - val_acc: 0.7708\n",
      "Epoch 875/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4471 - acc: 0.7726 - val_loss: 0.4913 - val_acc: 0.7708\n",
      "Epoch 876/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4470 - acc: 0.7726 - val_loss: 0.4913 - val_acc: 0.7708\n",
      "Epoch 877/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4470 - acc: 0.7726 - val_loss: 0.4914 - val_acc: 0.7708\n",
      "Epoch 878/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4469 - acc: 0.7708 - val_loss: 0.4914 - val_acc: 0.7708\n",
      "Epoch 879/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4469 - acc: 0.7726 - val_loss: 0.4914 - val_acc: 0.7708\n",
      "Epoch 880/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4469 - acc: 0.7726 - val_loss: 0.4914 - val_acc: 0.7708\n",
      "Epoch 881/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4469 - acc: 0.7726 - val_loss: 0.4914 - val_acc: 0.7708\n",
      "Epoch 882/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4468 - acc: 0.7708 - val_loss: 0.4915 - val_acc: 0.7708\n",
      "Epoch 883/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4468 - acc: 0.7726 - val_loss: 0.4915 - val_acc: 0.7708\n",
      "Epoch 884/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4468 - acc: 0.7726 - val_loss: 0.4915 - val_acc: 0.7708\n",
      "Epoch 885/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4468 - acc: 0.7726 - val_loss: 0.4915 - val_acc: 0.7708\n",
      "Epoch 886/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4467 - acc: 0.7708 - val_loss: 0.4915 - val_acc: 0.7708\n",
      "Epoch 887/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4467 - acc: 0.7708 - val_loss: 0.4915 - val_acc: 0.7708\n",
      "Epoch 888/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4467 - acc: 0.7708 - val_loss: 0.4916 - val_acc: 0.7708\n",
      "Epoch 889/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4467 - acc: 0.7708 - val_loss: 0.4916 - val_acc: 0.7708\n",
      "Epoch 890/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4466 - acc: 0.7708 - val_loss: 0.4916 - val_acc: 0.7708\n",
      "Epoch 891/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4466 - acc: 0.7708 - val_loss: 0.4916 - val_acc: 0.7708\n",
      "Epoch 892/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4465 - acc: 0.7708 - val_loss: 0.4916 - val_acc: 0.7708\n",
      "Epoch 893/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4465 - acc: 0.7708 - val_loss: 0.4916 - val_acc: 0.7708\n",
      "Epoch 894/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4465 - acc: 0.7708 - val_loss: 0.4917 - val_acc: 0.7708\n",
      "Epoch 895/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4465 - acc: 0.7708 - val_loss: 0.4917 - val_acc: 0.7708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 896/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4465 - acc: 0.7708 - val_loss: 0.4917 - val_acc: 0.7708\n",
      "Epoch 897/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4464 - acc: 0.7708 - val_loss: 0.4917 - val_acc: 0.7708\n",
      "Epoch 898/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4464 - acc: 0.7708 - val_loss: 0.4917 - val_acc: 0.7708\n",
      "Epoch 899/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4463 - acc: 0.7708 - val_loss: 0.4917 - val_acc: 0.7708\n",
      "Epoch 900/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4463 - acc: 0.7708 - val_loss: 0.4917 - val_acc: 0.7708\n",
      "Epoch 901/1500\n",
      "576/576 [==============================] - 0s 80us/step - loss: 0.4463 - acc: 0.7708 - val_loss: 0.4918 - val_acc: 0.7708\n",
      "Epoch 902/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4463 - acc: 0.7708 - val_loss: 0.4918 - val_acc: 0.7708\n",
      "Epoch 903/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4463 - acc: 0.7708 - val_loss: 0.4918 - val_acc: 0.7708\n",
      "Epoch 904/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4463 - acc: 0.7708 - val_loss: 0.4918 - val_acc: 0.7708\n",
      "Epoch 905/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4462 - acc: 0.7708 - val_loss: 0.4918 - val_acc: 0.7708\n",
      "Epoch 906/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4462 - acc: 0.7708 - val_loss: 0.4918 - val_acc: 0.7708\n",
      "Epoch 907/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4462 - acc: 0.7708 - val_loss: 0.4918 - val_acc: 0.7708\n",
      "Epoch 908/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4461 - acc: 0.7708 - val_loss: 0.4919 - val_acc: 0.7708\n",
      "Epoch 909/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4461 - acc: 0.7708 - val_loss: 0.4919 - val_acc: 0.7708\n",
      "Epoch 910/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4461 - acc: 0.7708 - val_loss: 0.4919 - val_acc: 0.7708\n",
      "Epoch 911/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4460 - acc: 0.7708 - val_loss: 0.4919 - val_acc: 0.7708\n",
      "Epoch 912/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4460 - acc: 0.7708 - val_loss: 0.4919 - val_acc: 0.7656\n",
      "Epoch 913/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4460 - acc: 0.7708 - val_loss: 0.4919 - val_acc: 0.7656\n",
      "Epoch 914/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4460 - acc: 0.7708 - val_loss: 0.4920 - val_acc: 0.7656\n",
      "Epoch 915/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4459 - acc: 0.7708 - val_loss: 0.4920 - val_acc: 0.7656\n",
      "Epoch 916/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4459 - acc: 0.7708 - val_loss: 0.4920 - val_acc: 0.7656\n",
      "Epoch 917/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4459 - acc: 0.7708 - val_loss: 0.4920 - val_acc: 0.7656\n",
      "Epoch 918/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4459 - acc: 0.7708 - val_loss: 0.4920 - val_acc: 0.7656\n",
      "Epoch 919/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4458 - acc: 0.7708 - val_loss: 0.4920 - val_acc: 0.7656\n",
      "Epoch 920/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4458 - acc: 0.7708 - val_loss: 0.4921 - val_acc: 0.7656\n",
      "Epoch 921/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4458 - acc: 0.7708 - val_loss: 0.4921 - val_acc: 0.7656\n",
      "Epoch 922/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4458 - acc: 0.7708 - val_loss: 0.4921 - val_acc: 0.7656\n",
      "Epoch 923/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4458 - acc: 0.7708 - val_loss: 0.4921 - val_acc: 0.7656\n",
      "Epoch 924/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4457 - acc: 0.7708 - val_loss: 0.4921 - val_acc: 0.7656\n",
      "Epoch 925/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4457 - acc: 0.7708 - val_loss: 0.4922 - val_acc: 0.7656\n",
      "Epoch 926/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4457 - acc: 0.7708 - val_loss: 0.4922 - val_acc: 0.7656\n",
      "Epoch 927/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4457 - acc: 0.7708 - val_loss: 0.4922 - val_acc: 0.7656\n",
      "Epoch 928/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4456 - acc: 0.7708 - val_loss: 0.4922 - val_acc: 0.7656\n",
      "Epoch 929/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4456 - acc: 0.7708 - val_loss: 0.4922 - val_acc: 0.7656\n",
      "Epoch 930/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4456 - acc: 0.7708 - val_loss: 0.4922 - val_acc: 0.7656\n",
      "Epoch 931/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4456 - acc: 0.7708 - val_loss: 0.4923 - val_acc: 0.7656\n",
      "Epoch 932/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4455 - acc: 0.7708 - val_loss: 0.4923 - val_acc: 0.7656\n",
      "Epoch 933/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4455 - acc: 0.7708 - val_loss: 0.4923 - val_acc: 0.7656\n",
      "Epoch 934/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4455 - acc: 0.7708 - val_loss: 0.4923 - val_acc: 0.7656\n",
      "Epoch 935/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4455 - acc: 0.7708 - val_loss: 0.4923 - val_acc: 0.7656\n",
      "Epoch 936/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4454 - acc: 0.7708 - val_loss: 0.4924 - val_acc: 0.7656\n",
      "Epoch 937/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4455 - acc: 0.7708 - val_loss: 0.4924 - val_acc: 0.7656\n",
      "Epoch 938/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4454 - acc: 0.7708 - val_loss: 0.4924 - val_acc: 0.7656\n",
      "Epoch 939/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4454 - acc: 0.7708 - val_loss: 0.4924 - val_acc: 0.7656\n",
      "Epoch 940/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4454 - acc: 0.7708 - val_loss: 0.4924 - val_acc: 0.7656\n",
      "Epoch 941/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4453 - acc: 0.7708 - val_loss: 0.4924 - val_acc: 0.7656\n",
      "Epoch 942/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4453 - acc: 0.7708 - val_loss: 0.4924 - val_acc: 0.7656\n",
      "Epoch 943/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4453 - acc: 0.7708 - val_loss: 0.4924 - val_acc: 0.7656\n",
      "Epoch 944/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4453 - acc: 0.7708 - val_loss: 0.4924 - val_acc: 0.7656\n",
      "Epoch 945/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4452 - acc: 0.7708 - val_loss: 0.4925 - val_acc: 0.7656\n",
      "Epoch 946/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4452 - acc: 0.7708 - val_loss: 0.4925 - val_acc: 0.7656\n",
      "Epoch 947/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4452 - acc: 0.7726 - val_loss: 0.4925 - val_acc: 0.7656\n",
      "Epoch 948/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4451 - acc: 0.7708 - val_loss: 0.4925 - val_acc: 0.7656\n",
      "Epoch 949/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4451 - acc: 0.7726 - val_loss: 0.4925 - val_acc: 0.7656\n",
      "Epoch 950/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4451 - acc: 0.7708 - val_loss: 0.4925 - val_acc: 0.7656\n",
      "Epoch 951/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4451 - acc: 0.7726 - val_loss: 0.4925 - val_acc: 0.7656\n",
      "Epoch 952/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4450 - acc: 0.7726 - val_loss: 0.4926 - val_acc: 0.7656\n",
      "Epoch 953/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4450 - acc: 0.7726 - val_loss: 0.4926 - val_acc: 0.7656\n",
      "Epoch 954/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4450 - acc: 0.7726 - val_loss: 0.4926 - val_acc: 0.7656\n",
      "Epoch 955/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4450 - acc: 0.7726 - val_loss: 0.4926 - val_acc: 0.7656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 956/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4450 - acc: 0.7726 - val_loss: 0.4926 - val_acc: 0.7656\n",
      "Epoch 957/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4449 - acc: 0.7726 - val_loss: 0.4927 - val_acc: 0.7656\n",
      "Epoch 958/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4449 - acc: 0.7726 - val_loss: 0.4927 - val_acc: 0.7656\n",
      "Epoch 959/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4449 - acc: 0.7726 - val_loss: 0.4927 - val_acc: 0.7656\n",
      "Epoch 960/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4449 - acc: 0.7726 - val_loss: 0.4927 - val_acc: 0.7656\n",
      "Epoch 961/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4448 - acc: 0.7726 - val_loss: 0.4927 - val_acc: 0.7656\n",
      "Epoch 962/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4448 - acc: 0.7726 - val_loss: 0.4927 - val_acc: 0.7656\n",
      "Epoch 963/1500\n",
      "576/576 [==============================] - 0s 85us/step - loss: 0.4448 - acc: 0.7743 - val_loss: 0.4927 - val_acc: 0.7656\n",
      "Epoch 964/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4448 - acc: 0.7726 - val_loss: 0.4928 - val_acc: 0.7656\n",
      "Epoch 965/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4447 - acc: 0.7743 - val_loss: 0.4928 - val_acc: 0.7656\n",
      "Epoch 966/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4447 - acc: 0.7743 - val_loss: 0.4928 - val_acc: 0.7656\n",
      "Epoch 967/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4447 - acc: 0.7743 - val_loss: 0.4928 - val_acc: 0.7656\n",
      "Epoch 968/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4446 - acc: 0.7743 - val_loss: 0.4928 - val_acc: 0.7656\n",
      "Epoch 969/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4446 - acc: 0.7743 - val_loss: 0.4928 - val_acc: 0.7656\n",
      "Epoch 970/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4446 - acc: 0.7743 - val_loss: 0.4928 - val_acc: 0.7656\n",
      "Epoch 971/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4446 - acc: 0.7743 - val_loss: 0.4928 - val_acc: 0.7656\n",
      "Epoch 972/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4446 - acc: 0.7743 - val_loss: 0.4928 - val_acc: 0.7656\n",
      "Epoch 973/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4445 - acc: 0.7760 - val_loss: 0.4929 - val_acc: 0.7656\n",
      "Epoch 974/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4445 - acc: 0.7743 - val_loss: 0.4929 - val_acc: 0.7656\n",
      "Epoch 975/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4444 - acc: 0.7760 - val_loss: 0.4929 - val_acc: 0.7656\n",
      "Epoch 976/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4444 - acc: 0.7760 - val_loss: 0.4929 - val_acc: 0.7656\n",
      "Epoch 977/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4444 - acc: 0.7760 - val_loss: 0.4929 - val_acc: 0.7656\n",
      "Epoch 978/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4444 - acc: 0.7760 - val_loss: 0.4929 - val_acc: 0.7656\n",
      "Epoch 979/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4443 - acc: 0.7760 - val_loss: 0.4930 - val_acc: 0.7656\n",
      "Epoch 980/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4443 - acc: 0.7760 - val_loss: 0.4930 - val_acc: 0.7656\n",
      "Epoch 981/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4443 - acc: 0.7760 - val_loss: 0.4930 - val_acc: 0.7656\n",
      "Epoch 982/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4442 - acc: 0.7760 - val_loss: 0.4930 - val_acc: 0.7656\n",
      "Epoch 983/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4442 - acc: 0.7760 - val_loss: 0.4930 - val_acc: 0.7656\n",
      "Epoch 984/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4442 - acc: 0.7760 - val_loss: 0.4931 - val_acc: 0.7656\n",
      "Epoch 985/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4442 - acc: 0.7760 - val_loss: 0.4931 - val_acc: 0.7656\n",
      "Epoch 986/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4441 - acc: 0.7760 - val_loss: 0.4931 - val_acc: 0.7656\n",
      "Epoch 987/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4441 - acc: 0.7760 - val_loss: 0.4931 - val_acc: 0.7656\n",
      "Epoch 988/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4441 - acc: 0.7760 - val_loss: 0.4931 - val_acc: 0.7656\n",
      "Epoch 989/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4440 - acc: 0.7760 - val_loss: 0.4931 - val_acc: 0.7656\n",
      "Epoch 990/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4440 - acc: 0.7760 - val_loss: 0.4932 - val_acc: 0.7656\n",
      "Epoch 991/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4440 - acc: 0.7778 - val_loss: 0.4932 - val_acc: 0.7656\n",
      "Epoch 992/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4440 - acc: 0.7760 - val_loss: 0.4932 - val_acc: 0.7656\n",
      "Epoch 993/1500\n",
      "576/576 [==============================] - 0s 79us/step - loss: 0.4440 - acc: 0.7778 - val_loss: 0.4932 - val_acc: 0.7656\n",
      "Epoch 994/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4439 - acc: 0.7778 - val_loss: 0.4932 - val_acc: 0.7656\n",
      "Epoch 995/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4439 - acc: 0.7778 - val_loss: 0.4932 - val_acc: 0.7656\n",
      "Epoch 996/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4439 - acc: 0.7778 - val_loss: 0.4933 - val_acc: 0.7656\n",
      "Epoch 997/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4439 - acc: 0.7778 - val_loss: 0.4933 - val_acc: 0.7656\n",
      "Epoch 998/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4438 - acc: 0.7778 - val_loss: 0.4933 - val_acc: 0.7656\n",
      "Epoch 999/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4438 - acc: 0.7778 - val_loss: 0.4933 - val_acc: 0.7656\n",
      "Epoch 1000/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4437 - acc: 0.7778 - val_loss: 0.4933 - val_acc: 0.7656\n",
      "Epoch 1001/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4437 - acc: 0.7778 - val_loss: 0.4934 - val_acc: 0.7656\n",
      "Epoch 1002/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4437 - acc: 0.7778 - val_loss: 0.4934 - val_acc: 0.7656\n",
      "Epoch 1003/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4437 - acc: 0.7778 - val_loss: 0.4934 - val_acc: 0.7656\n",
      "Epoch 1004/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4436 - acc: 0.7778 - val_loss: 0.4934 - val_acc: 0.7656\n",
      "Epoch 1005/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4436 - acc: 0.7778 - val_loss: 0.4934 - val_acc: 0.7656\n",
      "Epoch 1006/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4436 - acc: 0.7778 - val_loss: 0.4934 - val_acc: 0.7656\n",
      "Epoch 1007/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4436 - acc: 0.7778 - val_loss: 0.4935 - val_acc: 0.7656\n",
      "Epoch 1008/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4436 - acc: 0.7778 - val_loss: 0.4935 - val_acc: 0.7656\n",
      "Epoch 1009/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4435 - acc: 0.7778 - val_loss: 0.4935 - val_acc: 0.7656\n",
      "Epoch 1010/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4435 - acc: 0.7778 - val_loss: 0.4935 - val_acc: 0.7656\n",
      "Epoch 1011/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4435 - acc: 0.7778 - val_loss: 0.4935 - val_acc: 0.7656\n",
      "Epoch 1012/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4434 - acc: 0.7778 - val_loss: 0.4935 - val_acc: 0.7656\n",
      "Epoch 1013/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4434 - acc: 0.7778 - val_loss: 0.4935 - val_acc: 0.7656\n",
      "Epoch 1014/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4434 - acc: 0.7778 - val_loss: 0.4935 - val_acc: 0.7656\n",
      "Epoch 1015/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 47us/step - loss: 0.4434 - acc: 0.7778 - val_loss: 0.4936 - val_acc: 0.7656\n",
      "Epoch 1016/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4433 - acc: 0.7778 - val_loss: 0.4936 - val_acc: 0.7656\n",
      "Epoch 1017/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4433 - acc: 0.7778 - val_loss: 0.4936 - val_acc: 0.7656\n",
      "Epoch 1018/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4433 - acc: 0.7778 - val_loss: 0.4936 - val_acc: 0.7656\n",
      "Epoch 1019/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4433 - acc: 0.7778 - val_loss: 0.4936 - val_acc: 0.7656\n",
      "Epoch 1020/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4433 - acc: 0.7778 - val_loss: 0.4936 - val_acc: 0.7656\n",
      "Epoch 1021/1500\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4432 - acc: 0.7778 - val_loss: 0.4937 - val_acc: 0.7656\n",
      "Epoch 1022/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4432 - acc: 0.7778 - val_loss: 0.4937 - val_acc: 0.7656\n",
      "Epoch 1023/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4432 - acc: 0.7778 - val_loss: 0.4937 - val_acc: 0.7656\n",
      "Epoch 1024/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4432 - acc: 0.7778 - val_loss: 0.4937 - val_acc: 0.7656\n",
      "Epoch 1025/1500\n",
      "576/576 [==============================] - 0s 84us/step - loss: 0.4431 - acc: 0.7778 - val_loss: 0.4937 - val_acc: 0.7656\n",
      "Epoch 1026/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4431 - acc: 0.7778 - val_loss: 0.4937 - val_acc: 0.7656\n",
      "Epoch 1027/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4431 - acc: 0.7778 - val_loss: 0.4937 - val_acc: 0.7708\n",
      "Epoch 1028/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4431 - acc: 0.7778 - val_loss: 0.4938 - val_acc: 0.7708\n",
      "Epoch 1029/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4430 - acc: 0.7778 - val_loss: 0.4938 - val_acc: 0.7708\n",
      "Epoch 1030/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4430 - acc: 0.7778 - val_loss: 0.4938 - val_acc: 0.7708\n",
      "Epoch 1031/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4430 - acc: 0.7778 - val_loss: 0.4938 - val_acc: 0.7708\n",
      "Epoch 1032/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4430 - acc: 0.7778 - val_loss: 0.4938 - val_acc: 0.7708\n",
      "Epoch 1033/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4429 - acc: 0.7778 - val_loss: 0.4938 - val_acc: 0.7708\n",
      "Epoch 1034/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4429 - acc: 0.7795 - val_loss: 0.4939 - val_acc: 0.7708\n",
      "Epoch 1035/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4429 - acc: 0.7778 - val_loss: 0.4939 - val_acc: 0.7708\n",
      "Epoch 1036/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4429 - acc: 0.7795 - val_loss: 0.4939 - val_acc: 0.7708\n",
      "Epoch 1037/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4428 - acc: 0.7795 - val_loss: 0.4939 - val_acc: 0.7708\n",
      "Epoch 1038/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4428 - acc: 0.7778 - val_loss: 0.4939 - val_acc: 0.7708\n",
      "Epoch 1039/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4428 - acc: 0.7795 - val_loss: 0.4939 - val_acc: 0.7708\n",
      "Epoch 1040/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4428 - acc: 0.7795 - val_loss: 0.4940 - val_acc: 0.7708\n",
      "Epoch 1041/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4428 - acc: 0.7795 - val_loss: 0.4940 - val_acc: 0.7708\n",
      "Epoch 1042/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4427 - acc: 0.7795 - val_loss: 0.4940 - val_acc: 0.7708\n",
      "Epoch 1043/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4427 - acc: 0.7795 - val_loss: 0.4940 - val_acc: 0.7708\n",
      "Epoch 1044/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4427 - acc: 0.7795 - val_loss: 0.4940 - val_acc: 0.7708\n",
      "Epoch 1045/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4427 - acc: 0.7795 - val_loss: 0.4940 - val_acc: 0.7708\n",
      "Epoch 1046/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4426 - acc: 0.7795 - val_loss: 0.4940 - val_acc: 0.7708\n",
      "Epoch 1047/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4426 - acc: 0.7795 - val_loss: 0.4941 - val_acc: 0.7708\n",
      "Epoch 1048/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4426 - acc: 0.7795 - val_loss: 0.4941 - val_acc: 0.7708\n",
      "Epoch 1049/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4426 - acc: 0.7795 - val_loss: 0.4941 - val_acc: 0.7708\n",
      "Epoch 1050/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4426 - acc: 0.7795 - val_loss: 0.4941 - val_acc: 0.7708\n",
      "Epoch 1051/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4425 - acc: 0.7795 - val_loss: 0.4941 - val_acc: 0.7708\n",
      "Epoch 1052/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4425 - acc: 0.7795 - val_loss: 0.4942 - val_acc: 0.7708\n",
      "Epoch 1053/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4425 - acc: 0.7795 - val_loss: 0.4942 - val_acc: 0.7708\n",
      "Epoch 1054/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4425 - acc: 0.7795 - val_loss: 0.4942 - val_acc: 0.7708\n",
      "Epoch 1055/1500\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.4425 - acc: 0.7795 - val_loss: 0.4942 - val_acc: 0.7708\n",
      "Epoch 1056/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4424 - acc: 0.7795 - val_loss: 0.4942 - val_acc: 0.7708\n",
      "Epoch 1057/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4424 - acc: 0.7795 - val_loss: 0.4942 - val_acc: 0.7708\n",
      "Epoch 1058/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4424 - acc: 0.7795 - val_loss: 0.4943 - val_acc: 0.7708\n",
      "Epoch 1059/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4424 - acc: 0.7795 - val_loss: 0.4943 - val_acc: 0.7708\n",
      "Epoch 1060/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4424 - acc: 0.7795 - val_loss: 0.4943 - val_acc: 0.7708\n",
      "Epoch 1061/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4423 - acc: 0.7795 - val_loss: 0.4943 - val_acc: 0.7708\n",
      "Epoch 1062/1500\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.6765 - acc: 0.625 - 0s 48us/step - loss: 0.4424 - acc: 0.7795 - val_loss: 0.4943 - val_acc: 0.7708\n",
      "Epoch 1063/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4423 - acc: 0.7812 - val_loss: 0.4943 - val_acc: 0.7708\n",
      "Epoch 1064/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4423 - acc: 0.7795 - val_loss: 0.4943 - val_acc: 0.7708\n",
      "Epoch 1065/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4423 - acc: 0.7812 - val_loss: 0.4943 - val_acc: 0.7708\n",
      "Epoch 1066/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4422 - acc: 0.7812 - val_loss: 0.4944 - val_acc: 0.7708\n",
      "Epoch 1067/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4422 - acc: 0.7812 - val_loss: 0.4944 - val_acc: 0.7708\n",
      "Epoch 1068/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4422 - acc: 0.7812 - val_loss: 0.4944 - val_acc: 0.7708\n",
      "Epoch 1069/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4422 - acc: 0.7812 - val_loss: 0.4944 - val_acc: 0.7708\n",
      "Epoch 1070/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4421 - acc: 0.7812 - val_loss: 0.4944 - val_acc: 0.7708\n",
      "Epoch 1071/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4421 - acc: 0.7812 - val_loss: 0.4945 - val_acc: 0.7708\n",
      "Epoch 1072/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4421 - acc: 0.7795 - val_loss: 0.4945 - val_acc: 0.7708\n",
      "Epoch 1073/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4421 - acc: 0.7812 - val_loss: 0.4945 - val_acc: 0.7708\n",
      "Epoch 1074/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 45us/step - loss: 0.4421 - acc: 0.7830 - val_loss: 0.4945 - val_acc: 0.7708\n",
      "Epoch 1075/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4420 - acc: 0.7812 - val_loss: 0.4945 - val_acc: 0.7708\n",
      "Epoch 1076/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4420 - acc: 0.7830 - val_loss: 0.4945 - val_acc: 0.7708\n",
      "Epoch 1077/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4420 - acc: 0.7830 - val_loss: 0.4945 - val_acc: 0.7708\n",
      "Epoch 1078/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4420 - acc: 0.7830 - val_loss: 0.4946 - val_acc: 0.7708\n",
      "Epoch 1079/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4420 - acc: 0.7830 - val_loss: 0.4946 - val_acc: 0.7708\n",
      "Epoch 1080/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4419 - acc: 0.7830 - val_loss: 0.4946 - val_acc: 0.7708\n",
      "Epoch 1081/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4419 - acc: 0.7830 - val_loss: 0.4946 - val_acc: 0.7708\n",
      "Epoch 1082/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4419 - acc: 0.7830 - val_loss: 0.4946 - val_acc: 0.7708\n",
      "Epoch 1083/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4419 - acc: 0.7830 - val_loss: 0.4946 - val_acc: 0.7708\n",
      "Epoch 1084/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4419 - acc: 0.7830 - val_loss: 0.4946 - val_acc: 0.7708\n",
      "Epoch 1085/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4418 - acc: 0.7830 - val_loss: 0.4946 - val_acc: 0.7708\n",
      "Epoch 1086/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4418 - acc: 0.7830 - val_loss: 0.4946 - val_acc: 0.7708\n",
      "Epoch 1087/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4418 - acc: 0.7830 - val_loss: 0.4946 - val_acc: 0.7708\n",
      "Epoch 1088/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4418 - acc: 0.7830 - val_loss: 0.4946 - val_acc: 0.7708\n",
      "Epoch 1089/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4418 - acc: 0.7830 - val_loss: 0.4947 - val_acc: 0.7708\n",
      "Epoch 1090/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4417 - acc: 0.7830 - val_loss: 0.4947 - val_acc: 0.7708\n",
      "Epoch 1091/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4417 - acc: 0.7847 - val_loss: 0.4947 - val_acc: 0.7708\n",
      "Epoch 1092/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4417 - acc: 0.7830 - val_loss: 0.4947 - val_acc: 0.7708\n",
      "Epoch 1093/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4417 - acc: 0.7847 - val_loss: 0.4947 - val_acc: 0.7708\n",
      "Epoch 1094/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4416 - acc: 0.7830 - val_loss: 0.4948 - val_acc: 0.7708\n",
      "Epoch 1095/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4416 - acc: 0.7830 - val_loss: 0.4948 - val_acc: 0.7708\n",
      "Epoch 1096/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4416 - acc: 0.7830 - val_loss: 0.4948 - val_acc: 0.7708\n",
      "Epoch 1097/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4416 - acc: 0.7847 - val_loss: 0.4948 - val_acc: 0.7708\n",
      "Epoch 1098/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4416 - acc: 0.7830 - val_loss: 0.4948 - val_acc: 0.7708\n",
      "Epoch 1099/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4416 - acc: 0.7830 - val_loss: 0.4948 - val_acc: 0.7708\n",
      "Epoch 1100/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4416 - acc: 0.7830 - val_loss: 0.4949 - val_acc: 0.7708\n",
      "Epoch 1101/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4415 - acc: 0.7830 - val_loss: 0.4949 - val_acc: 0.7708\n",
      "Epoch 1102/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4415 - acc: 0.7830 - val_loss: 0.4949 - val_acc: 0.7708\n",
      "Epoch 1103/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4415 - acc: 0.7830 - val_loss: 0.4949 - val_acc: 0.7708\n",
      "Epoch 1104/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4415 - acc: 0.7847 - val_loss: 0.4949 - val_acc: 0.7708\n",
      "Epoch 1105/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4415 - acc: 0.7830 - val_loss: 0.4949 - val_acc: 0.7708\n",
      "Epoch 1106/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4414 - acc: 0.7830 - val_loss: 0.4949 - val_acc: 0.7708\n",
      "Epoch 1107/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4414 - acc: 0.7847 - val_loss: 0.4950 - val_acc: 0.7708\n",
      "Epoch 1108/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4414 - acc: 0.7830 - val_loss: 0.4950 - val_acc: 0.7708\n",
      "Epoch 1109/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4414 - acc: 0.7830 - val_loss: 0.4950 - val_acc: 0.7708\n",
      "Epoch 1110/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4414 - acc: 0.7847 - val_loss: 0.4950 - val_acc: 0.7708\n",
      "Epoch 1111/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4413 - acc: 0.7830 - val_loss: 0.4950 - val_acc: 0.7656\n",
      "Epoch 1112/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4413 - acc: 0.7847 - val_loss: 0.4951 - val_acc: 0.7656\n",
      "Epoch 1113/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4413 - acc: 0.7847 - val_loss: 0.4951 - val_acc: 0.7656\n",
      "Epoch 1114/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4413 - acc: 0.7847 - val_loss: 0.4951 - val_acc: 0.7656\n",
      "Epoch 1115/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4413 - acc: 0.7830 - val_loss: 0.4951 - val_acc: 0.7656\n",
      "Epoch 1116/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4413 - acc: 0.7830 - val_loss: 0.4951 - val_acc: 0.7656\n",
      "Epoch 1117/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4412 - acc: 0.7830 - val_loss: 0.4951 - val_acc: 0.7656\n",
      "Epoch 1118/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4412 - acc: 0.7847 - val_loss: 0.4951 - val_acc: 0.7656\n",
      "Epoch 1119/1500\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4412 - acc: 0.7830 - val_loss: 0.4951 - val_acc: 0.7656\n",
      "Epoch 1120/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4412 - acc: 0.7830 - val_loss: 0.4952 - val_acc: 0.7656\n",
      "Epoch 1121/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4412 - acc: 0.7812 - val_loss: 0.4952 - val_acc: 0.7656\n",
      "Epoch 1122/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4412 - acc: 0.7830 - val_loss: 0.4952 - val_acc: 0.7656\n",
      "Epoch 1123/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4411 - acc: 0.7830 - val_loss: 0.4952 - val_acc: 0.7656\n",
      "Epoch 1124/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4411 - acc: 0.7830 - val_loss: 0.4952 - val_acc: 0.7656\n",
      "Epoch 1125/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4411 - acc: 0.7830 - val_loss: 0.4952 - val_acc: 0.7656\n",
      "Epoch 1126/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4411 - acc: 0.7830 - val_loss: 0.4952 - val_acc: 0.7656\n",
      "Epoch 1127/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4411 - acc: 0.7830 - val_loss: 0.4952 - val_acc: 0.7656\n",
      "Epoch 1128/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4411 - acc: 0.7830 - val_loss: 0.4953 - val_acc: 0.7656\n",
      "Epoch 1129/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4410 - acc: 0.7830 - val_loss: 0.4953 - val_acc: 0.7656\n",
      "Epoch 1130/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4410 - acc: 0.7830 - val_loss: 0.4953 - val_acc: 0.7656\n",
      "Epoch 1131/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4410 - acc: 0.7812 - val_loss: 0.4953 - val_acc: 0.7656\n",
      "Epoch 1132/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4410 - acc: 0.7830 - val_loss: 0.4953 - val_acc: 0.7656\n",
      "Epoch 1133/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 55us/step - loss: 0.4410 - acc: 0.7830 - val_loss: 0.4953 - val_acc: 0.7656\n",
      "Epoch 1134/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4410 - acc: 0.7830 - val_loss: 0.4954 - val_acc: 0.7656\n",
      "Epoch 1135/1500\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4409 - acc: 0.7830 - val_loss: 0.4954 - val_acc: 0.7656\n",
      "Epoch 1136/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4409 - acc: 0.7812 - val_loss: 0.4954 - val_acc: 0.7656\n",
      "Epoch 1137/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4409 - acc: 0.7830 - val_loss: 0.4954 - val_acc: 0.7656\n",
      "Epoch 1138/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4409 - acc: 0.7830 - val_loss: 0.4955 - val_acc: 0.7656\n",
      "Epoch 1139/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4409 - acc: 0.7812 - val_loss: 0.4955 - val_acc: 0.7656\n",
      "Epoch 1140/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4408 - acc: 0.7830 - val_loss: 0.4955 - val_acc: 0.7656\n",
      "Epoch 1141/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4408 - acc: 0.7830 - val_loss: 0.4955 - val_acc: 0.7656\n",
      "Epoch 1142/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4408 - acc: 0.7830 - val_loss: 0.4956 - val_acc: 0.7656\n",
      "Epoch 1143/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4407 - acc: 0.7830 - val_loss: 0.4956 - val_acc: 0.7656\n",
      "Epoch 1144/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4407 - acc: 0.7812 - val_loss: 0.4956 - val_acc: 0.7656\n",
      "Epoch 1145/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4407 - acc: 0.7812 - val_loss: 0.4956 - val_acc: 0.7656\n",
      "Epoch 1146/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4407 - acc: 0.7795 - val_loss: 0.4956 - val_acc: 0.7656\n",
      "Epoch 1147/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4407 - acc: 0.7795 - val_loss: 0.4957 - val_acc: 0.7656\n",
      "Epoch 1148/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4407 - acc: 0.7812 - val_loss: 0.4957 - val_acc: 0.7656\n",
      "Epoch 1149/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4406 - acc: 0.7812 - val_loss: 0.4957 - val_acc: 0.7656\n",
      "Epoch 1150/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4406 - acc: 0.7795 - val_loss: 0.4957 - val_acc: 0.7656\n",
      "Epoch 1151/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4406 - acc: 0.7812 - val_loss: 0.4957 - val_acc: 0.7656\n",
      "Epoch 1152/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4406 - acc: 0.7795 - val_loss: 0.4958 - val_acc: 0.7656\n",
      "Epoch 1153/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4405 - acc: 0.7812 - val_loss: 0.4958 - val_acc: 0.7656\n",
      "Epoch 1154/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4405 - acc: 0.7812 - val_loss: 0.4958 - val_acc: 0.7656\n",
      "Epoch 1155/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4405 - acc: 0.7795 - val_loss: 0.4958 - val_acc: 0.7656\n",
      "Epoch 1156/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4405 - acc: 0.7812 - val_loss: 0.4958 - val_acc: 0.7656\n",
      "Epoch 1157/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4405 - acc: 0.7795 - val_loss: 0.4959 - val_acc: 0.7656\n",
      "Epoch 1158/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4405 - acc: 0.7795 - val_loss: 0.4959 - val_acc: 0.7656\n",
      "Epoch 1159/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4404 - acc: 0.7812 - val_loss: 0.4959 - val_acc: 0.7656\n",
      "Epoch 1160/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4404 - acc: 0.7795 - val_loss: 0.4959 - val_acc: 0.7656\n",
      "Epoch 1161/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4404 - acc: 0.7812 - val_loss: 0.4959 - val_acc: 0.7656\n",
      "Epoch 1162/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4404 - acc: 0.7812 - val_loss: 0.4960 - val_acc: 0.7656\n",
      "Epoch 1163/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4403 - acc: 0.7812 - val_loss: 0.4960 - val_acc: 0.7656\n",
      "Epoch 1164/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4403 - acc: 0.7812 - val_loss: 0.4960 - val_acc: 0.7656\n",
      "Epoch 1165/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4403 - acc: 0.7795 - val_loss: 0.4960 - val_acc: 0.7656\n",
      "Epoch 1166/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4403 - acc: 0.7812 - val_loss: 0.4960 - val_acc: 0.7656\n",
      "Epoch 1167/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4403 - acc: 0.7778 - val_loss: 0.4961 - val_acc: 0.7656\n",
      "Epoch 1168/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4403 - acc: 0.7812 - val_loss: 0.4961 - val_acc: 0.7656\n",
      "Epoch 1169/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4402 - acc: 0.7795 - val_loss: 0.4961 - val_acc: 0.7656\n",
      "Epoch 1170/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4402 - acc: 0.7812 - val_loss: 0.4961 - val_acc: 0.7656\n",
      "Epoch 1171/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4402 - acc: 0.7795 - val_loss: 0.4962 - val_acc: 0.7656\n",
      "Epoch 1172/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4402 - acc: 0.7795 - val_loss: 0.4962 - val_acc: 0.7656\n",
      "Epoch 1173/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4402 - acc: 0.7812 - val_loss: 0.4962 - val_acc: 0.7656\n",
      "Epoch 1174/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4402 - acc: 0.7795 - val_loss: 0.4962 - val_acc: 0.7656\n",
      "Epoch 1175/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4402 - acc: 0.7795 - val_loss: 0.4962 - val_acc: 0.7656\n",
      "Epoch 1176/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4402 - acc: 0.7795 - val_loss: 0.4963 - val_acc: 0.7656\n",
      "Epoch 1177/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4401 - acc: 0.7795 - val_loss: 0.4963 - val_acc: 0.7656\n",
      "Epoch 1178/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4401 - acc: 0.7795 - val_loss: 0.4963 - val_acc: 0.7656\n",
      "Epoch 1179/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4401 - acc: 0.7795 - val_loss: 0.4963 - val_acc: 0.7656\n",
      "Epoch 1180/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4400 - acc: 0.7795 - val_loss: 0.4963 - val_acc: 0.7656\n",
      "Epoch 1181/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4400 - acc: 0.7795 - val_loss: 0.4964 - val_acc: 0.7656\n",
      "Epoch 1182/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4400 - acc: 0.7795 - val_loss: 0.4964 - val_acc: 0.7656\n",
      "Epoch 1183/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4400 - acc: 0.7795 - val_loss: 0.4964 - val_acc: 0.7656\n",
      "Epoch 1184/1500\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4400 - acc: 0.7795 - val_loss: 0.4965 - val_acc: 0.7656\n",
      "Epoch 1185/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4399 - acc: 0.7795 - val_loss: 0.4965 - val_acc: 0.7656\n",
      "Epoch 1186/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4399 - acc: 0.7795 - val_loss: 0.4965 - val_acc: 0.7656\n",
      "Epoch 1187/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4399 - acc: 0.7795 - val_loss: 0.4966 - val_acc: 0.7656\n",
      "Epoch 1188/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4399 - acc: 0.7795 - val_loss: 0.4966 - val_acc: 0.7656\n",
      "Epoch 1189/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4399 - acc: 0.7795 - val_loss: 0.4966 - val_acc: 0.7656\n",
      "Epoch 1190/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4399 - acc: 0.7795 - val_loss: 0.4966 - val_acc: 0.7656\n",
      "Epoch 1191/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4399 - acc: 0.7795 - val_loss: 0.4967 - val_acc: 0.7656\n",
      "Epoch 1192/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 52us/step - loss: 0.4398 - acc: 0.7795 - val_loss: 0.4967 - val_acc: 0.7656\n",
      "Epoch 1193/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4398 - acc: 0.7795 - val_loss: 0.4967 - val_acc: 0.7656\n",
      "Epoch 1194/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4398 - acc: 0.7795 - val_loss: 0.4967 - val_acc: 0.7656\n",
      "Epoch 1195/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4398 - acc: 0.7795 - val_loss: 0.4967 - val_acc: 0.7656\n",
      "Epoch 1196/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4398 - acc: 0.7795 - val_loss: 0.4968 - val_acc: 0.7656\n",
      "Epoch 1197/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4398 - acc: 0.7795 - val_loss: 0.4968 - val_acc: 0.7656\n",
      "Epoch 1198/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4397 - acc: 0.7795 - val_loss: 0.4968 - val_acc: 0.7656\n",
      "Epoch 1199/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4397 - acc: 0.7795 - val_loss: 0.4968 - val_acc: 0.7656\n",
      "Epoch 1200/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4397 - acc: 0.7795 - val_loss: 0.4969 - val_acc: 0.7656\n",
      "Epoch 1201/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4397 - acc: 0.7795 - val_loss: 0.4969 - val_acc: 0.7656\n",
      "Epoch 1202/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4396 - acc: 0.7795 - val_loss: 0.4969 - val_acc: 0.7656\n",
      "Epoch 1203/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4396 - acc: 0.7795 - val_loss: 0.4970 - val_acc: 0.7656\n",
      "Epoch 1204/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4396 - acc: 0.7812 - val_loss: 0.4970 - val_acc: 0.7656\n",
      "Epoch 1205/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4396 - acc: 0.7795 - val_loss: 0.4970 - val_acc: 0.7656\n",
      "Epoch 1206/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4396 - acc: 0.7812 - val_loss: 0.4970 - val_acc: 0.7656\n",
      "Epoch 1207/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4396 - acc: 0.7795 - val_loss: 0.4971 - val_acc: 0.7656\n",
      "Epoch 1208/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4396 - acc: 0.7795 - val_loss: 0.4971 - val_acc: 0.7656\n",
      "Epoch 1209/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4395 - acc: 0.7812 - val_loss: 0.4971 - val_acc: 0.7656\n",
      "Epoch 1210/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4395 - acc: 0.7795 - val_loss: 0.4971 - val_acc: 0.7656\n",
      "Epoch 1211/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4395 - acc: 0.7812 - val_loss: 0.4972 - val_acc: 0.7656\n",
      "Epoch 1212/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4395 - acc: 0.7812 - val_loss: 0.4972 - val_acc: 0.7656\n",
      "Epoch 1213/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4395 - acc: 0.7812 - val_loss: 0.4972 - val_acc: 0.7656\n",
      "Epoch 1214/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4395 - acc: 0.7812 - val_loss: 0.4972 - val_acc: 0.7656\n",
      "Epoch 1215/1500\n",
      "576/576 [==============================] - 0s 79us/step - loss: 0.4394 - acc: 0.7812 - val_loss: 0.4972 - val_acc: 0.7656\n",
      "Epoch 1216/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4394 - acc: 0.7812 - val_loss: 0.4973 - val_acc: 0.7656\n",
      "Epoch 1217/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4394 - acc: 0.7812 - val_loss: 0.4973 - val_acc: 0.7656\n",
      "Epoch 1218/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4394 - acc: 0.7812 - val_loss: 0.4973 - val_acc: 0.7656\n",
      "Epoch 1219/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4394 - acc: 0.7812 - val_loss: 0.4973 - val_acc: 0.7656\n",
      "Epoch 1220/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4394 - acc: 0.7812 - val_loss: 0.4973 - val_acc: 0.7656\n",
      "Epoch 1221/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4393 - acc: 0.7812 - val_loss: 0.4973 - val_acc: 0.7656\n",
      "Epoch 1222/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4393 - acc: 0.7812 - val_loss: 0.4974 - val_acc: 0.7656\n",
      "Epoch 1223/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4393 - acc: 0.7812 - val_loss: 0.4974 - val_acc: 0.7656\n",
      "Epoch 1224/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4393 - acc: 0.7812 - val_loss: 0.4974 - val_acc: 0.7656\n",
      "Epoch 1225/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4393 - acc: 0.7812 - val_loss: 0.4974 - val_acc: 0.7656\n",
      "Epoch 1226/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4393 - acc: 0.7812 - val_loss: 0.4975 - val_acc: 0.7656\n",
      "Epoch 1227/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4392 - acc: 0.7812 - val_loss: 0.4975 - val_acc: 0.7656\n",
      "Epoch 1228/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4392 - acc: 0.7812 - val_loss: 0.4975 - val_acc: 0.7656\n",
      "Epoch 1229/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4392 - acc: 0.7812 - val_loss: 0.4975 - val_acc: 0.7656\n",
      "Epoch 1230/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4392 - acc: 0.7812 - val_loss: 0.4976 - val_acc: 0.7656\n",
      "Epoch 1231/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4392 - acc: 0.7812 - val_loss: 0.4976 - val_acc: 0.7656\n",
      "Epoch 1232/1500\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4391 - acc: 0.7812 - val_loss: 0.4976 - val_acc: 0.7656\n",
      "Epoch 1233/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4391 - acc: 0.7812 - val_loss: 0.4976 - val_acc: 0.7656\n",
      "Epoch 1234/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4391 - acc: 0.7812 - val_loss: 0.4976 - val_acc: 0.7656\n",
      "Epoch 1235/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4391 - acc: 0.7812 - val_loss: 0.4976 - val_acc: 0.7656\n",
      "Epoch 1236/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4391 - acc: 0.7812 - val_loss: 0.4977 - val_acc: 0.7656\n",
      "Epoch 1237/1500\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4391 - acc: 0.7812 - val_loss: 0.4977 - val_acc: 0.7656\n",
      "Epoch 1238/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4390 - acc: 0.7812 - val_loss: 0.4977 - val_acc: 0.7656\n",
      "Epoch 1239/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4390 - acc: 0.7812 - val_loss: 0.4977 - val_acc: 0.7656\n",
      "Epoch 1240/1500\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4390 - acc: 0.7812 - val_loss: 0.4978 - val_acc: 0.7656\n",
      "Epoch 1241/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4390 - acc: 0.7812 - val_loss: 0.4978 - val_acc: 0.7656\n",
      "Epoch 1242/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4390 - acc: 0.7812 - val_loss: 0.4978 - val_acc: 0.7656\n",
      "Epoch 1243/1500\n",
      "576/576 [==============================] - 0s 97us/step - loss: 0.4389 - acc: 0.7812 - val_loss: 0.4978 - val_acc: 0.7656\n",
      "Epoch 1244/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4390 - acc: 0.7812 - val_loss: 0.4978 - val_acc: 0.7656\n",
      "Epoch 1245/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4389 - acc: 0.7830 - val_loss: 0.4979 - val_acc: 0.7656\n",
      "Epoch 1246/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4389 - acc: 0.7830 - val_loss: 0.4979 - val_acc: 0.7656\n",
      "Epoch 1247/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4389 - acc: 0.7830 - val_loss: 0.4979 - val_acc: 0.7656\n",
      "Epoch 1248/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4389 - acc: 0.7830 - val_loss: 0.4979 - val_acc: 0.7656\n",
      "Epoch 1249/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4389 - acc: 0.7830 - val_loss: 0.4979 - val_acc: 0.7656\n",
      "Epoch 1250/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4389 - acc: 0.7830 - val_loss: 0.4980 - val_acc: 0.7656\n",
      "Epoch 1251/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 58us/step - loss: 0.4389 - acc: 0.7830 - val_loss: 0.4980 - val_acc: 0.7656\n",
      "Epoch 1252/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4388 - acc: 0.7830 - val_loss: 0.4980 - val_acc: 0.7656\n",
      "Epoch 1253/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4388 - acc: 0.7830 - val_loss: 0.4980 - val_acc: 0.7656\n",
      "Epoch 1254/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4388 - acc: 0.7830 - val_loss: 0.4981 - val_acc: 0.7656\n",
      "Epoch 1255/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4388 - acc: 0.7830 - val_loss: 0.4981 - val_acc: 0.7656\n",
      "Epoch 1256/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4388 - acc: 0.7830 - val_loss: 0.4981 - val_acc: 0.7656\n",
      "Epoch 1257/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4388 - acc: 0.7812 - val_loss: 0.4981 - val_acc: 0.7656\n",
      "Epoch 1258/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4387 - acc: 0.7830 - val_loss: 0.4981 - val_acc: 0.7656\n",
      "Epoch 1259/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4387 - acc: 0.7830 - val_loss: 0.4981 - val_acc: 0.7656\n",
      "Epoch 1260/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4387 - acc: 0.7812 - val_loss: 0.4981 - val_acc: 0.7656\n",
      "Epoch 1261/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4387 - acc: 0.7830 - val_loss: 0.4982 - val_acc: 0.7656\n",
      "Epoch 1262/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4387 - acc: 0.7830 - val_loss: 0.4982 - val_acc: 0.7656\n",
      "Epoch 1263/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4386 - acc: 0.7830 - val_loss: 0.4982 - val_acc: 0.7656\n",
      "Epoch 1264/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4386 - acc: 0.7830 - val_loss: 0.4982 - val_acc: 0.7656\n",
      "Epoch 1265/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4386 - acc: 0.7812 - val_loss: 0.4982 - val_acc: 0.7656\n",
      "Epoch 1266/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4386 - acc: 0.7812 - val_loss: 0.4983 - val_acc: 0.7708\n",
      "Epoch 1267/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4386 - acc: 0.7830 - val_loss: 0.4983 - val_acc: 0.7708\n",
      "Epoch 1268/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4386 - acc: 0.7830 - val_loss: 0.4983 - val_acc: 0.7656\n",
      "Epoch 1269/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4385 - acc: 0.7812 - val_loss: 0.4983 - val_acc: 0.7656\n",
      "Epoch 1270/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4386 - acc: 0.7812 - val_loss: 0.4983 - val_acc: 0.7656\n",
      "Epoch 1271/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4385 - acc: 0.7830 - val_loss: 0.4983 - val_acc: 0.7656\n",
      "Epoch 1272/1500\n",
      "576/576 [==============================] - 0s 83us/step - loss: 0.4385 - acc: 0.7830 - val_loss: 0.4984 - val_acc: 0.7656\n",
      "Epoch 1273/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4385 - acc: 0.7830 - val_loss: 0.4984 - val_acc: 0.7656\n",
      "Epoch 1274/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4385 - acc: 0.7830 - val_loss: 0.4984 - val_acc: 0.7656\n",
      "Epoch 1275/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4385 - acc: 0.7830 - val_loss: 0.4984 - val_acc: 0.7656\n",
      "Epoch 1276/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4385 - acc: 0.7830 - val_loss: 0.4984 - val_acc: 0.7656\n",
      "Epoch 1277/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4385 - acc: 0.7830 - val_loss: 0.4984 - val_acc: 0.7656\n",
      "Epoch 1278/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4384 - acc: 0.7830 - val_loss: 0.4985 - val_acc: 0.7656\n",
      "Epoch 1279/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4384 - acc: 0.7830 - val_loss: 0.4985 - val_acc: 0.7656\n",
      "Epoch 1280/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4384 - acc: 0.7812 - val_loss: 0.4985 - val_acc: 0.7656\n",
      "Epoch 1281/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4384 - acc: 0.7830 - val_loss: 0.4985 - val_acc: 0.7656\n",
      "Epoch 1282/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4384 - acc: 0.7830 - val_loss: 0.4985 - val_acc: 0.7656\n",
      "Epoch 1283/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4383 - acc: 0.7830 - val_loss: 0.4985 - val_acc: 0.7656\n",
      "Epoch 1284/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4383 - acc: 0.7812 - val_loss: 0.4986 - val_acc: 0.7656\n",
      "Epoch 1285/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4383 - acc: 0.7830 - val_loss: 0.4986 - val_acc: 0.7656\n",
      "Epoch 1286/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4383 - acc: 0.7812 - val_loss: 0.4986 - val_acc: 0.7656\n",
      "Epoch 1287/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4383 - acc: 0.7830 - val_loss: 0.4986 - val_acc: 0.7656\n",
      "Epoch 1288/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4383 - acc: 0.7830 - val_loss: 0.4986 - val_acc: 0.7656\n",
      "Epoch 1289/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4383 - acc: 0.7830 - val_loss: 0.4986 - val_acc: 0.7656\n",
      "Epoch 1290/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4382 - acc: 0.7812 - val_loss: 0.4987 - val_acc: 0.7656\n",
      "Epoch 1291/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4382 - acc: 0.7830 - val_loss: 0.4987 - val_acc: 0.7656\n",
      "Epoch 1292/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4382 - acc: 0.7830 - val_loss: 0.4987 - val_acc: 0.7656\n",
      "Epoch 1293/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4382 - acc: 0.7812 - val_loss: 0.4987 - val_acc: 0.7656\n",
      "Epoch 1294/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4382 - acc: 0.7830 - val_loss: 0.4987 - val_acc: 0.7656\n",
      "Epoch 1295/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4382 - acc: 0.7830 - val_loss: 0.4987 - val_acc: 0.7656\n",
      "Epoch 1296/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4381 - acc: 0.7830 - val_loss: 0.4987 - val_acc: 0.7656\n",
      "Epoch 1297/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4381 - acc: 0.7830 - val_loss: 0.4988 - val_acc: 0.7656\n",
      "Epoch 1298/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4381 - acc: 0.7830 - val_loss: 0.4988 - val_acc: 0.7656\n",
      "Epoch 1299/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4381 - acc: 0.7830 - val_loss: 0.4988 - val_acc: 0.7656\n",
      "Epoch 1300/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4381 - acc: 0.7812 - val_loss: 0.4988 - val_acc: 0.7656\n",
      "Epoch 1301/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4381 - acc: 0.7830 - val_loss: 0.4988 - val_acc: 0.7656\n",
      "Epoch 1302/1500\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4380 - acc: 0.7830 - val_loss: 0.4988 - val_acc: 0.7656\n",
      "Epoch 1303/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4381 - acc: 0.7830 - val_loss: 0.4989 - val_acc: 0.7656\n",
      "Epoch 1304/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4381 - acc: 0.7830 - val_loss: 0.4989 - val_acc: 0.7656\n",
      "Epoch 1305/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4380 - acc: 0.7830 - val_loss: 0.4989 - val_acc: 0.7656\n",
      "Epoch 1306/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4380 - acc: 0.7830 - val_loss: 0.4989 - val_acc: 0.7656\n",
      "Epoch 1307/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4380 - acc: 0.7830 - val_loss: 0.4989 - val_acc: 0.7656\n",
      "Epoch 1308/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4380 - acc: 0.7830 - val_loss: 0.4989 - val_acc: 0.7656\n",
      "Epoch 1309/1500\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.4961 - acc: 0.750 - 0s 47us/step - loss: 0.4380 - acc: 0.7812 - val_loss: 0.4989 - val_acc: 0.7656\n",
      "Epoch 1310/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 46us/step - loss: 0.4380 - acc: 0.7830 - val_loss: 0.4989 - val_acc: 0.7656\n",
      "Epoch 1311/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4379 - acc: 0.7830 - val_loss: 0.4989 - val_acc: 0.7656\n",
      "Epoch 1312/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4379 - acc: 0.7830 - val_loss: 0.4990 - val_acc: 0.7656\n",
      "Epoch 1313/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4379 - acc: 0.7830 - val_loss: 0.4990 - val_acc: 0.7604\n",
      "Epoch 1314/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4379 - acc: 0.7830 - val_loss: 0.4990 - val_acc: 0.7604\n",
      "Epoch 1315/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4379 - acc: 0.7830 - val_loss: 0.4990 - val_acc: 0.7604\n",
      "Epoch 1316/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4379 - acc: 0.7830 - val_loss: 0.4990 - val_acc: 0.7604\n",
      "Epoch 1317/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4379 - acc: 0.7830 - val_loss: 0.4990 - val_acc: 0.7604\n",
      "Epoch 1318/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4378 - acc: 0.7830 - val_loss: 0.4990 - val_acc: 0.7604\n",
      "Epoch 1319/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4378 - acc: 0.7830 - val_loss: 0.4990 - val_acc: 0.7604\n",
      "Epoch 1320/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4378 - acc: 0.7830 - val_loss: 0.4990 - val_acc: 0.7604\n",
      "Epoch 1321/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4378 - acc: 0.7830 - val_loss: 0.4990 - val_acc: 0.7604\n",
      "Epoch 1322/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4377 - acc: 0.7830 - val_loss: 0.4991 - val_acc: 0.7604\n",
      "Epoch 1323/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4378 - acc: 0.7830 - val_loss: 0.4991 - val_acc: 0.7604\n",
      "Epoch 1324/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4377 - acc: 0.7830 - val_loss: 0.4991 - val_acc: 0.7604\n",
      "Epoch 1325/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4377 - acc: 0.7830 - val_loss: 0.4991 - val_acc: 0.7604\n",
      "Epoch 1326/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4377 - acc: 0.7830 - val_loss: 0.4991 - val_acc: 0.7604\n",
      "Epoch 1327/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4377 - acc: 0.7830 - val_loss: 0.4991 - val_acc: 0.7604\n",
      "Epoch 1328/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4377 - acc: 0.7830 - val_loss: 0.4991 - val_acc: 0.7604\n",
      "Epoch 1329/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4377 - acc: 0.7830 - val_loss: 0.4992 - val_acc: 0.7604\n",
      "Epoch 1330/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4377 - acc: 0.7830 - val_loss: 0.4992 - val_acc: 0.7604\n",
      "Epoch 1331/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4376 - acc: 0.7830 - val_loss: 0.4992 - val_acc: 0.7604\n",
      "Epoch 1332/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4376 - acc: 0.7830 - val_loss: 0.4992 - val_acc: 0.7604\n",
      "Epoch 1333/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4376 - acc: 0.7830 - val_loss: 0.4992 - val_acc: 0.7604\n",
      "Epoch 1334/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4376 - acc: 0.7830 - val_loss: 0.4992 - val_acc: 0.7604\n",
      "Epoch 1335/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4376 - acc: 0.7830 - val_loss: 0.4992 - val_acc: 0.7604\n",
      "Epoch 1336/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4376 - acc: 0.7830 - val_loss: 0.4993 - val_acc: 0.7604\n",
      "Epoch 1337/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4376 - acc: 0.7830 - val_loss: 0.4993 - val_acc: 0.7604\n",
      "Epoch 1338/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4376 - acc: 0.7830 - val_loss: 0.4993 - val_acc: 0.7604\n",
      "Epoch 1339/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4375 - acc: 0.7830 - val_loss: 0.4993 - val_acc: 0.7604\n",
      "Epoch 1340/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4375 - acc: 0.7830 - val_loss: 0.4993 - val_acc: 0.7604\n",
      "Epoch 1341/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4375 - acc: 0.7830 - val_loss: 0.4993 - val_acc: 0.7604\n",
      "Epoch 1342/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4374 - acc: 0.7830 - val_loss: 0.4994 - val_acc: 0.7604\n",
      "Epoch 1343/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4375 - acc: 0.7830 - val_loss: 0.4993 - val_acc: 0.7604\n",
      "Epoch 1344/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4375 - acc: 0.7830 - val_loss: 0.4994 - val_acc: 0.7604\n",
      "Epoch 1345/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4374 - acc: 0.7830 - val_loss: 0.4994 - val_acc: 0.7604\n",
      "Epoch 1346/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4374 - acc: 0.7830 - val_loss: 0.4994 - val_acc: 0.7604\n",
      "Epoch 1347/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4374 - acc: 0.7830 - val_loss: 0.4994 - val_acc: 0.7604\n",
      "Epoch 1348/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4374 - acc: 0.7830 - val_loss: 0.4994 - val_acc: 0.7604\n",
      "Epoch 1349/1500\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.4374 - acc: 0.7830 - val_loss: 0.4995 - val_acc: 0.7604\n",
      "Epoch 1350/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4374 - acc: 0.7830 - val_loss: 0.4995 - val_acc: 0.7604\n",
      "Epoch 1351/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4373 - acc: 0.7830 - val_loss: 0.4995 - val_acc: 0.7604\n",
      "Epoch 1352/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4373 - acc: 0.7830 - val_loss: 0.4995 - val_acc: 0.7604\n",
      "Epoch 1353/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4373 - acc: 0.7830 - val_loss: 0.4995 - val_acc: 0.7604\n",
      "Epoch 1354/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4373 - acc: 0.7847 - val_loss: 0.4996 - val_acc: 0.7604\n",
      "Epoch 1355/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4373 - acc: 0.7830 - val_loss: 0.4996 - val_acc: 0.7604\n",
      "Epoch 1356/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4373 - acc: 0.7830 - val_loss: 0.4996 - val_acc: 0.7604\n",
      "Epoch 1357/1500\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4373 - acc: 0.7847 - val_loss: 0.4996 - val_acc: 0.7604\n",
      "Epoch 1358/1500\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4373 - acc: 0.7830 - val_loss: 0.4996 - val_acc: 0.7604\n",
      "Epoch 1359/1500\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4373 - acc: 0.7847 - val_loss: 0.4996 - val_acc: 0.7604\n",
      "Epoch 1360/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4372 - acc: 0.7847 - val_loss: 0.4997 - val_acc: 0.7604\n",
      "Epoch 1361/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4372 - acc: 0.7847 - val_loss: 0.4997 - val_acc: 0.7604\n",
      "Epoch 1362/1500\n",
      "576/576 [==============================] - 0s 84us/step - loss: 0.4372 - acc: 0.7847 - val_loss: 0.4997 - val_acc: 0.7604\n",
      "Epoch 1363/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4372 - acc: 0.7847 - val_loss: 0.4997 - val_acc: 0.7604\n",
      "Epoch 1364/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4372 - acc: 0.7847 - val_loss: 0.4997 - val_acc: 0.7604\n",
      "Epoch 1365/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4372 - acc: 0.7847 - val_loss: 0.4998 - val_acc: 0.7604\n",
      "Epoch 1366/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4371 - acc: 0.7847 - val_loss: 0.4998 - val_acc: 0.7604\n",
      "Epoch 1367/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4371 - acc: 0.7847 - val_loss: 0.4998 - val_acc: 0.7604\n",
      "Epoch 1368/1500\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4371 - acc: 0.7847 - val_loss: 0.4998 - val_acc: 0.7604\n",
      "Epoch 1369/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 57us/step - loss: 0.4371 - acc: 0.7847 - val_loss: 0.4998 - val_acc: 0.7604\n",
      "Epoch 1370/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4371 - acc: 0.7847 - val_loss: 0.4998 - val_acc: 0.7604\n",
      "Epoch 1371/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4371 - acc: 0.7847 - val_loss: 0.4999 - val_acc: 0.7604\n",
      "Epoch 1372/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4371 - acc: 0.7847 - val_loss: 0.4999 - val_acc: 0.7604\n",
      "Epoch 1373/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4370 - acc: 0.7847 - val_loss: 0.4999 - val_acc: 0.7604\n",
      "Epoch 1374/1500\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4370 - acc: 0.7847 - val_loss: 0.4999 - val_acc: 0.7604\n",
      "Epoch 1375/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4370 - acc: 0.7847 - val_loss: 0.4999 - val_acc: 0.7552\n",
      "Epoch 1376/1500\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4370 - acc: 0.7847 - val_loss: 0.4999 - val_acc: 0.7552\n",
      "Epoch 1377/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4370 - acc: 0.7847 - val_loss: 0.5000 - val_acc: 0.7552\n",
      "Epoch 1378/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4370 - acc: 0.7847 - val_loss: 0.5000 - val_acc: 0.7552\n",
      "Epoch 1379/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4370 - acc: 0.7847 - val_loss: 0.5000 - val_acc: 0.7552\n",
      "Epoch 1380/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4369 - acc: 0.7847 - val_loss: 0.5000 - val_acc: 0.7552\n",
      "Epoch 1381/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4370 - acc: 0.7847 - val_loss: 0.5000 - val_acc: 0.7552\n",
      "Epoch 1382/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4369 - acc: 0.7847 - val_loss: 0.5001 - val_acc: 0.7552\n",
      "Epoch 1383/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4369 - acc: 0.7847 - val_loss: 0.5001 - val_acc: 0.7552\n",
      "Epoch 1384/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4369 - acc: 0.7847 - val_loss: 0.5001 - val_acc: 0.7552\n",
      "Epoch 1385/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4368 - acc: 0.7847 - val_loss: 0.5002 - val_acc: 0.7552\n",
      "Epoch 1386/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4368 - acc: 0.7847 - val_loss: 0.5002 - val_acc: 0.7552\n",
      "Epoch 1387/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4368 - acc: 0.7847 - val_loss: 0.5002 - val_acc: 0.7552\n",
      "Epoch 1388/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4368 - acc: 0.7847 - val_loss: 0.5002 - val_acc: 0.7552\n",
      "Epoch 1389/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4368 - acc: 0.7847 - val_loss: 0.5002 - val_acc: 0.7552\n",
      "Epoch 1390/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4368 - acc: 0.7847 - val_loss: 0.5003 - val_acc: 0.7552\n",
      "Epoch 1391/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4367 - acc: 0.7847 - val_loss: 0.5003 - val_acc: 0.7552\n",
      "Epoch 1392/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4368 - acc: 0.7847 - val_loss: 0.5003 - val_acc: 0.7552\n",
      "Epoch 1393/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4367 - acc: 0.7847 - val_loss: 0.5003 - val_acc: 0.7552\n",
      "Epoch 1394/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4367 - acc: 0.7847 - val_loss: 0.5003 - val_acc: 0.7552\n",
      "Epoch 1395/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4367 - acc: 0.7847 - val_loss: 0.5003 - val_acc: 0.7552\n",
      "Epoch 1396/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4367 - acc: 0.7847 - val_loss: 0.5004 - val_acc: 0.7552\n",
      "Epoch 1397/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4367 - acc: 0.7847 - val_loss: 0.5004 - val_acc: 0.7552\n",
      "Epoch 1398/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4367 - acc: 0.7865 - val_loss: 0.5004 - val_acc: 0.7552\n",
      "Epoch 1399/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4367 - acc: 0.7865 - val_loss: 0.5004 - val_acc: 0.7552\n",
      "Epoch 1400/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4366 - acc: 0.7847 - val_loss: 0.5004 - val_acc: 0.7552\n",
      "Epoch 1401/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4366 - acc: 0.7865 - val_loss: 0.5005 - val_acc: 0.7552\n",
      "Epoch 1402/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4366 - acc: 0.7865 - val_loss: 0.5005 - val_acc: 0.7552\n",
      "Epoch 1403/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4366 - acc: 0.7865 - val_loss: 0.5005 - val_acc: 0.7552\n",
      "Epoch 1404/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4366 - acc: 0.7865 - val_loss: 0.5005 - val_acc: 0.7552\n",
      "Epoch 1405/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4365 - acc: 0.7865 - val_loss: 0.5005 - val_acc: 0.7552\n",
      "Epoch 1406/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4365 - acc: 0.7865 - val_loss: 0.5006 - val_acc: 0.7552\n",
      "Epoch 1407/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4365 - acc: 0.7882 - val_loss: 0.5006 - val_acc: 0.7552\n",
      "Epoch 1408/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4365 - acc: 0.7865 - val_loss: 0.5006 - val_acc: 0.7552\n",
      "Epoch 1409/1500\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4365 - acc: 0.7882 - val_loss: 0.5006 - val_acc: 0.7552\n",
      "Epoch 1410/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4365 - acc: 0.7865 - val_loss: 0.5006 - val_acc: 0.7552\n",
      "Epoch 1411/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4364 - acc: 0.7882 - val_loss: 0.5006 - val_acc: 0.7552\n",
      "Epoch 1412/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4364 - acc: 0.7865 - val_loss: 0.5006 - val_acc: 0.7552\n",
      "Epoch 1413/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4364 - acc: 0.7882 - val_loss: 0.5007 - val_acc: 0.7552\n",
      "Epoch 1414/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4364 - acc: 0.7882 - val_loss: 0.5007 - val_acc: 0.7552\n",
      "Epoch 1415/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4364 - acc: 0.7882 - val_loss: 0.5007 - val_acc: 0.7552\n",
      "Epoch 1416/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4364 - acc: 0.7882 - val_loss: 0.5007 - val_acc: 0.7552\n",
      "Epoch 1417/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4364 - acc: 0.7882 - val_loss: 0.5007 - val_acc: 0.7552\n",
      "Epoch 1418/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4363 - acc: 0.7882 - val_loss: 0.5007 - val_acc: 0.7552\n",
      "Epoch 1419/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4363 - acc: 0.7882 - val_loss: 0.5008 - val_acc: 0.7552\n",
      "Epoch 1420/1500\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4363 - acc: 0.7882 - val_loss: 0.5008 - val_acc: 0.7552\n",
      "Epoch 1421/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4363 - acc: 0.7882 - val_loss: 0.5008 - val_acc: 0.7552\n",
      "Epoch 1422/1500\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4363 - acc: 0.7865 - val_loss: 0.5008 - val_acc: 0.7552\n",
      "Epoch 1423/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4363 - acc: 0.7882 - val_loss: 0.5008 - val_acc: 0.7552\n",
      "Epoch 1424/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4362 - acc: 0.7882 - val_loss: 0.5008 - val_acc: 0.7552\n",
      "Epoch 1425/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4362 - acc: 0.7865 - val_loss: 0.5008 - val_acc: 0.7552\n",
      "Epoch 1426/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4362 - acc: 0.7882 - val_loss: 0.5009 - val_acc: 0.7552\n",
      "Epoch 1427/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4362 - acc: 0.7847 - val_loss: 0.5009 - val_acc: 0.7552\n",
      "Epoch 1428/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 43us/step - loss: 0.4362 - acc: 0.7882 - val_loss: 0.5009 - val_acc: 0.7552\n",
      "Epoch 1429/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4362 - acc: 0.7865 - val_loss: 0.5009 - val_acc: 0.7552\n",
      "Epoch 1430/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4362 - acc: 0.7865 - val_loss: 0.5009 - val_acc: 0.7552\n",
      "Epoch 1431/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4361 - acc: 0.7865 - val_loss: 0.5009 - val_acc: 0.7552\n",
      "Epoch 1432/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4362 - acc: 0.7865 - val_loss: 0.5010 - val_acc: 0.7552\n",
      "Epoch 1433/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4361 - acc: 0.7865 - val_loss: 0.5010 - val_acc: 0.7552\n",
      "Epoch 1434/1500\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4361 - acc: 0.7882 - val_loss: 0.5010 - val_acc: 0.7552\n",
      "Epoch 1435/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4361 - acc: 0.7882 - val_loss: 0.5010 - val_acc: 0.7552\n",
      "Epoch 1436/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4360 - acc: 0.7882 - val_loss: 0.5011 - val_acc: 0.7552\n",
      "Epoch 1437/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4361 - acc: 0.7865 - val_loss: 0.5011 - val_acc: 0.7552\n",
      "Epoch 1438/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4360 - acc: 0.7865 - val_loss: 0.5011 - val_acc: 0.7552\n",
      "Epoch 1439/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4360 - acc: 0.7882 - val_loss: 0.5011 - val_acc: 0.7552\n",
      "Epoch 1440/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4360 - acc: 0.7865 - val_loss: 0.5012 - val_acc: 0.7552\n",
      "Epoch 1441/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4360 - acc: 0.7882 - val_loss: 0.5012 - val_acc: 0.7552\n",
      "Epoch 1442/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4360 - acc: 0.7865 - val_loss: 0.5012 - val_acc: 0.7552\n",
      "Epoch 1443/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4359 - acc: 0.7865 - val_loss: 0.5012 - val_acc: 0.7552\n",
      "Epoch 1444/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4359 - acc: 0.7865 - val_loss: 0.5012 - val_acc: 0.7552\n",
      "Epoch 1445/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4359 - acc: 0.7882 - val_loss: 0.5013 - val_acc: 0.7552\n",
      "Epoch 1446/1500\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.4599 - acc: 0.812 - 0s 79us/step - loss: 0.4359 - acc: 0.7882 - val_loss: 0.5013 - val_acc: 0.7552\n",
      "Epoch 1447/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4359 - acc: 0.7882 - val_loss: 0.5013 - val_acc: 0.7552\n",
      "Epoch 1448/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4359 - acc: 0.7882 - val_loss: 0.5013 - val_acc: 0.7552\n",
      "Epoch 1449/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4358 - acc: 0.7882 - val_loss: 0.5013 - val_acc: 0.7552\n",
      "Epoch 1450/1500\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.4358 - acc: 0.7899 - val_loss: 0.5013 - val_acc: 0.7552\n",
      "Epoch 1451/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4358 - acc: 0.7882 - val_loss: 0.5014 - val_acc: 0.7552\n",
      "Epoch 1452/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4358 - acc: 0.7882 - val_loss: 0.5014 - val_acc: 0.7552\n",
      "Epoch 1453/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4358 - acc: 0.7899 - val_loss: 0.5014 - val_acc: 0.7552\n",
      "Epoch 1454/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4358 - acc: 0.7899 - val_loss: 0.5014 - val_acc: 0.7552\n",
      "Epoch 1455/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4357 - acc: 0.7899 - val_loss: 0.5015 - val_acc: 0.7552\n",
      "Epoch 1456/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4357 - acc: 0.7882 - val_loss: 0.5015 - val_acc: 0.7552\n",
      "Epoch 1457/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4357 - acc: 0.7899 - val_loss: 0.5015 - val_acc: 0.7552\n",
      "Epoch 1458/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4357 - acc: 0.7899 - val_loss: 0.5015 - val_acc: 0.7552\n",
      "Epoch 1459/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4356 - acc: 0.7899 - val_loss: 0.5015 - val_acc: 0.7552\n",
      "Epoch 1460/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4356 - acc: 0.7899 - val_loss: 0.5016 - val_acc: 0.7552\n",
      "Epoch 1461/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4357 - acc: 0.7899 - val_loss: 0.5016 - val_acc: 0.7552\n",
      "Epoch 1462/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4356 - acc: 0.7899 - val_loss: 0.5016 - val_acc: 0.7552\n",
      "Epoch 1463/1500\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.5048 - acc: 0.687 - 0s 50us/step - loss: 0.4356 - acc: 0.7899 - val_loss: 0.5016 - val_acc: 0.7552\n",
      "Epoch 1464/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4356 - acc: 0.7899 - val_loss: 0.5016 - val_acc: 0.7552\n",
      "Epoch 1465/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4356 - acc: 0.7899 - val_loss: 0.5016 - val_acc: 0.7552\n",
      "Epoch 1466/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4355 - acc: 0.7899 - val_loss: 0.5017 - val_acc: 0.7552\n",
      "Epoch 1467/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4355 - acc: 0.7917 - val_loss: 0.5017 - val_acc: 0.7552\n",
      "Epoch 1468/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4355 - acc: 0.7899 - val_loss: 0.5017 - val_acc: 0.7552\n",
      "Epoch 1469/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4355 - acc: 0.7917 - val_loss: 0.5017 - val_acc: 0.7552\n",
      "Epoch 1470/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4355 - acc: 0.7917 - val_loss: 0.5017 - val_acc: 0.7552\n",
      "Epoch 1471/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4354 - acc: 0.7899 - val_loss: 0.5017 - val_acc: 0.7552\n",
      "Epoch 1472/1500\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4354 - acc: 0.7917 - val_loss: 0.5017 - val_acc: 0.7552\n",
      "Epoch 1473/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4354 - acc: 0.7917 - val_loss: 0.5018 - val_acc: 0.7552\n",
      "Epoch 1474/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4354 - acc: 0.7899 - val_loss: 0.5018 - val_acc: 0.7552\n",
      "Epoch 1475/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4354 - acc: 0.7917 - val_loss: 0.5018 - val_acc: 0.7552\n",
      "Epoch 1476/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4353 - acc: 0.7917 - val_loss: 0.5018 - val_acc: 0.7552\n",
      "Epoch 1477/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4353 - acc: 0.7917 - val_loss: 0.5018 - val_acc: 0.7552\n",
      "Epoch 1478/1500\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4353 - acc: 0.7917 - val_loss: 0.5018 - val_acc: 0.7552\n",
      "Epoch 1479/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4353 - acc: 0.7882 - val_loss: 0.5019 - val_acc: 0.7552\n",
      "Epoch 1480/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4353 - acc: 0.7917 - val_loss: 0.5019 - val_acc: 0.7552\n",
      "Epoch 1481/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4353 - acc: 0.7899 - val_loss: 0.5019 - val_acc: 0.7552\n",
      "Epoch 1482/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4353 - acc: 0.7899 - val_loss: 0.5019 - val_acc: 0.7552\n",
      "Epoch 1483/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4353 - acc: 0.7917 - val_loss: 0.5019 - val_acc: 0.7552\n",
      "Epoch 1484/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4352 - acc: 0.7899 - val_loss: 0.5019 - val_acc: 0.7552\n",
      "Epoch 1485/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4352 - acc: 0.7899 - val_loss: 0.5020 - val_acc: 0.7552\n",
      "Epoch 1486/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4352 - acc: 0.7882 - val_loss: 0.5020 - val_acc: 0.7552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1487/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4352 - acc: 0.7882 - val_loss: 0.5020 - val_acc: 0.7552\n",
      "Epoch 1488/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4352 - acc: 0.7917 - val_loss: 0.5020 - val_acc: 0.7552\n",
      "Epoch 1489/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4352 - acc: 0.7899 - val_loss: 0.5020 - val_acc: 0.7552\n",
      "Epoch 1490/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4351 - acc: 0.7899 - val_loss: 0.5020 - val_acc: 0.7500\n",
      "Epoch 1491/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4351 - acc: 0.7899 - val_loss: 0.5020 - val_acc: 0.7500\n",
      "Epoch 1492/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4351 - acc: 0.7899 - val_loss: 0.5020 - val_acc: 0.7500\n",
      "Epoch 1493/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4351 - acc: 0.7917 - val_loss: 0.5021 - val_acc: 0.7500\n",
      "Epoch 1494/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4351 - acc: 0.7899 - val_loss: 0.5021 - val_acc: 0.7500\n",
      "Epoch 1495/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4350 - acc: 0.7899 - val_loss: 0.5021 - val_acc: 0.7500\n",
      "Epoch 1496/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4350 - acc: 0.7899 - val_loss: 0.5021 - val_acc: 0.7500\n",
      "Epoch 1497/1500\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4350 - acc: 0.7899 - val_loss: 0.5021 - val_acc: 0.7500\n",
      "Epoch 1498/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4350 - acc: 0.7899 - val_loss: 0.5021 - val_acc: 0.7500\n",
      "Epoch 1499/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4350 - acc: 0.7899 - val_loss: 0.5022 - val_acc: 0.7500\n",
      "Epoch 1500/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4350 - acc: 0.7899 - val_loss: 0.5022 - val_acc: 0.7500\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "# Compile the model with Optimizer, Loss Function and Metrics\n",
    "\n",
    "model_2.compile(SGD(lr = .003), \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "run_hist_2 = model_2.fit(X_train_norm, y_train, validation_data=(X_test_norm, y_test), epochs=1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x116794080>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\njyjpi4jkESV9EZE8oqQvIpJH/j+ltIhxoTlvTQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x116e49d68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Type your code here to plot the loss accuracy and ROC curve\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(run_hist_2.history[\"loss\"],'r', marker='.', label=\"Train Loss\")\n",
    "ax.plot(run_hist_2.history[\"val_loss\"],'b', marker='.', label=\"Validation Loss\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Seems like loss function is actually getting worse for the validation data. We minimize the loss function for the validation data at around 500 epoch's, and that should be the number of epoch's for the model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAecAAAHiCAYAAADSwATnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1162abc50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Generate a probabilitistic score \n",
    "y_pred_prob_nn_2 = model_2.predict(X_test_norm)\n",
    "plot_roc(y_test, y_pred_prob_nn_2, 'NN')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "auc here looks pretty big, which means that the classifier is good as separating the two cases: diabetes and no-diabetes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
